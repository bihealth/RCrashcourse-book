[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Five days of R",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#five-days-of-r",
    "href": "index.html#five-days-of-r",
    "title": "Five days of R",
    "section": "Five days of R",
    "text": "Five days of R\nI have been teaching R to biologists and medical students for many years now. At the Core Unit for Bioinformatics at the Berlin Institute of Health, Charit√© - Universit√§tsmedizin Berlin, we have developed a five-day, 5 hour per day R crash course running for the last three years. This book is a companion to that course.\nThis is also the reason for how the materials in the book are arranged. Rather then discussing everything about vectors first, then everything about matrices etc., we start with easy things, and return to them later to build on them. I call this ‚Äúhelical learning‚Äù1 ‚Äì we spiral around the same topics, but each time going a bit deeper, and each time you will understand a bit more. This is also why some topics are spread between the days ‚Äì by trial and error, we have found the amount of material that can be covered in a day of learning.\n1¬†I got this idea from the professor Barbara P≈Çytycz from the Jagiellonian University, who taught me my first ‚Äúhelix‚Äù of the immune system.There are two goals of this book. The first one is that after five days of learning R, you will be able to load, inspect, manipulate and save data files (such as Excel tables or CSV files), make some basic plots and perform simple statistical tests. The second goal is that you are in a good starting position to continue learning R on your own.\nIn other words, this course should give you a jump start, allowing to overcome this first big hurdle in learning R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Five days of R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstalling R and RStudio\nBefore you dive in to the course, we would like to ask you to install both R and RStudio: you need R (obviously) and RStudio is a great and very popular interface for R. Minimal installation guide:\n\nInstall R. Go to R website and download the version for your operating system. Install it.\nInstall R Studio. Go to R Studio installation page and follow the instructions.\n\nThis should be it. However, if you are stuck or have problems with installing R or RStudio, please search the internet ‚Äì the installation and especially related problems unfortunately depends on your operating system and the version of the software, so it is hard to give a general advice.\nHere are some other websites with somewhat more detailed instructions:\n\nInstalling R on all systems\nInstalling R on Windows 7, 8, 10\nInstalling R, RStudio and configuring RStudio\nGetting started chapter of the R Cookbook by James JD Long.\n\nPotential problems:\n\nYou already had R installed on the system. Having multiple R versions may be a source of problems. If you have an old version of R, unless you have a good reason to keep it, uninstall it.\nYou need to compile packages. Packages come in two forms: precompiled packages and source packages. Some exotic packages might not have the precompiled version for your system. If you need to compile packages, you will have to install additional packages. For example, for Windows, you need to install Rtools - make sure you download the version that corresponds to your version of R.\n\n\n\nInstalling R packages\nDuring the course we will use several R packages that you need to install on your computer. On Day 2, we will discuss installing and loading packages, and we will make a note to install the required packages when they are needed. However, you can also install them right now, after you have installed R and RStudio. This can be more effective ‚Äì after all, installing might take some time.\nHere is the list of packages that you will have to install:\n\ntidyverse\nggplot2\nskimr\npander\nreadxl\nwritexl\njanitor\nbroom\ncowplot\nggbeeswarm\npheatmap\ntinytex (only if you want to produce PDF output from Rmarkdown)\n\nYou can install them by running the following code in your RStudio:\n\ninstall.packages(c(\"tidyverse\", \"ggplot2\", \"skimr\", \"pander\", \n                   \"readxl\", \"writexl\", \"janitor\", \"broom\", \n                   \"cowplot\", \"ggbeeswarm\", \"pheatmap\", \"tinytex\"))",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#the-structure-of-this-book",
    "href": "index.html#the-structure-of-this-book",
    "title": "Five days of R",
    "section": "The structure of this book",
    "text": "The structure of this book\nThis book is divided into five chapters, each corresponding to one day of the course. At the beginning of each chapter, you will find a short list of topics for the given day.\nSome parts of the book are highlighted:\nCode blocks with output:\n\n# this is a comment\nx &lt;- 1 + 1\n\nThe numbers on the left (if present) are not part of the code ‚Äì they are just line numbers. You can copy the code by clicking on the ‚ÄúCopy‚Äù button (üìã) and it will not copy these numbers.\n\n\n\n\n\n\n\nExercise 1 (Example) ¬†\n\nThis is how an exercise looks like!\nPlease do all exercises. It helps a lot.\nSome things are learned only through exercises.\n\n\n\n\n\n\n\nSolution (click me!)\n\n\n\n\n\nSome exercises have a solution which you can click to reveal.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful tips\n\n\n\n\nSome exercises have solutions in the ‚ÄúSolutions‚Äù chapter. If they do, please read the solution after you have completed the exercise ‚Äì often there will be a comment or a hint that will help you understand the material.\n\n\n\n\n\n\n\n\n\nRemember!\n\n\n\n\nRun all code chunks in your RStudio.\nDo all exercises.\nGo through the ‚ÄúReview‚Äù section at the end of each chapter and make sure you understand everything on the list.\n\n\n\nTake a look at the right margin! New concepts are highlighted on the right margin of the book\nAnd again2!\n2¬†And also footnotes.In each chapter there are several exercises. However, that does not mean that you should only do the exercises. In fact, you should try out every piece of code that is in the book. Copy it (there is a üìã button next to each code block that will do it for you), paste it into your RStudio and run it. Then try to modify it and see what happens.\nExercises in this book are important. They are not only there to check if you understood the material, but they can also introduce new concepts or ideas. This is because this book is not only about learning R, but also learning how to learn about R. So, for example, sometimes we will want you to figure stuff on your own rather than give you a ready-made answer.\nMany of the exercises have a solution provided, either inline (you have to expand it by clicking on the ‚ÄúSolution‚Äù button) or in the ‚ÄúSolutions‚Äù section of the book. It is really important that you do not give up too quickly. Try to solve the exercise on your own, and only then look at the solution.\nEach chapter is ended by a ‚ÄúReview‚Äù section, which contains a list of things that you have learned that day. It is really important that you go through that list and make sure that you understand everything on it. Some of the new things appeared in the exercises, so if you skipped them, you might want to go back and do them.\nIf you do all that, I personally guarantee you that by the end of this course you will be able to use R in your work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#general-advice",
    "href": "index.html#general-advice",
    "title": "Five days of R",
    "section": "General advice",
    "text": "General advice\nThe course is a real crash course. There is a lot of material coming at you in a very short time. You will feel overloaded and overwhelmed ‚Äì this is normal. Don‚Äôt worry! It will soon get better, and in a few days you will be able to do fairly advanced things with R.\nThe key is to keep playing with your R; trying out new things, breaking it. Please go through all exercises in that book, even if they seem simple at the first glance (some of them are tricky, others are used to smuggle in new concepts and useful tidbits of information).\nWhenever you feel you don‚Äôt understand something, stop and try to figure it out. Use internet search very liberally. Many answers can be found on sites such as StackOverflow, R-bloggers, or simply in the R documentation3. Try out the code you will find in these sources ‚Äì just copy-paste it into your RStudio and adapt it to your needs. Feel free to use Large Language Models (such as GPT) ‚Äì they are very good at explaining code, especially when you are learning basic concepts.\n3¬†You can access the R documentation by typing ?function_name in the console. You can search for concepts using ??keyword.However, if you want to learn R, simply doing this course will not be enough. You need to start using it in a real world setting. Unfortunately ‚Äì the better you already are at Excel, Word and other such tools, the harder it will be switching to R: tasks that are a breeze in Excel will at first require you to spend substantially more time in R. However, trust me: it pays off in the long run. Therefore, for best results, force yourself to use R even if at first it is less efficient then other tools.\nFinally: programming can be fun. Most programmers I know simply enjoy doing that. It is a satisfaction similar to building Lego models or solving puzzles or reading a crime story ‚Äì and also very much like doing experiments. Unlike experiments, however, you can‚Äôt break expensive equipment and the results of your attempts are usually immediately visible, no need to wait for weeks for the results. You can play, experiment, try out various things at no expense and no risk. If you can get into this mindset, learning R will be much, much easier.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Five days of R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI have been teaching statistics and bioinformatics for more than two decades now. About ten years ago, I started teaching the first ‚ÄúR crash course‚Äù with the goal of introducing PhD students and postdocs to R as quickly and as painlessly as possible. The course evolved over the years, and I had many partners and co-teachers.\nIn the first place, I would like to thank Carlo Pecoraro from Physalia Courses for the opportunity to teach my first R crash course for Physalia.\nSeveral times I have taught the course together with my colleague, Dr.¬†Manuela Benary from the Core Unit of Bioinformatics at the Berlin Institute of Health, Charit√© - Universit√§tsmedizin Berlin. Manuela has been a great partner in teaching the course, and many ideas in this book are actually hers.\nI would also like to thank many colleagues and friends who had the patience to go through this book and provide feedback.\nFinally, I thank you, the Reader, for your interest in this book. I hope it will be useful and that you will enjoy using R in your research.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html",
    "href": "day1-intro-to-r.html",
    "title": "1¬† First day of R",
    "section": "",
    "text": "1.1 Goals for today\nWhat you should know after today:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#goals-for-today",
    "href": "day1-intro-to-r.html#goals-for-today",
    "title": "1¬† First day of R",
    "section": "",
    "text": "what is R?\nwhy use R?\nfirst steps in R\n\n\n\nwhat R is\nhow to start R\nhow to use R as a calculator\nhow to assign variables\nhow to use functions\nhow to use vectors\nhow to use data frames\nhow to use packages",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#r-rstudio-and-other-languages",
    "href": "day1-intro-to-r.html#r-rstudio-and-other-languages",
    "title": "1¬† First day of R",
    "section": "1.2 R, RStudio and other languages",
    "text": "1.2 R, RStudio and other languages\n\n1.2.1 Why R?\n\n\n1.2.2 R and RStudio\nR is the name of both, the programming language and of the language interpreter. When you start RStudio, you can see the R language interpreter working in the part of the window left and bottom - called ‚Äúconsole‚Äù. So yes, you don‚Äôt need RStudio to work with R and, in fact, many people prefer to work with R in a different environment.\nRStudio is a so called IDE, an Integrated Development Environment. That is, it provides a lot of goodies that help make your work easier, faster and more efficient.\n\n\n1.2.3 R and other languages\nR is not the only language that you can use for data analysis. There are many other languages that are used for this purpose, including Python, Matlab and many others. Each of these languages has its own strengths and weaknesses, and the choice of language depends on your needs. In fact, most bioinformaticians know more than one language, and use the one that is best suited for the task at hand.\nWe think that R is a particularly good choice for all those who just need a tool to use from time to time to help them with their work. It is relatively easy to learn, and it is very powerful. However, other choices are also worth mentioning.\nMatlab is a language that is in many ways similar to R. The main  differnce is maybe that unlike R, Matlab is not free ‚Äì it is closed source and you have to pay for a license. This has some advantages. For example, and as you will see during this course, R development is not centralized and so there are many packages that do the same thing. Matlab is in some aspects more consistent and more polished than R, and in some comparisons appears to be faster ‚Äì and for this, it is often the language of choice for areas such as image analysis.Matlab\nPython is completely different story. This is a powerful, fast, general  purpose programming language. It is more versatile than R, has a much more standardized syntax and development process. However, it is harder to learn and it is not really meant to be used interactively (although it can be ‚Äì especially when combined with Quarto or Jupyter Notebook). While many statistical modules exist for Python, it is not as strong in this area as R.Python",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#projects-and-workspaces",
    "href": "day1-intro-to-r.html#projects-and-workspaces",
    "title": "1¬† First day of R",
    "section": "1.3 Projects and Workspaces",
    "text": "1.3 Projects and Workspaces\n\n1.3.1 Creating a project: start here!\nWhen starting work with a new project, do the following: (i) create a new  directory for the project, (ii) open an R script file and save it in the directory you created and (iii) copy necessary data files.RStudio projects\n\n\n\nCreating a new project in RStudio\n\n\nTo create a new directory in RStudio, go to File -&gt; New Project. When the dialog window appears, select first ‚ÄúNew Directory‚Äù and then ‚ÄúNew Project‚Äù. Click on ‚ÄúBrowse‚Ä¶‚Äù to select the location where you would like to have the directory created. Enter a name for your project and click on ‚ÄúCreate Project‚Äù. Presto!\n\n\n\n\n\n\n\nExercise 1.1 ¬†\n\nCreate a new project in RStudio. Do it now, you will continue to work with this project over the next few days (we hope).\nInspect the contents of the project directory. What files are there?\n\n\n\n\n\nWhen Rstudio creates a new project, it creates a new directory with the same name as the project. Furthermore, it creates a new file in this directory called projectname.Rproj. This file is used by RStudio to keep track of  project-specific settings.projectname.Rproj\nYou can open this file by double-clicking on it in the Files pane in RStudio. Like most of the files that you will be working with, it is a simple text file: you can open it in any text editor, including RStudio.\nThe other file I told you to create is a script file. This is where you will later be typing your code, and we will discuss it in more detail in a moment.\nLater on, if you choose to do so, R can create two hidden files, Rhistory  (called .Rhistory on Unix-like systems and _Rhistory on Windows) and .RData (or _RData). This files save the state of your R session (of your R workspace, to be specific)..Rhistory and .RData\n\n\n1.3.2 RStudio components\nAs I mentioned before, RStudio is a so-called ‚Äúintegrated development environment‚Äù (IDE). It makes working with R much easier and more efficient. You will be exploring many of its components when you start working with R and RStudio, but let us just mention a couple general things. Below is a screenshot of an R session opened in RStudio:\n\n\n\nRStudio components\n\n\nThere are four main RStudio panels on the screenshot above1. Top left is where you usually see your scripts, R markdown documents or any other opened files. Sometimes you will have a tab with the view of a data frame. Most of the time, this is where you will be working ‚Äì mostly typing your code in a script or R markdown file. Chances are that you don‚Äôt see this panel yet, as you have not opened any files ‚Äì you will see it in a moment.\n1¬†You can customize that view, of course. You can change colors, position, elements shown and much, much more. Take a look in Tools -&gt; Global Options.2¬†The ‚Äúcommands‚Äù that you execute in R are properly called ‚Äúexpressions‚Äù.On the top right panel you have several tabs. You will soon be using the ‚ÄúEnvironment‚Äù tab, which shows you the variables that you have created and their values. The ‚ÄúHistory‚Äù tab shows you the history of commands2 that you have typed in the console. There are other tabs here that might come in handy at some point in your work with R ‚Äì one that you might want to try to supplement this course is the ‚ÄúTutorial‚Äù, which walks you through basics of R (a bit like this book).\nThe bottom left panel is where the actual R is, or, more precisely, the R console3. The R console is where you can directly access R. If you were to start R standalone (without R studio), the console is the only thing that you would see. You will start typing in the console in a moment, but soon you will learn to indirectly access the console by typing your code in the script and executing it. Note that at the top of the console you can see the path to your project directory. We will discuss paths and directories in a moment.\n3¬†Pronounced /Ààk…ín.s…ô äl/Finally, on the bottom right you have, again, several tabs. ‚ÄúFiles‚Äù is a file browser. You can navigate your file system here, click to load files in R or preview them, and more. In another tab, you will see the plots once you generate them.\n\n\n\n\n\n\n\nExercise 1.2 (Help) Click on the ‚ÄúHelp‚Äù tab in the right bottom panel. Type ‚Äút.test‚Äù in the search bar (mind the dot!) and press Enter. What do you see? Go through the document and notice the structure of the manual page. The majority of the manual pages will have precisely the same structure, as it is a part of the R documentation system.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#lets-start-with-r",
    "href": "day1-intro-to-r.html#lets-start-with-r",
    "title": "1¬† First day of R",
    "section": "1.4 Let‚Äôs start with R",
    "text": "1.4 Let‚Äôs start with R\n\n1.4.1 R as a calculator\nYou can use R as a very powerful calculator. For example, do you want to know what \\(\\sin(\\pi/2)\\) is? Just type sin(pi/2) in the console and press  Enter. Addition and subtraction work, as expected, with + and -. To multiply two numbers, type 2*3; to divide, type 2/3. You can get exponents (powers, eg. \\(2^3\\)) by typing 2^3. If the ^ symbol (called ‚Äúcaret‚Äù) is not available on your keyboard, you can use ** instead. Parentheses () are used to group expressions, just like in mathematics. To logarithmize, you can use log(), log2() and log10() functions. For  example, to calculate \\(\\log_{10}(100)\\), type log10(100). Can you guess how to calculate \\(\\sqrt{2}\\)? Yes, you are right: sqrt(2). Or 2^(1/2),  that will also do. Finally, the exp() function calculates the exponential function \\(e^x\\). sin()log(), log2() and log10()sqrt()exp()\n\n\n\n\n\n\n\nExercise 1.3 Calculate the following expressions in R:\n\n\\(\\log_{2}(8)\\)\n\\(\\sin(\\pi)\\)\n\\(2^{10}\\)\n\\(\\sqrt{e}\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlog2(8)\n\n[1] 3\n\nsin(pi)\n\n[1] 1.224647e-16\n\n2^10\n\n[1] 1024\n\nsqrt(exp(1))\n\n[1] 1.648721\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 Using script files\nOn the left side of the RStudio window you have (by default) two panels: the lower one is called ‚ÄúConsole‚Äù. When you create a new script file, as you have done it a moment ago, it appears above.\n\n\n\n\n\n\nTyping in console\n\n\n\nYou can type your commands (properly called ‚Äúexpressions‚Äù) directly into the console, but it is generally not a good idea. Why? The truthful answer is: because it is messy and sooner or later you will regret it. You can save the history of what you type in the console, but it is easier (and cleaner) to save your program in a script file.\n\n\nWhen you open or create an R script file and type something into it, you can send it to console and execute it. To do that, you have two options. First, you can use the ‚ÄúRun‚Äù button in the script panel:\n\n\n\nUsing the ‚ÄúRun‚Äù button\n\n\nHowever, this is one of the most common operations, so it is much more efficient to use a keyboard shortcut: Ctrl+Enter (or Cmd+Enter on Mac). This will send the current line of code to the console, where it will be executed, and the cursor in the script will move to the next line of code. You can also select a fragment of the code before you press Ctrl+Enter, and then the whole selected fragment will be sent to the console.\n\n\n\n\n\n\nKeyboard shortcuts\n\n\n\nThere are many keyboard shortcuts in RStudio. You can see them all in the ‚ÄúHelp‚Äù menu, under ‚ÄúKeyboard Shortcuts Help‚Äù. You can also customize them in the ‚ÄúTools‚Äù -&gt; ‚ÄúModify Keyboard Shortcuts‚Äù menu.\n\n\n\n\n1.4.3 Comments\nIf you start your line with # (called ‚Äúhash‚Äù or ‚Äúpound‚Äù sign), the rest of the line will be ignored by R. This is called a comment and I will spend some time later on convincing you that you should use a lot of comments in your code. Comments\n\n# this is a comment\n# this is another comment\nx &lt;- 2\n\n# the following will not be executed:\n# x &lt;- 5\n\nComments are also a great way to temporarily disable a line of code - we call it ‚Äúcommenting out‚Äù. This is for the cases when you want to try out something, but you do not want to delete a line of code that may be still useful later on.\n\n\n\n\n\n\n\nExercise 1.4 Repeat Exercise¬†1.3, but now type the expressions into the script which you have created in Exercise¬†1.1. Before each expression, insert a comment line stating what it does, for example # calculate sinus of pi. Run the script by pressing Ctrl+Enter after each line.\n\n\n\n\nScript files are also text documents. You can open them in any text editor, for example Notepad or even Word (but don‚Äôt do that). In RStudio, you see the script file in many colors: for example, comments can be green, strings (text in quotes) can be red, and so on. This is called syntax highlighting and is done by RStudio to make your code more readable. You will not see the colors when you open your R script in Notepad.\n\n\n\n\n\n\nImportant\n\n\n\nFrom now on, you should only type your code in script files.\n\n\n\n\n1.4.4 Variables\nWhat if we want to store the result of a calculation for later use? We can do this by assigning the result to a variable. In R, you assign a value to a variable using the &lt;- operator: &lt;- assignment operator\n\nx &lt;- 2\ny &lt;- sin(pi/2)\nz &lt;- x + y\n\nIf you want to see the value of a variable, just type its name in the console and press Enter, or use print() function: print()\n\nprint(z)\n\n[1] 3\n\n\n\n\n\n\n\n\nAssignment operator\n\n\n\nMany other languages use = as an assignment operator. In R, you can use = as well, but do yourself a favour and don‚Äôt. Use &lt;- instead. Why? Your code will be more readable and you will avoid many common mistakes.\n\n\nVariables are like boxes in which you can store values. However, unlike boxes, when you assign one variable to another, the first variable keeps its content:\n\nx &lt;- 2\ny &lt;- x\nx\n\n[1] 2\n\n\nWe now come to a very important point which we will revisit often, as it is one of the most common beginner (and not only beginner) mistakes. When you forget to assign the value to a variable, R will print it to the console, but the variable will not be modified:\n\nx &lt;- 2\n# prints 0.9092974:\nsin(x) \n# prints 2\nx \n\nIn the code above, the value of x is not changed by the sin() function. To store the value of a function, you need to assign it to a variable:\n\nx &lt;- 2\n# does not print anything:\nx &lt;- sin(x) \n# prints 0.9092974:\nx    \n\nPlease spend some time on this, as it is a very common source of errors.\n\n\n\n\n\n\n\nExercise 1.5 Without actually running the code, guess what will be the value of x if you execute the following code:\n\n# first, assign starting value\nx &lt;- 2\n\n# multiply by 100\nx &lt;- x * 100\n\n# power of 2, add 10\nx^2 + 10\n\n# divide by 10\nx / 10\n\n# subtract 10*20 and take the square root\nx &lt;- sqrt(x - 10 * 20)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nJust run the code ;-) Which lines contain the assignment operator?\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAs a rule of thumb4, if the expression you type in your script does not contain the &lt;- operator, it will not modify any variables.\n\n\n4¬†There are exceptions to this rule, but they are relatively rare and I will not discuss them here.\n\n\n\n\n\n\nExercise 1.6 Create a variable using x &lt;- 42. Take a look at the Environment pane in RStudio (top left part of the window). Do you notice anything?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA new entry appeared in the ‚ÄúEnvironment pane‚Äù. It shows that there is a new variable present in your environment.\n\n\n\n\n\n\n\n\n\n1.4.5 Character variables\nVariables can store not only numbers, but also text. Text in R is called a character string. To create a character string, you need to enclose the text in quotes (both single and double quotes are allowed, but try to be consistent and use only one type). For example:\n\nname &lt;- \"January\"\ncity &lt;- \"Hoppegarten\"\ngreeting &lt;- \"Hello, world!\"\n\nCharacter variables cannot be used with algebraic computations, the following code will throw an error:\n\n# this does not work!\nname + city\n\nError in name + city: non-numeric argument to binary operator\n\n\nHowever, if you want to ‚Äúadd‚Äù two character strings (that is, concatenate them), you can use the paste() function: paste()\n\npaste(name, city)\n\n[1] \"January Hoppegarten\"\n\n\nQuite often, you don‚Äôt want to have a space between the two strings. This is such a common operation that R has a shortcut for it: paste0()\n\npaste0(name, city)\n\n[1] \"JanuaryHoppegarten\"\n\n\n\n\n\n\n\n\nOther types\n\n\n\nThere are other types of data types in R. Later on, we will briefly touch on factors, which look like character strings but behave like numbers. Another important data type is a logical type, which can have only two values: TRUE and FALSE. We will talk about logical types in more detail tomorrow. And under the hood, numeric vectors can be either integers (numbers like 1, 2, ‚Ä¶) or floating point numbers (numbers like 1.1, 2.2 or \\(\\pi\\)).\n\n\n\n\n\n\n\n\n\nExercise 1.7 (Variables) Create the following variables in your script:\n\nname with the value of your first name\ncity with the value of the city where you live\nage with the value of your age\ngreeting with the value ‚ÄúHello,‚Äù\nconcatenate the variables greeting and name and store the result in a new variable hellothere\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nname &lt;- \"January\"\ncity &lt;- \"Hoppegarten\"\nage &lt;- 199\ngreeting &lt;- \"Hello,\"\nhellothere &lt;- paste(greeting, name)\n\n\n\n\n\n\n\n\n\n\n1.4.6 Workspaces, history and environments\nWhen you work with R, you create variables, functions and other objects. They appear in the ‚ÄúEnvironment‚Äù tab in RStudio and constitute what is known as a workspace. worspace\nWhen you exit R, for example when closing RStudio or switching to another project, R / RStudio will ask you whether you wish to save the current workspace and / or history. This can create two files in your project directory: .RData and .Rhistory (or _RData and _Rhistory on Windows). The .RData file contains the workspace, that is, all the objects (variables, functions, etc.) that you have created. The .Rhistory is a text file (you can open it in any text editor) that contains the history of commands that you have typed in the console.\n\n\n\nSave the workspace?\n\n\nThat all sounds like a good and useful thing, right? By saving the workspace, you do not have to repeat all your calculations! And by saving the history, you don‚Äôt even need to type your code in a script, since R saves everything automatically, right?\nWell, not so fast. Relying on that can get you into trouble.\nWorkspace. Saving a workspace can be useful, but it can also get you in a mess. When working with R interactively, one tends to create a lot of objects just to try out various things. Not all of them will go into your final version of the script, but the mere fact that they exist in your environment can be a problem: they can interfere with your code, take up memory and even can hide certain bugs.\nConsider this example: trying out various things, you create a variable called foo. Later on, in a much later version of the script, you no longer create foo, you now have a ‚Äúproper‚Äù name like mouse_transcripts. However, one of your script function still uses foo instead of mouse_transcripts. You run the script and it works! But it works only as long as you have the foo variable in your workspace. If you were to run the script on a different computer, or give it to your colleague, it would stop working.\nHistory. Beware! By default, R saves only the last 1000 lines of your history.\n\n\n\n\n\n\nHistory and Workspace\n\n\n\nDo not rely on the history and workspace to save your work. Always save your scripts, and if you want to save the data, save it in a separate file.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#vectors-and-vectorization",
    "href": "day1-intro-to-r.html#vectors-and-vectorization",
    "title": "1¬† First day of R",
    "section": "1.5 Vectors and vectorization",
    "text": "1.5 Vectors and vectorization\n\n1.5.1 Vectors\nVariables can (and do) store a lot more than single values. One of the most basic and important data types in R is a vector. A vector is simply a sequence of values ‚Äì just like in maths. And you know what? You have already created vectors in R. In mathematics, any scalar value can be treated as a one-dimensional vector and it is exactly like that in R: any single value is a 1-element vector, including all the variables that you have created in the previous exercise.\nTo create a vector with more than one value, you can use the c() function  (‚Äúc‚Äù stands for ‚Äúcombine‚Äù). For integer numbers, you can use the : operator  to create a sequence of numbers. For example:c()Creating a sequence of numbers with :\n\nsequence &lt;- 5:15\nnumbers &lt;- c(10, 42, 33, 14, 25)\nperson &lt;- c(\"January\", \"Weiner\", \"Hoppegarten\")\n\nIt is also possible to combine two vectors longer than 1 into one: Combining vectors\n\nfirst_v &lt;- c(1, 2, 3)\nsecond_v &lt;- c(4, 5)\ncombined_v &lt;- c(first_v, second_v)\nprint(combined_v)\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\n\nExercise 1.8 (Vectors) Create a vector that combines the numeric value 1 and the string \"one\". What happens? Can you venture a guess?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncombined &lt;- c(1, \"one\")\ncombined\n\n[1] \"1\"   \"one\"\n\n\nIn the resulting vector, the 1 is shown with quotes around it. This means that it is treated as a character string, not as a numeric value. The reason for that is that vectors can hold only one type of values. We will discuss it in more detail later.\n\n\n\n\n\n\n\nYou can access individual elements of a vector using the [ ] operator: Accessing elements of a vector with [ ]\n\nnumbers[1]\n\n[1] 10\n\nperson[2]\n\n[1] \"Weiner\"\n\n\nBut hey, I told you that every value is a vector in R, right? And that includes the indices 1 and 2 that you have just used. So, what would happen if we used more than two values as an index? Try it:\n\nnumbers[1:3]\n\n[1] 10 42 33\n\nperson[3:1]\n\n[1] \"Hoppegarten\" \"Weiner\"      \"January\"    \n\nsel &lt;- c(1, 5, 3)\nnumbers[sel]\n\n[1] 10 25 33\n\n\nAs you can see, not only can you use a vector as an index, but you can also use a variable as an index.\n\n\n\n\n\n\nDo not use a comma\n\n\n\nIt is tempting to select, say, first and the third element of a vector numbers by writing numbers[1, 3]. This will not work! As you will see tomorrow, this way of writing is for two-dimensional objects. You must use a vector as an index: numbers[c(1, 3)].\n\n\n\n\n\n\n\n\nVectors and indices\n\n\n\nIn many (most?) programming languages, the first element of a vector is accessed using the index 0. For example in Python, to access the first element of an array, you need to type array[0]. This has something to do with how computers work. In R, the first element is always 1 ‚Äì R was designed by statisticians, and in mathematics we always start counting from 1. For some reason, this seems to make some computer scientists angry.\n\n\n\n\n1.5.2 Named vectors\nAccessing elements of a vector using indices is all well and good, but sometimes it can be very inconvenient, especially if the vectors are very long. Or maybe you do not remember the order in which you have stored the elements of the vector ‚Äì was the last name first, or second element of the person vector?\nVectors allow you to name their elements. We can either define the names at the very beginning, when we create the vector, or we can add them later using the names() function. Here is how you can do it: Named vectors\n\nperson &lt;- c(first=\"January\", last=\"Weiner\", city=\"Hoppegarten\")\n\nOnce you have named the elements of a vector, you can access them using their names:\n\nperson[\"city\"]\n\n         city \n\"Hoppegarten\" \n\nperson[c(\"first\", \"last\")]\n\n    first      last \n\"January\"  \"Weiner\" \n\n\nOr, we can change the names with the names() function: names()\n\nnames(person) &lt;- c(\"name\", \"given\", \"place\")\nperson\n\n         name         given         place \n    \"January\"      \"Weiner\" \"Hoppegarten\" \n\n\n\n\n1.5.3 Assigning values to selected elements\nOK, one more thing about vectors. Above we have selected elements from a vector. It turns out, we can do more with that selections then just print it to a console: Assigning values to selected elements\n\nnumbers &lt;- c(10, 42, 33, 14, 25)\nsel &lt;- c(1, 5)\nnumbers[sel] &lt;- c(100, 500)\nnumbers\n\n[1] 100  42  33  14 500\n\n\nHere is what happened: we assigned new values to the first and the fifth element of the vector numbers. This is a very powerful feature of R and you will be using it a lot.\n\n\n\n\n\n\n\nExercise 1.9 (Accessing and modifying vectors) ¬†\n\nCreate a vector with the first 10 prime numbers. Call it primes.\nHow do you access the 3rd, 5th and 7th prime number?\nWhat happens when you do primes[11]?\nWhat happens when you do primes[11] &lt;- 31?\nWhat happens when you do primes[15] &lt;- 47?\nWhat happens when you do primes[-1]?\nChange the 3rd, 5th and 7th prime number to 100, 500 and 700, respectively.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# note that 1 is not a prime number!\nprimes &lt;- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29)\nprimes[c(3, 5, 7)]\n\n[1]  5 11 17\n\n# returns a special value, NA\n# (not available)\nprimes[11]\n\n[1] NA\n\n# adds a new element to the vector\n# at the end\nprimes[11] &lt;- 31\nprimes\n\n [1]  2  3  5  7 11 13 17 19 23 29 31\n\n# adds a new element to the vector\n# at position 15, fills the gap with\n# NAs\nprimes[15] &lt;- 47\nprimes\n\n [1]  2  3  5  7 11 13 17 19 23 29 31 NA NA NA 47\n\n# returns the vector without the first\n# element\nprimes[-1]\n\n [1]  3  5  7 11 13 17 19 23 29 31 NA NA NA 47\n\nprimes[c(3, 5, 7)] &lt;- c(100, 500, 700)\n\n\n\n\n\n\n\n\n\n\n1.5.4 Vectorization\nVectors are very useful ‚Äì but wait, there is more. What happens if we add a value to a vector? Try it:\n\nnumbers &lt;- c(10, 42, 33, 14, 25)\nnumbers + 10\n\n[1] 20 52 43 24 35\n\n\nAs you can see, R has added the value 10 to every single element of the vector numbers. The same thing happens with other operators, like -, * and /. Try it yourself.\nThis is called vectorization and it is one of the most powerful features  of R compared to other languages. It will allow you to write very concise and, at the same time, readable code.Vectorization\nThe vectorization works not only with operators like +, -, * and /, but with many functions. For example, it works with most of the mathematical functions like sin() or log(). Try it:\n\nlog10(numbers)\n\n[1] 1.000000 1.623249 1.518514 1.146128 1.397940\n\nsin(numbers)\n\n[1] -0.5440211 -0.9165215  0.9999119  0.9906074 -0.1323518\n\n\nHowever, there is a catch. What happens if you try to add two vectors when both of them with more than one element? First, let us try to add two vectors of the same length:\n\nnumbers1 &lt;- c(1, 2, 3)\nnumbers2 &lt;- c(4, 5, 6)\nnumbers1 + numbers2\n\n[1] 5 7 9\n\n\nAs you can see, R has added the first element of the first vector to the first element of the second vector, the second element of the first vector to the second element of the second vector, and so on. Makes sense, right? Same would happen if we were to subtract, multiply or divide the vectors (or use logical operations, which you will learn on Day 3).\nImagine the two vectors one beneath the other:\nnumbers1:   c(1,    4,    5)\n              +     +     +\nnumbers2:   c(2,    5,    6)\n              ‚Üì     ‚Üì     ‚Üì\nresult:     c(3,    9,   11)\n\nR is simply adding up corresponding elements. This does not look like much now, but trust me, it will be extremely useful in the future.\nHowever, if the vectors have different lengths, it is a different story altogher. Take a look:\n\nnumbers1 &lt;- c(1, 2, 3)\nnumbers2 &lt;- c(4, 5)\nnumbers1 + numbers2\n\nWarning in numbers1 + numbers2: longer object length is not a multiple of\nshorter object length\n\n\n[1] 5 7 7\n\n\nOoops, what exactly happened here? First thing to note is that there was no error. There was a warning, but still our code executed and produced a result. But what is that result? For the first element of the result, it is clear enough: 1 + 4 = 5. Same for the second, 2 + 5 = 7. But what about the third? It seems that R added 3 + 4 = 7. But why?\nR noticed that it is missing an element to be added to the third element of the vector numbers1. So, it did what is called recycling. It  ‚Äúrewound‚Äù the vector numbers2 to the beginning and added the first element of numbers2 to the third element of numbers1. However, since after the rewinding and adding one element of vector numbers2 was left (because numbers1 did not have any more elements), R issued a warning.Recycling\nnumbers1:   c(1,     2,      3)\n              +      +       +\nnumbers2:   c(4,     5)    c(4,    5)\n              ‚Üì      ‚Üì       ‚Üì\nresult:     c(3,     9,     11)\n\nIf the length of the first vector was a multiple of the length of the second vector, R would not have complained:\n\nnumbers1 &lt;- c(1, 2, 3, 4, 5, 6)\nnumbers2 &lt;- c(7, 8)\nnumbers1 + numbers2\n\n[1]  8 10 10 12 12 14\n\n\nSee? No warning. R was recycling the second vector over and over again. Recycling is a dangerous business: if you are not careful, you can get results which you have not expected.\nnumbers1:   c(1,     2,      3,     4,     5,     6)\n              +      +       +      +      +      +\nnumbers2:   c(7,     8)    c(7,     8)   c(7,     8)\n              ‚Üì      ‚Üì       ‚Üì      ‚Üì      ‚Üì      ‚Üì\nresult:     c(8,    10,     10,    12,    12,    14)\n\nTake it slow. This is advanced stuff, but I had to warn you already at this stage ‚Äì this is one of the common sources of errors in R. Watch out for this ‚Äúlonger object length is not a multiple of shorter object length‚Äù warning.\n\n\n\n\n\n\nRecycling advice\n\n\n\nHere is our advice to you: either use a vector and a single element vector, or two vectors of the same length. And in the cases where, for some reason, you need to recycle, make sure that you know what you are doing. For example, check the length of both vectors.\n\n\nWith vectors that have only a couple of numbers it is quite easy to see what is happening, but what if you have thousands of variables? In other words, how to check the lenght of a vector? You can use the length(): length()\n\nlength(numbers1)\n\n[1] 6\n\nlength(numbers2)\n\n[1] 2\n\n\n\n\n\n\n\n\n\nExercise 1.10 (Vectorisation) ¬†\n\nCreate a vector with several numbers and try to add, subtract, multiply and divide it by a single number. What happens?\nSay, you have three values which are the diameters of three circles: 1, 5 and 13. You would like to have a vector containing the areas of these circles. What is the simplest way of doing that?\nHow do you check the length of this vector?\nOne vector, lengths, contains the lengths of the sides of three rectangles, and the other, widths, contains their widths. Create a vector containing the areas of these rectangles.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# create a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\n# add, subtract, multiply and divide\nnumbers + 10\n\n[1] 11 12 13 14 15\n\nnumbers - 10\n\n[1] -9 -8 -7 -6 -5\n\nnumbers * 10\n\n[1] 10 20 30 40 50\n\nnumbers / 10\n\n[1] 0.1 0.2 0.3 0.4 0.5\n\ndiameters &lt;- c(1, 5, 13)\nareas &lt;- pi * (diameters/2)^2\nareas\n\n[1]   0.7853982  19.6349541 132.7322896\n\nlength(areas)\n\n[1] 3\n\nlengths &lt;- c(1, 2, 3)\nwidths &lt;- c(4, 5, 6)\nareas &lt;- lengths * widths\nareas\n\n[1]  4 10 18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessages, Warnings and Errors\n\n\n\nR has three types of information to pass to you: messages, warnings and errors. Messages are just that ‚Äì messages. Warnings are messages that tell you that something might be wrong and you should pay attention, but R will nonetheless do what you asked it to do. Errors stop the execution of your code, but warnings do not.\nYou should pay attention to warnings, but you do not have always to do something about them ‚Äì some you can safely ignore. Errors, on the other hand, you should always fix.\n\n\n\n\n1.5.5 The special value NA\nOne more thing: there are a couple of special values in R that you should know about. One of the most prominent, useful and frequently encountered is the NA value, which stands for ‚ÄúNot Available‚Äù. You will see it  frequently when you work with data. Actually, you have already seen it when you tried to access an element of a vector that does not exist.NA\nIt is possible to apply mathematical operations to NA values, but the result is inadvertently NA:\n\nNA + 1\n\n[1] NA\n\nnumbers &lt;- c(1, 2, NA, 4)\nnumbers * 3\n\n[1]  3  6 NA 12\n\n\nThis also goes for some functions, which, quite often, have a special argument to omit the NA values. For example, the mean() function  calculates the mean of a vector:mean()\n\nnumbers &lt;- c(1, 2, NA, 4)\nmean(numbers)\n\n[1] NA\n\nmean(numbers, na.rm=TRUE)\n\n[1] 2.333333\n\n\n\n\n\n\n\n\nTip¬†1.1: Useful functions\n\n\n\nThere is a whole bunch of functions that you can use to work with vectors, and here are some of them ‚Äì with mostly self-explanatory names: sum(), min(), max(), range(), sd(), var(), median(), quantile().  Look them up in the help system by typing, for example, ?sum in the console, and try them out to see how they work.\n\n\nDescriptive statistics: sum(), min(), max() etc.The NA value very frequently pop up when you try to convert a character vector holding what looks like numbers into a numeric vector. We will see many such examples in the days to come; the conversion is often done using the as.numeric() function. For example, it is quite common that values  typed in a spreadsheet contain comments or values which look like this &gt; 50 (measurement out of range).as.numeric()\n\nimported_data &lt;- c(\"10\", \"20\", \"30\", \"&gt; 50\", \"40\", \"N.A.\", \"60 (unsure)\")\n# this will generate a warning\nimported_data &lt;- as.numeric(imported_data)\n\nWarning: NAs introduced by coercion\n\nimported_data\n\n[1] 10 20 30 NA 40 NA NA\n\n\nAs you can see, R conveniently warns you that some elements of the vector were changed to NA. Look out for that warning!\n\n\n\n\n\n\nSpecial values\n\n\n\nThere are a few other special values in R that behave similarly to NA.  Inf stands for infinity, you will get it when you divide a positive number by zero: 1/0. -Inf is the negative infinity (when you divide a negative number by 0), and NaN stands for ‚ÄúNot a Number‚Äù ‚Äì this is what you get when you try to subtracting Inf - Inf or dividing 0/0. They have also other uses ‚Äì for example, if a function wants to know how many rows of output you would like to see, and your answer is ‚Äúall of them‚Äù, you can use Inf as the number of rows.\n\n\nInf, -Inf and NaN",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#putting-it-all-together",
    "href": "day1-intro-to-r.html#putting-it-all-together",
    "title": "1¬† First day of R",
    "section": "1.6 Putting it all together",
    "text": "1.6 Putting it all together\n\n1.6.1 Water lillies on a lake\nThere is an old puzzle that goes: ‚ÄúOn a lake, there is a water lily. Each day the lily doubles in size. After 30 days, the lily covers the entire lake. On which day was the lily covering half of the lake?‚Äù.\nIn the following section we will model the behavior of the lily using R. Let us start with some assumptions.\n\nWe designate the first day as Day 1.\nEvery day, the lily doubles the area it covers.\nOn the first day, the lily covers 1% of the lake.\n\n\n\n\n\n\n\n\nExercise 1.11 (Modelling water lilies) Take a piece of paper and a pen. Your task is to come up with a formula to describe the area of the lily on the \\(n\\)-th day. Write down the formula.\nHint: if you are stuck, try to calculate the area of the lily on the first few days.\n\n\n\n\nThere is an important point that we wish to demonstrate here. Quite often it pays off to close your laptop and think for a moment what it is it that you want to do, rather than start coding right away. Pen and paper are helpful (we will be making this point again when it comes to visualizations). If you do not have a clear idea of what you want to do, you can get stuck thinking about what you already know how to do.\nThe formula for calculating the area of the lily on the \\(n\\)-th day is \\(0.01 \\times 2^{n-1}\\). You can come up with that result quite easily if you consider the first few days. On day 1, the area is \\(1\\% = 0.01\\). On day 2, is twice that, that is, \\(0.01 \\times 2 = 0.01 \\times 2^1 = 0.02\\). On day 3, it is twice the area from the previous day: \\(0.02 \\times 2 = 0.04 = 0.01 \\times 2\n\\times 2 = 0.01 \\times 2^2\\). And again, on day 4, it is \\(0.01 \\times 2^3\\). And so on5. We can show it in a table:\n5¬†If started counting from 0 ‚Äì that is, if we designated the first day as Day 0 ‚Äì the formula would be \\(0.01 \\times 2^n\\).\n\n\n\n\n\n\n\n\nDay\nArea\nCalculation\nFormula\n\n\n\n\n1\n0.01\n\\(0.01\\)\n\\(0.01 \\times 2^0\\)\n\n\n2\n0.02\n\\(0.01 \\times 2\\)\n\\(0.01 \\times 2^1\\)\n\n\n3\n0.04\n\\(0.01 \\times 2 \\times 2\\)\n\\(0.01 \\times 2^2\\)\n\n\n4\n0.08\n\\(0.01 \\times 2 \\times 2 \\times 2\\)\n\\(0.01 \\times 2^3\\)\n\n\n5\n0.16\n\\(0.01 \\times 2 \\times 2 \\times 2 \\times 2\\)\n\\(0.01 \\times 2^4\\)\n\n\n\nOnce we have the formula, it is very easy to calculate the area covered by water lillies on the first 10 days. We will use vectorization to do this:\n\ndays &lt;- 1:10\narea &lt;- 0.01 * 2^(days - 1)\narea\n\n [1] 0.01 0.02 0.04 0.08 0.16 0.32 0.64 1.28 2.56 5.12\n\n\nThis calls out for a plot. We will talk about visualizations more extensively on Day 5, but for now, we will use a very basic and simple function to plot the area of the lily on the first 30 days. The function is called plot() and can be used to plot a graph of two vectors. The first vector is the days, the second vector is the area.\n\nplot(days, area, type=\"b\")\n\n\n\n\n\n\n\n\n\n\n\n\nplot(‚Ä¶, type=‚Äúb‚Äù)\nO-K, days and area are clear, but what is this type=\"b\"? This is a  so-called named argument6. The plot() function has many arguments, and if you want to use only some of them, you can use their names with an equal sign. You will see that a lot in the days to come. This particular argument, type, tells R what kind of plot to draw. The \"b\" stands for ‚Äúboth‚Äù and tells R to draw both points and lines. If you want only points, you can use \"p\" (or simple leave the argument out), if you want only lines, you can use \"l\".Named arguments6¬†Full disclosure: all arguments have names in R and can be named explicitely. However, some of the arguments have a default value, so we do not have to specify them unless we need them. The type argument is one of them. Others must always be specified.\nNote another thing on this plot: after day 7, the area is greater than 1. But 1 means 100%, so after day 7, the lily covers more than the entire lake. Obviously, this is not possible ‚Äì and it shows a limitation of our model. We can show it by adding a horizontal line to the plot:\n\nplot(days, area, type=\"b\")\nabline(h=1, col=\"red\")\n\n\n\n\n\n\n\n\nabline()\n\n\n\n\n\n\n\nExercise 1.12 (Plotting water lilies) ¬†\n\nCreate the same plot using the plot() function, but add, as a parameter, col=\"blue\". What happens?\nNow add a parameter pch=19. What happens?\nUse the argument xlab to label the x-axis with ‚ÄúDay‚Äù. Use the argument ylab to label the y-axis with ‚ÄúArea‚Äù. Use the argument main to give the plot a title.\nUse the argument ylim=c(0, 1) to change the range of the y-axis. How would you change the limit of the x axis?\nWhat is the formula for the area of the lily assuming that each day, the lily covers 1.75 times the area of the previous day?\nCreate a new area vector (call it area_slow) which will be calculated with the new formula.\nAdd the new vector to the plot using the function lines(). What does the lines() function do? (Hint: type ?lines in the console).\n\n(Solution)\n\n\n\n\n\n\n1.6.2 Functions in R\nR is a so-called functional language. This is different from many other languages (including Python). It has some interesting implications which we will partially explore over the next few days. For now, however, we will be content with one important statement: in R, most of the stuff you do, you do using functions. A function takes zero or more arguments and returns exactly one argument.\nDuring this course, we will not really discuss or require creating your own functions. However, we would nonetheless like to show you how it is done. There are two reasons for that. Firstly, it is really, really easy. Secondly, it will help you understand how functions work in R, and that will help you understand how to use functions that others have created.\nIn the water lilies example we have used a formula to calculate the area of the lily on the \\(n\\)-th day. The formula includes three parameters: the initial fraction of the area covered by the lily on day 1, the day number and the factor by which the area is increased each day. We will now create a function that takes two parameters: the day (or days) and the factor, and return the area of the lily on that day. Here is how you can do it using function() keyword in R: function() { ... }\n\narea_lily &lt;- function(day, fct) {\n  ret &lt;- 0.1 * fct^(day - 1)\n  return(ret)\n}\n\nAs you can see, the function is created using the function keyword. In parentheses (( and )), you specify the arguments that the function takes, separated by commas. Then comes the body of the function, enclosed in the curly braces ({ and }). On the last line of the function code, the return() function is used to return the value of the area_lily function.\nOnce you have run the code above, you can use it to calculate the area of the lily on the first 10 days like this:\n\narea &lt;- area_lily(1:10, 2)\narea\n\n [1]  0.1  0.2  0.4  0.8  1.6  3.2  6.4 12.8 25.6 51.2\n\n\nOne interesting and important fact about the defining the functions is that you use the assignment operator &lt;- to assign the function to a variable. In other words, area_lily is, in fact, a variable! A variable which holds not a value or character string, but computer code that can be used to do stuff. You can copy it to another variable and it will behave exactly as the original function:\n\narea_lily2 &lt;- area_lily\narea_lily2(1:10, 2)\n\n [1]  0.1  0.2  0.4  0.8  1.6  3.2  6.4 12.8 25.6 51.2\n\n\n\n\n\n\n\n\n\nExercise 1.13 (Creating your own function) Modify the area_lily function so that it takes three arguments: the day, the initial fraction and the factor. Use the new function to calculate the area of the lily on the first 10 days with the initial fraction of 0.001 and the factor of 1.5. What is the area on the 10th day?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\narea_lily &lt;- function(day, fct, start) {\n  ret &lt;- start * fct^(day - 1)\n  return(ret)\n}\n\narea_lily(1:10, 1.5, 0.001)\n\n [1] 0.00100000 0.00150000 0.00225000 0.00337500 0.00506250 0.00759375\n [7] 0.01139062 0.01708594 0.02562891 0.03844336",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#coding-practices",
    "href": "day1-intro-to-r.html#coding-practices",
    "title": "1¬† First day of R",
    "section": "1.7 Coding practices",
    "text": "1.7 Coding practices\n\n1.7.1 Computer programs as means for communication\nIt is now time to conclude today‚Äôs lesson with a bit of philosophy. When you write an R script, the first goal you have in mind is to analyse your data ‚Äì in other words, by means of what you write you are trying the computer to do something for you. That is correct and fine, but there is an important aspect of programming that is often overlooked.\nWhen you write a program, you are writing it not only for the computer, but also for other people. These other people may include your colleagues, readers of your scientific articles, your students, and, last but not least, a future version of yourself. All these people need more then just a piece of code that works. You will quickly find it out yourself when you open a script or a project that you have not been working on for a few months ‚Äì trust me on this, you will not know what it does, how it does and sometimes even whether you have written it or copied from somewhere.\nYou might be thinking that you are never willing to show your code to another person. You are wrong, and not only because it is useful to you for another person to review your code. Firstly, you will want to share your code because as a scientist you will want to share your results, and results are nothing if the methods to obtain them are unknown. And secondly, you will need to share your code because you will be asked to do so by your colleagues (yes, I was as surprised as you will be when I was asked to share my code for the first time). And thirdly, your code is part of your methods and you will have to share it when you publish your results7.\n7¬†Top journals already require that you share your code when you publish your results. This will become more and more common in the future.For communication with another human being to be efficient, you need to make it as clear as possible. There are several ways how to make your code more readable and understandable. Here are some of them.\nComments. Comments are lines in your code that are not executed. In R, they start with a # sign. Comments help to explain what exactly are you  trying to achieve with your code. The old saying goes: ‚ÄúCode tells you how, comments tell you why‚Äù. You can hardly overdo with comments, but you can easily underdo.Comments with #\nNaming. The names of your variables, functions and files should be meaningful. If you have a variable that stores the number of days, call it days, not x. If you have a function that calculates the area of a circle, call it calculate_circle_area(), not f(). If you have a comma separated values (CSV) file that contains the CRP values, call it crp_values.csv, not data.txt. This is sometimes much more difficult than it looks, but it is very important. Also, that does not mean that you can‚Äôt use short names ‚Äì but use it only for ‚Äúthrowaway‚Äù variables that are used only once or twice, or for example code.\nFormatting. Your code should be formatted in a consistent way. For example, you should always put a space around your operators, like x &lt;- 2 (and not x&lt;-2), you should always put a space after a comma, like c(1, 2) and not c(1,2) (and also not c( 1 , 2 )). Lines should not be too long ‚Äì 80 characters at most is a good rule of thumb. If a line is too long, you can split it into several lines ‚Äì R will not mind. See here for a more detailed guide on how to format your code.\n\n\n1.7.2 Example\nThe following fragment of code shows how you should not format your code:\n\na&lt;- 4\nb &lt;-c(1,10, \n20, 21, 5)\nr&lt;-sqrt(sum((b-mean(b))^2)/\n               a)\n\nThe code is correct, but it is hard to read. What does it do, quickly? If you carefuly examine it, you will see that it calculates the standard deviation of the vector b, following the formula\n\\[SD = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\]\nwhere \\(n\\) is the length of the vector b, \\(x_i\\) is the \\(i\\)-th element of the vector b and \\(\\bar{x}\\) is the mean of the vector b. However, there are several issues.\nFirstly, there are no comments in the code which would give a hint of what it does. Secondly, \\(n - 1\\) (the variable a) is hard encoded - if you modify the vector b by adding one number, the code will execute, but the result will be incorrect.\nThirdly, line 4 combines several operations making it very hard to read. It should be split for clarity. The following code is much more readable:\n\n# ---------------------------------------------------------\n# Calculating the standard deviation of a sample\n# ---------------------------------------------------------\n\n# example values for five samples\nsamples &lt;- c(1, 10, 20, 21, 5)\nsamples_n &lt;- length(samples)\n\n# calculate standard deviation of samples manually\nsamples_mean &lt;- mean(samples)\nsamples_devs &lt;- samples - samples_mean\n\n# samples variance\nsamples_var &lt;- sum(samples_devs^2) / \n                (samples_n - 1)\n\nsamples_sd &lt;- sqrt(samples_var)\n\nThis makes it absolutely clear what you are trying to do, and, in addition, calculates the mean, the deviations, sample length and sample variance ‚Äì all of which might come in handy later on. Also note of the use of # ----‚Ä¶ comments. Programmers often use these to highlight the beginning of a new section of code. This is not necessary, but adds to readability. Lines 14 and 15 show how you can split a line of code in a way that is both readable and clear.\nOf course, the example is a bit silly ‚Äì R has a lot of built-in statistical functions, and standard deviation naturally is one of them. You can calculate the standard deviation of a vector b using the sd() function: sd()\n\nsamples_sd &lt;- sd(samples)\n\nNonetheless, the principle stands.\n\n\n1.7.3 Tab completion\nOK, so you need to use explicit variables. Isn‚Äôt that a bit cumbersome? A lot of typing? An opportunity to make typos?\nNot really, thanks to a bit of magic called ‚Äútab completion‚Äù. Say, you have defined a variable samples_sd: Tab completion\n\nsamples_sd &lt;- sd(samples)\n\nNow you would like to display it. You start typing sam and press the tabulator (Tab) key or simply wait for an eyeblink, and then R will show you a list of all variables and functions that start with sam. You can then use the arrow keys to select the one you want, and press Enter or Tab to insert it into your code.\nThis is an extremely useful feature. You will use it a lot and there is way more to it. For example, if you start typed a function name with the opening parentheses, like sd(, then pressing ‚ÄúTab‚Äù will show you all available arguments of the function, and you can scroll through them to see what they do. Go on, try it!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#review",
    "href": "day1-intro-to-r.html#review",
    "title": "1¬† First day of R",
    "section": "1.8 Review",
    "text": "1.8 Review\nThings that you learned today:\n\nWorking with R and RStudio\n\nCreating and running scripts\nUsing the console\nUsing the ‚ÄúEnvironment‚Äù panel of RStudio\nUsing comments with the # sign\nUsing tab completion\n\nVariables\n\nAssigning values to variables\nUsing variables in calculations\nUsing character variables\npasting together character variables with paste() and paste0()\n\nVectors\n\nCreating vectors with c() and :\nAccessing elements of a vector with [ ]\nNamed vectors\nAssigning values to selected elements\nVectorization\nRecycling\n\nUseful functions\n\nsqrt(), log(), log2(), log10(), sin(), cos(), tan()\nsum(), mean(), min(), max(), range(), sd(), var(), median(), quantile()\n\nOther\n\nThe special values NA, Inf, -Inf, NaN\nLogical values TRUE and FALSE\nFunctions in R\nPlotting with plot()\nAdding parameters to functions\nCoding practices\n\n\nXXX",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First day of R</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html",
    "href": "day2-complex-structures.html",
    "title": "2¬† Complex data structures",
    "section": "",
    "text": "2.1 Goals for today",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#goals-for-today",
    "href": "day2-complex-structures.html#goals-for-today",
    "title": "2¬† Complex data structures",
    "section": "",
    "text": "Complex data structures (matrices, lists and data frames)\nLogical vectors and subsetting",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#logical-vectors",
    "href": "day2-complex-structures.html#logical-vectors",
    "title": "2¬† Complex data structures",
    "section": "2.2 Logical vectors",
    "text": "2.2 Logical vectors\n\n2.2.1 Truth and falsehood\nYesterday, we mentioned briefly two special values in R: TRUE and FALSE. These are logical constants, and they are used to represent truth and falsehood, respectively. Using these values, it is possible to create logical vectors ‚Äì vectors that contain only these two values.\n\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\nThe FALSE value is equivalent to 0, and TRUE is equivalent to 1. That means, we can use logical vectors in arithmetic operations. One very common application of this is using sum() to count the number of TRUE values in a logical vector.\n\nsum(logical_vector)\n\n[1] 3\n\n\nLogical vectors are incredibly useful, because they can be used to subset other objects. For example, if you have a vector of numbers, you can use a logical vector to select some of the elements.\n\nnumbers &lt;- 1:5\nnumbers[ logical_vector ]\n\n[1] 1 3 4\n\n\nThe real usefulness of logical vectors comes when you create them using different operators that check for equality, inequality, etc. Let‚Äôs look at that in more detail.\n\n\n\n\n\n\nTRUE, FALSE and T and F\n\n\n\nIn R, TRUE and FALSE are the only two logical constants. However, there are also two other constants, T and F, which are equivalent to TRUE and FALSE, respectively. Do not use them. Unlike TRUE and FALSE, they can be overwritten, which can lead to chaos and mayhem.\n\n\n\n\n2.2.2 Comparison operators\nThere are six comparison operators in R:\n\n== ‚Äì equality\n!= ‚Äì inequality\n&gt; ‚Äì greater than\n&lt; ‚Äì less than\n&gt;= ‚Äì greater than or equal to\n&lt;= ‚Äì less than or equal to\n\nThey are vectorized like any other arithmetic operators, but their result is not a number ‚Äì but a logical value. Take a look:\n\nnumbers &lt;- c(42, 3, -17, 0, -2, 1)\nnumbers &gt; 0\n\n[1]  TRUE  TRUE FALSE FALSE FALSE  TRUE\n\n\nFor each element of the vector, R checks if it is greater than zero. If it is, it returns TRUE, otherwise FALSE, producing, in the end, a vector containing as many elements as there were in the vector numbers. This logical vector can be used to subset the original vector.\n\n# prints only numbers greater than 0\nnumbers[ numbers &gt; 0 ]\n\n[1] 42  3  1\n\n# prints only numbers different from 0\nnumbers[ numbers != 0 ]\n\n[1]  42   3 -17  -2   1\n\n\nBut wait, there is more. There is a number of functions that check the elements of a vector and return logical vectors. One of the most commonly used and most useful of these is the is.na() function, which checks if an element is ‚Äúnot available‚Äù. This will come in handy later, when we start reading data from files ‚Äì data which is full of NA‚Äôs!\n\nnumbers &lt;- c(42, 3, NA, 0, -2, 1)\nis.na(numbers)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE\n\n\nBut hey, this does not tell us which elements are NA‚Äôs. It is easy to see in the example above, but what if we have a vector with a million elements? Actually, to answer which elements are NA‚Äôs you can simply use the which() function:\n\nwhich(is.na(numbers))\n\n[1] 3\n\n\nFine, but what about the numbers which are not NA? What if we want to find all ‚Äúgood‚Äù numbers and store them for future use? In this case, we can use the ! operator, which negates the logical vector. That is, each TRUE becomes FALSE and each FALSE becomes TRUE.\n\nnas &lt;- is.na(numbers)\nnas\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE\n\n# change TRUE to FALSE and FALSE to TRUE\n!nas\n\n[1]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\nuseful_numbers &lt;- numbers[!nas]\nuseful_numbers\n\n[1] 42  3  0 -2  1\n\n\nThere is one more thing to mention here. The comparison operators == and != can be used to compare strings as well1.\n1¬†Actually, they can be used to compare any two objects in R. Also, the &gt; and &lt; operators can be used to compare strings, but the result is not always what you might expect. Can you guess what it does? Hint: pick up a dictionary‚Ä¶ or any other alphabetically sorted list.\npatient_measurements &lt;- c(1, 16, 7, 42, 3)\npatient_gender &lt;- c(\"male\", \"female\", \"female\", \"male\", \"female\")\npatient_measurements[ patient_gender == \"male\" ]\n\n[1]  1 42\n\n\n\n\n\n\n\n\n\nExercise 2.1 (Creating logical vectors) Create a vector with 50 random numbers as follows:\n\nvec &lt;- rnorm(50)\n\nHow can you filter out all the numbers that are greater than 0.5?\nHow can you filter out all the numbers that are (i) greater than 0.5 and (ii) smaller than 1.0?\nHow many numbers that are greater than 0.5 are in your vector?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# over 0.5\nover05 &lt;- vec[ vec &gt; 0.5 ]\n\n# between 0.5 and 1.0\nbetween &lt;- over05[ over05 &lt; 1.0 ]\n\n# how many numbers are greater than 0.5?\nsum(vec &gt; 0.5)\n\n[1] 13",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#matrices",
    "href": "day2-complex-structures.html#matrices",
    "title": "2¬† Complex data structures",
    "section": "2.3 Matrices",
    "text": "2.3 Matrices\n\n2.3.1 Creating matrices\nMatrices are just what says on the box: 2-dimensional structures; just like in mathematics. They behave in a very similar way to vectors in R, for example, they always hold elements of the same type (either numeric, character, etc., but never elements of both types). If you have your data stored in an Excel spreadsheet, chances are that different columns have different types of data. You can hardly use matrices in such a case. In fact, you will probably rarely use matrices in R, at least at the beginning ‚Äì however, they are very useful for storing large data sets, for example from a transcriptomic analysis. For storing ‚ÄúExcel-like‚Äù data you will be using lists and data frames, which we will discuss later today. Nonetheless, we will spend some time on matrices today ‚Äì because 90% of what you will learn today about matrices you will be able to use with data frames as well.\nCreating a matrix is very simple. You can use the matrix() function, which takes a vector as input and reshapes it into a matrix:\n\nmtx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nprint(mtx)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nAs you remember from yesterdays course, the 1:12 is a vector of numbers from 1 to 12. The nrow and ncol parameters specify the number of rows and columns, respectively. You don‚Äôt really need to specify both of them ‚Äì if you specify only one, R will calculate the other one for you.\nAs you have noticed above, by default the matrix() function fills the matrix by columns. If you want to fill it by rows, you can use the byrow parameter:\n\nmtx &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nprint(mtx)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nHere we use a special value ‚Äì TRUE ‚Äì which is a logical constant. It is equivalent to 1, but it is more readable. You can also use FALSE (or 0) to specify that you want to fill the matrix by columns (this is the default).\nIt is also possible to create a large matrix by passing only one value (typically 0 or NA) and specifying the number of rows and columns. In that case, you have to specify both nrow and ncol.\n\nmtx &lt;- matrix(0, nrow = 3, ncol = 4)\n\nYou can also create a matrix by binding vectors together, either by rows or by columns. The rbind() function binds vectors by rows, and the cbind() function binds vectors by columns. Always make sure that the vectors you bind have the same length and the same element type.\n\na &lt;- 1:3\nb &lt;- 4:6\nmtx &lt;- rbind(a, b)\nprint(mtx)\n\n  [,1] [,2] [,3]\na    1    2    3\nb    4    5    6\n\nmtx &lt;- cbind(a, b)\nprint(mtx)\n\n     a b\n[1,] 1 4\n[2,] 2 5\n[3,] 3 6\n\n\n\n\n\n\n\n\nMatrices and algebra\n\n\n\nR matrices are very powerful for linear algebra operations. If you ever learned linear algebra, you will find that R matrices can do pretty much everything you learned in class. For example, you can multiply matrices, transpose them, invert them, calculate determinants, etc. We will not cover these operations in this course.\n\n\nJust like vectors have a length which you can check with the length() function, matrices have dimensions ‚Äì the number of rows and the number of columns. You can access them using the dim() function, which returns a vector of length 2 (row and column number), and with functions nrow() and ncol(), which return the number of rows and columns, respectively.\n\ndim(mtx)\n\n[1] 3 2\n\nnrow(mtx)\n\n[1] 3\n\nncol(mtx)\n\n[1] 2\n\n\n\n\n2.3.2 Accessing matrix elements\nFor vectors, we have used the square brackets ([]) to access elements. Same with matrices, really, however we have two dimensions now. Think about that: with vectors we could only select one or more elements. With matrices, it should be possible to select an element, a number of elements from a row (or the whole row), a number of elements from a column (or the whole column), or even a submatrix. All this is possible using the square brackets.\n\nmtx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nprint(mtx)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n# Accessing the element in the third row and the second column\nmtx[3, 2]\n\n[1] 6\n\n# Accessing first three numbers in the second column\nmtx[1:3, 2]\n\n[1] 4 5 6\n\n# Accessing second to fourth numbers in the first row\nmtx[1, 2:4]\n\n[1]  4  7 10\n\n\n\n\n\n\n\n\nRows and columns\n\n\n\nWhen we talk about plotting, the first dimension, ‚Äúx‚Äù, is usually the horizontal one, and the second dimension, ‚Äúy‚Äù, is the vertical one. However, in R matrices, just like in real algebra, the first dimension corresponds to the rows, and the second dimension corresponds to the columns. You need to get used to it ‚Äì it‚Äôs the same for data frames which you will be using extensively.\n\n\nIf you select more then one row and more than one column, you will get a new matrix ‚Äì albeit smaller than the original one.\n\n# Selecting the first two rows and the last two columns\n# This will create a 2 x 2 matrix\nmtx[1:2, 3:4]\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n\n\nIf you want to access whole rows or columns, you do not need to specify anything ‚Äì simply leave an empty space before (for selecting whole columsn) or after (for selecting whole rows) the comma.\n\n# Selecting the whole second row\nmtx[2, ]\n\n[1]  2  5  8 11\n\n# Selecting the whole third column\nmtx[, 3]\n\n[1] 7 8 9\n\n# Row 1 and three - returns a matrix\nsel &lt;- c(1, 3)\nmtx[sel, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    3    6    9   12\n\n\nThis last example shows that just like with vectors, we can use a variable to make our selection.\n\n\n\n\n\n\nRemember!\n\n\n\n\nRows first, then columns\nIf you select a single column or a single row, you will get a vector\nIf you select more than one row or column, you will get a (smaller) matrix\nIf you select more rows or columns than are present, you will get a ‚Äúsubscript out of bonds‚Äù error\nVectors and matrices always have only one data type (string, numerical, logical etc.)\n\n\n\n\n\n2.3.3 Row and column names\nJust like in case of named vectors, we can name rows and columns of a matrix. However, for this we need two different functions: rownames() and colnames() (for row names and column names, duh).\n\nrownames(mtx) &lt;- c(\"first\", \"second\", \"third\")\ncolnames(mtx) &lt;- LETTERS[1:4]\n\nmtx[\"first\", ]\n\n A  B  C  D \n 1  4  7 10 \n\nmtx[ , \"A\"]\n\n first second  third \n     1      2      3 \n\n\nIn the code above, we use the LETTERS constant, which is a vector containing all the letters of the English alphabet. Just like constants pi and e, LETTERS is available in R by default, along with its lower-case counterpart, letters. It is useful for labeling.\nUnfortunately, we only have 26 letters in the alphabet, so what can we do with a matrix that has more columns than that? Well, we can use the same trick that Excel uses: after Z, we have AA, AB, etc.\nTo create such a long vector, we will use two functions: rep() and paste0().\n\n\n2.3.4 Using rep() to generate column names\n The rep() function is a little and very useful utility function that repeats element of a vector a given number of times. It either repeats the whole vector several times, or, using the each parameter, repeats each element a given number of times:rep()\n\nabc &lt;- LETTERS[1:3] # A, B, C\nabc3 &lt;- rep(abc, 3)\nabc3\n\n[1] \"A\" \"B\" \"C\" \"A\" \"B\" \"C\" \"A\" \"B\" \"C\"\n\na3b3c3 &lt;- rep(abc, each = 3)\na3b3c3\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n\nIn the code above, we created two vectors, each of the length \\(3 \\times 3\\); first one goes ‚ÄúA, B, C, A, B, C, ‚Ä¶‚Äù, and the second one goes ‚ÄúA, A, A, B, B, ‚Ä¶‚Äù. To get at our goal, we would have to paste together the first element from the first vector with the first element from the second vector etc.:\n\n\n\na3b3c3\nabc3\nresult\n\n\n\n\nA\nA\nAA\n\n\nA\nB\nAB\n\n\nA\nC\nAC\n\n\nB\nA\nBA\n\n\nB\nB\nBB\n\n\nB\nC\nBC\n\n\n\nTo do this, we will use the paste0() function, which concatenates strings and is vectorized, so it does exactly what we need.\n\ncol_names &lt;- paste0(abc3, a3b3c3)\n\nOf course, we need all the letters (we used only three in the example above for demonstration purposes).\n\nn &lt;- length(LETTERS)\nabc3 &lt;- rep(LETTERS, n)\na3b3c3 &lt;- rep(LETTERS, each = n)\ncol_names &lt;- paste0(abc3, a3b3c3)\nlength(col_names)\n\n[1] 676\n\nhead(col_names)\n\n[1] \"AA\" \"BA\" \"CA\" \"DA\" \"EA\" \"FA\"\n\ntail(col_names)\n\n[1] \"UZ\" \"VZ\" \"WZ\" \"XZ\" \"YZ\" \"ZZ\"\n\n\nThe head() and tail() functions are very useful for inspecting the beginning and the end of a very large object such as a vector, matrix, or data frame. They are very useful for checking if the operation you just performed did what you expected it to do.\n\n\n\n\n\n\n\nExercise 2.2 (Creating column names) Repeat the procedure above, but generate column names for a matrix with more than a 1000 columns. Use the LETTERS constant, but rather then generating two-letter column names, generate three-letter column names: AAA, AAB, AAC, ‚Ä¶, ABA, ABC, ‚Ä¶, ZZZ. Store the result in a variable called col_names.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.3 (Matrices - accessing and changing elements) Assume you have a 48 well-plate for a drug sensitivity analysis with viability scores.\n\nCreate a 48-element vector ‚ÄúdrugSensitivity_v‚Äù with random numbers between 0 and 1. Use runif(48) to generate these values. These reflect your viability scores.\nWhat does the runif() function do?\nCreate a 6x8 matrix (6 rows, 8 columns) ‚ÄúdrugSensitivity‚Äù from the vector.\n\nBefore starting you experiment, you decided to leave out the border wells to avoid edge effects:\n\nChange the values of all the border elements to NA.\n\nThe rows are treated with inhibitor 1 with increasing concentrations (control, low, medium, high). Columns 2 to 4 are treated with inhibitor 2 with increasing concentrations (control, low, high) and column 5 to 7 are treated with inhibitor 3 (same concentrations as inhibitor 2).\n\nUse row and column names to reflect treatments.\nSelect all wells with inhibitor 3.\nSelect only wells with a combination of inhibitor 1 and inhibitor 2.\n\n(Solution)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#lists",
    "href": "day2-complex-structures.html#lists",
    "title": "2¬† Complex data structures",
    "section": "2.4 Lists",
    "text": "2.4 Lists\n\n2.4.1 Creating lists\nThe objects that we have discussed so far ‚Äì vectors and matrices ‚Äì can only hold one type of data; you cannot mix strings with numbers, for example. This is obviously a problem ‚Äì quite frequently you need to store both numbers and strings in one object. This is where lists come in.\nLists are created using the list() function. Lists have elements, just like vectors; but unlike vectors, every element can be of any possible type. It can be a vector, of course, but can also be a matrix, a data frame, even a function ‚Äì or another list. Actually, it is quite common to have a list of lists (or even list of lists of lists) in R.\n\nlst &lt;- list(numbers=1:3, strings=c(\"a\", \"bu\"), \n            matrix = matrix(1:4, nrow = 2), \n            logical = c(TRUE, FALSE))\nlst\n\n$numbers\n[1] 1 2 3\n\n$strings\n[1] \"a\"  \"bu\"\n\n$matrix\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n$logical\n[1]  TRUE FALSE\n\n\n\n\n2.4.2 Accessing elements of a list\nLike vectors, lists can be named. In fact, they very often are. However, accessing them is a bit different than with vectors.\nYou can use [ and ] square brackets to access elements of a list, but this produces another list, containing only the selected elements. Thus, if you type lst[\"numbers\"], you will get a list with one element, which is the vector of numbers:\n\nlst[\"numbers\"]\n\n$numbers\n[1] 1 2 3\n\n\nYou can see that it is a list because of this weird $ (dollar) sign, which we will discuss in a moment. However, you can also check its type directly:\n\ntypeof(lst[\"numbers\"])\n\n[1] \"list\"\n\n\nIf, however, you want to work with the actual vector that is stored in the ‚Äúnumbers‚Äù slot of the list, you need to use one of two approaches. First approach is to use double square brackets:\n\nlst[[\"numbers\"]]\n\n[1] 1 2 3\n\ntypeof(lst[[\"numbers\"]])\n\n[1] \"integer\"\n\n\nThat requires a lot of typing, four times square brackets and then, in addition, the quote marks. But programmers are lazy, and therefore, we have a shortcut: the $ sign. It is used to access elements of a list by name:\n\nlst$numbers\n\n[1] 1 2 3\n\n\nYou will use this construct a lot in R.\nThe elements of a list behave exactly like regular variables. If an element is a vector, you can do with it all the things you can do with a vector; if it is a matrix, you can treat is as a matrix (because it is a matrix).\n\npatient_data &lt;- list(name = \"John Doe\", age = 42, \n                     measurements = runif(5))\npatient_data\n\n$name\n[1] \"John Doe\"\n\n$age\n[1] 42\n\n$measurements\n[1] 0.4941126 0.6420069 0.1438457 0.3685637 0.4824686\n\npatient_data$measurements[1]\n\n[1] 0.4941126\n\npatient_data$measurements[1] &lt;- 42\npatient_data$measurements * 3\n\n[1] 126.0000000   1.9260207   0.4315372   1.1056912   1.4474058\n\n\nSince the lists are named, there must be a way to access and modify these names. And, of course, there is: the names() function.\n\nnames(patient_data)\n\n[1] \"name\"         \"age\"          \"measurements\"\n\nnames(patient_data) &lt;- c(\"patient_name\", \"patient_age\", \"patient_measurements\")\nnames(patient_data)[3] &lt;- \"crp_measurement\"\n\n\n\n\n\n\n\nTab completion with lists and data frames\n\n\n\nIf you type the name of your data frame variable in a script, the $ and press the TAB key, RStudio will show you all the elements of a list (or columns of the data frame) to choose from. No need for tedious typing!\n\n\n\n\n\n\n\n\n\nExercise 2.4 (Lists) Create a list called misc containing the following elements:\n\nvector, a vector of numbers from 1 to 5\nmatrix, a matrix with 2 rows and 3 columns, filled with numbers from 1 to 6\nlogical, a logical vector with three elements: TRUE, FALSE, TRUE.\nperson ‚Äì another list, which contains your first (first) and last (last) name\n\nWhat happens when you type misc[2:4]?\nClick on the misc list in the Environment tab in RStudio. What do you see? Does it make sense?\n\n\n\n\n\n\n2.4.3 Lists as return values\nA common application of lists has something to do with functions. Remeber that a function can return only one object? But what if a function would like to return several things at once? It can return a list!\nWe will run now our first statistical test. First, we need to generate two groups of measurements to compare. We will simulate them using the rnorm() function which produces normally distributed random numbers. The function takes additional parameters, mean and sd, which specify the mean and the standard deviation of the distribution, respectively. That allows us to ensure that the groups differ:\n\ngroup_a &lt;- rnorm(10, mean = 10, sd = 2)\ngroup_b &lt;- rnorm(10, mean = 14, sd = 2)\n\nOf course, before running any statistical test we usually want to have a look at the data, to see if the groups differ visually. We can do this by using the boxplot() function, which creates a boxplot of the data. boxplot()\n\nboxplot(group_a, group_b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\n\n\nBoxplots are a great way to visualize the data. The whiskers show the minimum and maximum values (excluding outliers, which are shown as separate points), the box shows the interquartile range (25th to 75th percentile), and the thick line in the middle of the box shows the median. There are better ways, which we will discuss on Day 5, but still, boxplots are pretty cool.\n\n\nOK, now we run a t-test. We will use the t.test() function for that, and store the result in a variable called t_test.\n\nt_test &lt;- t.test(group_a, group_b)\ntypeof(t_test)\n\n[1] \"list\"\n\n\nAs you can see, the result is a list. However, when we print it to the console, it does not look like one:\n\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  group_a and group_b\nt = -4.9269, df = 15.666, p-value = 0.0001611\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.708389 -1.872092\nsample estimates:\nmean of x mean of y \n 9.997091 13.287332 \n\n\nThis is because R has a special function for formatting the results of a t-test2, so that it is easier to read.\n2¬†OK, this is way beyond the scope of this course, but the result returned by t.test has the class ‚Äúhtest‚Äù. This is a list, but also it is something special, which is why R knows to print it in a different way. R is a functional language, but it also allows object oriented (OO) programming.Nonetheless, we can access the elements of the list in the usual way:\n\nt_test$p.value\n\n[1] 0.0001611309\n\n\n\n\n2.4.4 Replacing, adding and removing elements of a list\nYou can assign elements to a list using the $ sign. If the element does not exist yet in the list, it will be created; if it does, it will be replaced.\nTo remove an element, you need to use the special value NULL.\n\nperson &lt;- list(name = \"January\", age = 117, pets=c(\"cat\", \"dog\"))\n\n# change the age\nperson$age &lt;- 118\n\n# add a new element\nperson$city &lt;- \"Hoppegarten\"\n\n# remove pets\nperson$pets &lt;- NULL\nperson\n\n$name\n[1] \"January\"\n\n$age\n[1] 118\n\n$city\n[1] \"Hoppegarten\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#data-frames",
    "href": "day2-complex-structures.html#data-frames",
    "title": "2¬† Complex data structures",
    "section": "2.5 Data Frames",
    "text": "2.5 Data Frames\n\n2.5.1 Creating data frames\nFinally, we come to possibly the most important data structure in R ‚Äì at least for us, biologists and medical researchers: data frames. Data frames are the closest thing to an Excel spreadsheet in R. They are used to store data in a tabular form, where each column can be of a different type. This makes them perfect for storing data from experiments, clinical trials, etc. You will be using them a lot.\nIn R, data frames are lists that were made to behave a lot like matrices. Thus, everything that you learned so far about lists can be applied to data frames, including accessing their elements (columns) using the $ operator. However, there are some differences between data frames and matrices.\nThe main feature of data frame that makes them a bit like matrices is the fact that each element of a data frame is a vector3 and that these vectors have always the same length. This means that one of the major differences between data frames and, say, Excel spreadsheets, is that a column of a data frame contains only elements of a single type. If a ‚Äúcell‚Äù in a data frame is a character string, then the whole column is a character vector; if it is numerical, then the whole column is a numerical vector, etc.\n3¬†Actually, it is a bit more complicated than that, but for now, let‚Äôs just say that each column of a data frame is a vector.This may seem like a limitation, but it is, in fact, a good thing. It makes data more consistent and less prone to errors.\nLike lists, data frames can be named or not, but typically they are. The names of a data frame are precisely the column names and you can access them (and modify) using both, names and colnames functions.\n\nnames &lt;- c(\"January\", \"William\", \"Bill\")\nlastn &lt;- c(\"Weiner\", \"Shakespeare\", \"Gates\")\nage   &lt;- c(NA, 460, 65)\n\npeople &lt;- data.frame(names=names, last_names=lastn, age=age)\npeople\n\n    names  last_names age\n1 January      Weiner  NA\n2 William Shakespeare 460\n3    Bill       Gates  65\n\nnames(people)\n\n[1] \"names\"      \"last_names\" \"age\"       \n\ncolnames(people)\n\n[1] \"names\"      \"last_names\" \"age\"       \n\n\nLike matrices, data frames have dimensions, which you can access using the dim(), nrow() and ncol() functions.\n\ndim(people)\n\n[1] 3 3\n\nnrow(people)\n\n[1] 3\n\nncol(people)\n\n[1] 3\n\n\nAlso, like matrices, the data frames can have rownames; however, for reasons that will be clear later, they are not as important as column names and in fact, we will not be using them4.\n4¬†Row names in R data frames are very old school. Many people still use them, and many R functions produce them. However, we will be using the packages from the tidyverse family further down the line, which ignore the rownames, and for good reasons.\n\n\n\n\n\n\nExercise 2.5 ¬†\n\nCreate a 5x3 matrix with random numbers. Use matrix and rnorm.\nTurn the matrix into a data frame. Use as.data.frame for that.\nAdd column and row names.\nAdd a column. Each value in the column should be ‚ÄúA‚Äù (a string). Use the rep function for that.\nAdd a column with five numbers from 0 to 1. Use the seq function for that. Hint: look at the help for the seq function (?seq).\n\n(Solution)\n\n\n\n\n\n\n2.5.2 Accessing and modifying columns of a data frame\nSince data frames are lists, you can access their columns using the $ operator. This is the most common way of accessing columns of a data frame:\n\npeople$names\n\n[1] \"January\" \"William\" \"Bill\"   \n\n\nYou can also access columns using the square brackets, just like with matrices, using a comma to denote the columns and rows. However, there is a fine difference between matrices and data frames: if you select a single row, you will not get a vector, but a data frame with one row. If you think about that, it makes perfect sense: the different columns can have different data types, so they cannot be easily combined into a vector without losing some information.\n\npeople[1, ]\n\n    names last_names age\n1 January     Weiner  NA\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere is an issue when you try to access a single column of a data frame. We will discuss it at length in the following, but basically, this behavior is different for different flavors of data frames: base R data frames created with data.frame() return a vector, while others may return a data frame. Watch out for this!\n\n\n\n\n2.5.3 Subsetting data frames with logical vectors\nJust like with vectors, you can use logical vectors to subset data frames. This is extremely useful and very common in R. For example, we might want to select only rows that do not contain NA values in the age column.\n\npeople_with_age &lt;- people[!is.na(people$age), ]\npeople_with_age\n\n    names  last_names age\n2 William Shakespeare 460\n3    Bill       Gates  65\n\n\nThat way we have filtered the data frame, leaving only persons with known age.\nActually, for data frames, you will commonly use the filter() function (which we will introduce tomorrow). However:\n\nthe filter() function also uses logical vectors;\nyou can subset many different data types using logical vectors (including matrices, lists, vectors, data frames), but filter() works only with data frames; and\nsometimes using logical vectors is just more convenient.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#libraries-in-r",
    "href": "day2-complex-structures.html#libraries-in-r",
    "title": "2¬† Complex data structures",
    "section": "2.6 Libraries in R",
    "text": "2.6 Libraries in R\n\n2.6.1 Installing and loading packages\nStarting tomorrow, we will cease to only use ‚Äúbase‚Äù R functions (that is, functions that are available in R ‚Äúout of the box‚Äù) and start using additional packages. Packages in R are collections of functions (and often some other things, like data sets, documents and more).\nPackages need to be installed before they can be used ‚Äì but once you have installed a package, you don‚Äôt need to install it again (unless you upgrade R or want to update the package to a newer version).\nHowever, to use a package, you also need to load it using the library() function. This you must do every time you start a new R session, because R ‚Äúforgets‚Äù which packages have been loaded the previous time. It is a bit like with the software for your operating system: you need to install your browser only once, but you have to start each each time after you start your computer.\nInstalling packages usually is straightforward, however at times it can be tricky.\n\n\n\n\n\n\npackage ‚Äò‚Äî‚Äô was built under R version x.y.z\n\n\n\nAt times you will get a warning that a package was ‚Äúbuilt‚Äù under a different version of R than the one that you are running. It is not an error (just a warning), and most of the time it can be ignored. It means what it says: that the installed package was built (e.g., compiled) with a different version of R. This can sometimes lead to problems, but most of the time it does not.\n\n\nWe will return to installing packages from different sources on Day 5.\n\n\n\n\n\n\nRemember!\n\n\n\nRemember: you need to install a package only once with install.package(), but you need to load it every time you start a new R session with library().\n\n\n\n\n\n\n\n\n\nExercise 2.6 Use install.packages to install the package skimr from CRAN. Then, load the package using library(skimr). What does that package do? How can you check that? (Hint: use ??skimr, ?skim).\n\n\n\n\n\n\n2.6.2 Data frames, tibbles & co.: different flavors of R\nIt is now time to reveal some ugly truths about R. R is open source, and everyone can modify it, add new packages etc. This is great and resulted in the vibrant user community that R has. Also, there is hardly a statistical method or framework that is not represented in R. In addition, developing and publishing new packages for R is incredibly easy, at least compared to some other languages.\nHowever, this has a downside. There are many different flavors of R, and, unfortunately, some of the most popular packages or groups of packages can clash. We will spend now some time with one particular example: data frames. Firstly, because you will be using data frames a lot, secondly, because it neatly illustrates the problem, and thirdly to introduce a new type of data ‚Äì tibbles.\nBase R data frames are created using the data.frame() function. They are useful and we use them a lot, but they have one tiny inconsistency that can cause a lot of trouble. Say, you define your data frame like this:\n\ndf &lt;- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"))\n\nWhen you access a single column of this data frame using the square brackets [ ], you will get a vector. We can check it with the is.data.frame() function:\n\nis.data.frame(df[ , 1])\n\n[1] FALSE\n\n\nThis is consistent with the behavior of matrices (where one column or one row becomes a vector), but not with the behavior of the data frames themselves: because when you access a single row, you are getting a data frame, not a vector:\n\nis.data.frame(df[1, ])\n\n[1] TRUE\n\n\nThis inconsistency can mess up your code. Imagine that you have somehow automatically selected some columns of a data frame ‚Äì for example, by selecting columns that start with a certain letter (we will learn how to do that on Day 4). You store it in the variable called sel_cols. You can select the columns from the data frame using the df[ , sel_cols] syntax. However, depending on whether there was a single column selected or more, you will get either a vector or a data frame. This is annoying and in the worst case scenario, it can break your program.\nMany people noticed this, and proposed solutions. In R, it is possible to take a class of an object (like data.frame) and modify it. One of such most commonly used modifications is called a tibble and has been implemented in the Tidyverse group of packages, which we will be using extensively in the days to come.\n\n\n2.6.3 Data frames and tibbles\nThe Tidyverse data frame is called a tibble. It behaves almost exactly like a data frame, with a few crucial differences:\n\nwhen printing the tibble to the console, the output is nicer\ntibbles never have row names (and that is why we will not be using row names)\nwhen you access a single column of a tibble, you always get a tibble, not a vector\n\nTo use tibbles, you need the tidyverse package5. You can install it using install.packages(\"tidyverse\") and load it using library(tidyverse).\n5¬†Actually, the tidyverse package is a meta-package: it just loads a collection of packages that are often used together. The tibble package which defines tibbles is one of them.\nlibrary(tidyverse)\ntbl &lt;- tibble(a = 1:3, b = c(\"a\", \"b\", \"c\"))\ntbl\n\n# A tibble: 3 √ó 2\n      a b    \n  &lt;int&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 c    \n\nis.data.frame(tbl)\n\n[1] TRUE\n\nis_tibble(tbl)\n\n[1] TRUE\n\n\nAs you can see, a tibble is both a data frame and a tibble. You can use tibbles as a drop-in replacement for data frames. We will be seeing it a lot, because many useful functions from the Tidyverse family produce tibbles, not data frames. However, for you, as the user, the differences will be mostly cosmetic. So far, so good.\nHowever, if you are reading this book, chances are that you are a biologist or medical researcher, and that means that sooner or later you will be using packages from the Bioconductor project. Bioconductor is a collection of R packages for bioinformatics, genomics, and related fields. They are incredibly valuable, you can hardly do bioinformatics without them. However, Bioconductor defines its own alternative to data.frames, called DataFrame. It is a bit different from data frames, and it is not compatible with tibbles. If you want to process DataFrames produced by Bioconductor packages with Tidyverse functions, you need to convert them to a regular data.frame using the as.data.frame() function.\nThe code below will not work until you install the Bioconductor package S4Vectors. Don‚Äôt worry if it does not work ‚Äì we will not be using Bioconductor in this course, but I wanted to show you where the problem is.\n\nlibrary(S4Vectors)\nDF &lt;- DataFrame(a = 1:3, b = c(\"a\", \"b\", \"c\"))\nDF\n\nDataFrame with 3 rows and 2 columns\n          a           b\n  &lt;integer&gt; &lt;character&gt;\n1         1           a\n2         2           b\n3         3           c\n\nis.data.frame(DF)\n\n[1] FALSE\n\n\nAs you can see, the DataFrame object is not a data.frame. And this means that the Tidyverse function filter() does not see it as one (you will learn about the filter() function on Day 4):\n\nlibrary(tidyverse)\nfilter(DF, a &gt; 1)\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"c('DFrame', 'DataFrame', 'SimpleList', 'RectangularData', 'List', 'DataFrame_OR_NULL', 'Vector', 'list_OR_List', 'Annotated', 'vector_OR_Vector')\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#review",
    "href": "day2-complex-structures.html#review",
    "title": "2¬† Complex data structures",
    "section": "2.7 Review",
    "text": "2.7 Review\nThings you learned today:\n\nLogical vectors:\n\nsubsetting with logical vectors\nusing comparison operators like &gt; to create logical vectors\nusing the is.na() function to check for NA values\nusing the which() function to find the positions of TRUE values\nusing the ! operator to negate logical vectors\n\nMatrices:\n\ncreating matrices using the matrix() function\nmeasuring matrices with dim(), nrow() and ncol()\nrows first, then columns\naccessing elements, rows, columns and submatrices of a matrix\nnaming rows and columns of a matrix\n\nLists:\n\ncreating lists using the list() function\naccessing elements of a list using [, [[ and $\nlists as return values from functions\nreplacing, adding and removing elements of a list\n\nData frames:\n\ncreating data frames using the data.frame() function\naccessing and modifying column names of a data frame\naccessing and modifying elements of a data frame\nsubsetting data frames\nadding and removing columns from a data frame\nmerging data frames\ncreating tibbles with Tidyverse and tibble()\n\nOther:\n\nconstant vectors LETTERS and letters\nusing the rep() function\ngenerating random numbers with rnorm() and runif()\ngenerating sequences with seq()\nrunnning a t-test using function t.test()\nmaking a boxplot with boxplot()\nthe special value NULL\nconverting matrices to data frames with as.data.frame()\ninstalling packages with install.packages()\nloading packages with library()",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html",
    "href": "day3-reading-your-data.html",
    "title": "3¬† Reading and Writing Files",
    "section": "",
    "text": "3.1 Aims for today\nToday is a special day. If there is one thing that I would like you to learn from this course, it is how to read and write the data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#aims-for-today",
    "href": "day3-reading-your-data.html#aims-for-today",
    "title": "3¬† Reading and Writing Files",
    "section": "",
    "text": "Reading data\nCleaning data\nRegular expressions\nData management",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#reading-data",
    "href": "day3-reading-your-data.html#reading-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.2 Reading data",
    "text": "3.2 Reading data\n\n3.2.1 Data types\nIn your work you will encounter many different types of data. Most frequently, you will work with tabular data, either as Excel files or as comma or tab separated values (CSV and TSV files, respectively). I am sure you have worked with such files before.\n To read these files, we will use two packages: readr and readxl. The former, readr, is part of the tidyverse package, so when you load the tidyverse using library(tidyverse), readr is loaded as well. The latter, readxl, is a separate package that you need to install and load separately.readr and readxl packages\n There are also ‚Äúbase R‚Äù functions read.table, read.csv, read.tsv (there is no function for reading XLS[X] files in base R). These are always available when you start R, but don‚Äôt use them. The tidyverse functions are not only faster, but also much better behaving and, which is most important, they are safer ‚Äì it is less likely to mess up your data with them.read.table(), read.csv(), read.tsv()\n tidyverse functions return tibbles, which, as you remember from yesterday, are a special flavor of data frames. Just to refresh your memory, here are key facts about tibbles:tibbles\n\nin most of the cases, they behave exactly like data frames\nwhen you print them, they are nicer\ntibbles have no row names\nwhen you select columns using [ , sel ], you always get a tibble, even if you selected only one column\n\nread_table(), read_tsv(), read_csv(), read_delim(), read_xls(), read_xlsx(), read_excel()\n\n\n\n\n\n\n\n\n\nData type\nFunction\nPackage\nNotes\n\n\n\n\nColumns separated by spaces\nread_table()\nreadr/tidyverse\none or more spaces separate each column\n\n\nTSV / TAB separated values\nread_tsv()\nreadr/tidyverse\nDelimiter is tab (\\t).\n\n\nCSV / comma separated\nread_csv()\nreadr/tidyverse\nComma separated values\n\n\nAny delimiter\nread_delim()\nreadr/tidyverse\nCustomizable\n\n\nXLS (old Excel)\nread_xls() read_excel()\nreadxl\nAvoid using XLS files. From the readxl package.\n\n\nXLSX (new Excel)\nread_xlsx() read_excel()\nreadxl\nFrom the readxl package. You need to provide the sheet number you wish to read. Note: returns a tibble, not a data frame!\n\n\n\nAs you can see, the functions we recommend to use can be used by loading the packages tidyverse and readxl. If you haven‚Äôt done that yet, please install these packages now:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")\n\nHowever, before we start using these functions, we need to dive into a very important problem: where are your files?\n\n\n\n\n\n\nRemember!\n\n\n\n\nFor reading text files (csv, tsv etc.), use the readr package. This package is loaded automatically when you load the tidyverse package: library(tidyverse). Then, use the functions read_csv, read_tsv etc.\nFor reading Excel files, use the readxl package: library(readxl). Then, use the function read_excel.\n\n\n\n\n\n3.2.2 Where is my file? Relative and absolute paths\nWe are coming now to a crucial problem and a source of endless frustration for beginning R programmers: how to tell R where your file is. Fortunately, there are many ways to deal with that, both with R and RStudio. Still, this is a key problem and we would like you to spend some time on the following chapter.\n In order to access a file, a program (any program) needs what is known a path. Path is a character string that tells the program how to get to the file and looks, for example, like this on a Windows computer: C:/Users/johndoe/Documents/RProjects/MyFirstRProject/data.csv, and like this on a Mac: /Users/johndoe/Documents/RProjects/MyFirstRProject/data.csv.Path\n\n\n\n\n\n\nPath separators on different systems\n\n\n\nMost computer systems separate the directories and files in a path using a slash (/). However, Windows uses a backslash (\\). This is quite annoying for R users, because in R character vectors, the slash has a special meaning. To use backslashes, you need to ‚Äúescape‚Äù them by putting another backslash in front of each backslash. So instead of C:\\Users\\johndoe\\Documents, you need to write C:\\\\Users\\\\johndoe\\\\Documents. Alternatively, you can use the forward slash even on a Windows system, so type C:/Users/johndoe/Documents. We recommend the latter approach.\nThis is also why simply copying a path from the Windows Explorer to your R script will not work in R ‚Äì because the copied text contains single backslashes.\n\n\nIf R cannot find your file, it will return an error message. At first, you will be seeing this error a lot:\nError: file not found\n\nlibrary(tidyverse)\nmydata &lt;- read_csv(\"data.csv\")\n\nError: 'data.csv' not found in the current working directory\n('C:/Users/johndoe/Documents/RProjects/MyFirstRProject/').\n\n\n Before we proceed, you need to understand one important thing. When you start your RStudio and R session, the R session runs in a specific directory. This is called the working directory. You can check what is using the getwd() function1:getwd()1¬†You can change it using the setwd() function, but avoid doing that. This path leads to madness, trust me on that.\n\ngetwd()\n\n[1] \"C:/Users/johndoe/Documents/RProjects/MyFirstRProject\"\n\n\nOf course, the result will be different on your computer. By the way, the above is called an absolute path. That means that it works no matter where you are in your system, because a program can always find the file or directory using this path.\nThe easiest way to read the data is this: copy your data files to the directory returned by getwd().\n\n\n\n\n\n\n\nExercise 3.1 (Reading your first file) ¬†\n\nCheck your working directory using getwd()\nLoad the tidyverse package using library(tidyverse)\nGo to the URL https://github.com/bihealth/RCrashcourse-book/Datasets\nClick on ‚Äúiris.csv‚Äù\nClick on the ‚ÄúDownload raw file‚Äù button on the right side of the screen\nSave the file in the directory returned by getwd()\nRead the file using read_csv(\"iris.csv\")\n\n\n\n\n\nThe following code should now work without an error:\n\nlibrary(tidyverse)\niris_data &lt;- read_csv(\"iris.csv\")\n\nNow the contents of the file are stored in the iris_data object. There are many ways to have a look at it:\n\ntype iris_data or print(iris_data) in the console\ntype View(iris_data) in the console\nclick on the little spreadsheet icon next to the iris_data object in the Environment tab in RStudio (upper right panel)\n\nPlease make sure that the above works for you. If it does not, read the instructions again. In RStudio, there is a ‚ÄúFiles‚Äù tab in the lower right panel. You should see your working directory as well as the ‚Äúiris.csv‚Äù file there.\n\n\n3.2.3 Reading with relative paths\nOK, so far, so good. That was easy. Now comes a slightly harder part.\nSaving your data files in the working directory works well if you have one or two. However, the more files you read and write, the more cluttered your project directory becomes. You will soon find yourself in a situation where you have no idea which file is which.\nIt is generally a good idea to keep your data files in a separate directory, or even multiple directories.\n\n\n\n\n\n\n\nExercise 3.2 (Reading your first file from a data directory) ¬†\n\nIn the working directory, create a new directory called ‚ÄúDatasets‚Äù\nMove the ‚Äúiris.csv‚Äù file to the ‚ÄúDatasets‚Äù directory\nRead the file using read_csv(\"Datasets/iris.csv\")\n\n\n\n\n\n The path ‚ÄúDatasets/iris.csv‚Äù is called a relative path, because it is relative to the working directory and will not work from another location. So why should we use relative paths? Wouldn‚Äôt it be easier to use absolute paths all the time, for example read_csv(\"C:/Users/johndoe/Documents/RProjects/MyFirstRProject/Datasets/iris.csv\")?Relative path\nActually, no. The problem is that if you move your R project to another location, of if you share it with someone else, the absolute path will no longer work. In other words, the absolute path is not portable.\n\n\n\n\n\n\nRemember!\n\n\n\nDo not use absolute paths in your code. Always use relative paths.\n\n\n\n\n3.2.4 More on relative paths\nSome times the data files are not in your R project directory. For example, you are writing your PhD thesis. You have created a directory called ‚ÄúPhD‚Äù, which contains directories ‚Äúmanuscript‚Äù, ‚Äúdata‚Äù, ‚Äúimages‚Äù, ‚ÄúR_project‚Äù and so on. You use R to do a part of the calculations, but you want to keep the data files in the ‚Äúdata‚Äù directory. How to read them?\nWhen you type getwd(), you will get the path to the ‚ÄúR_project‚Äù directory, something like C:/Users/johndoe/Documents/PhD/R_project. The data files are in the directory C:/Users/johndoe/Documents/PhD/data. To get the relative path from the R project directory to the data directory, think about how you would navigate from one directory to another in the Windows Explorer or Finder. You need to go up one level, to get into ‚ÄúPhD‚Äù, and then down again to ‚Äúdata‚Äù.\n Getting ‚Äúup one level‚Äù in a path is done by using ... So the relative path to the file ‚Äúiris.csv‚Äù in your ‚Äúdata‚Äù directory is ../data/iris.csv.Up one level with ..\n\n\n\n\n\n\n\nExercise 3.3 (Reading a file from a data directory using relative paths) ¬†\n\nIn the directory that contains the working directory create a directory called ‚ÄúData‚Äù. That is, if your working directory is C:/Users/johndoe/Documents/PhD/R_project, create the directory C:/Users/johndoe/Documents/PhD/Data\nMove the ‚Äúiris.csv‚Äù file to the new ‚ÄúData‚Äù directory\nRead the file using read_csv(\"../Data/iris.csv\")\n\n\n\n\n\nBut what about the case when your data directory is at a completely different location? For example, on a different drive, or maybe on your desktop?\nFirst, I don‚Äôt recommend keeping your data files separately from the R project directory. In general, try to put everything in one place, as part of one structure. This structure can be complex, but it should be coherent. If necessary, copy the data files into your R project directory.\nHowever, sometimes it simply isn‚Äôt possible. Maybe the files are huge and you need to read them from a special drive. In this case, there are three options.\nUsing absolute paths. Yes, I told you not to use absolute paths, but sometimes you have no choice.\nCreate shortcuts. In all systems it is possible to create shortcuts to your data directories (on Unix-like systems like MacOS they are called ‚Äúsymbolic links‚Äù). You can put these shortcuts in your R project directory ‚Äì R will treat them as normal directories.\nCreate a complex relative path. Depending on how far ‚Äúaway‚Äù your data directory is, you can create a complex relative path. For example, if your R project directory is C:/Users/johndoe/Documents/PhD/R_project and your data directory is D:/Data, you can use the path ../../../../Data/iris.csv. Unfortunately, despite being a relative path, this is not very portable (and it is easy to lose count on the ..‚Äôs).\n\n\n3.2.5 Using RStudio to import files\nRStudio has a very nice feature that allows you to import files using a point-and-click interface. When you click on a data file in the ‚ÄúFiles‚Äù tab (lower right panel), you will see two options: ‚ÄúView File‚Äù and ‚ÄúImport Dataset‚Äù. Choosing the latter opens a dialog window which shows a preview of the file and allows you to construct your read_csv, read_tsv or another command using a point-and-click interface. You also see a preview of the expression that will be executed.\nThis feature is particularly useful when you are not sure about the format of the data to import, e.g.¬†what type of delimiter is used, how many lines should be skipped etc.\nThen you can click on ‚ÄúImport‚Äù to actually run the command, however I would recommend another approach. Clicking on ‚ÄúImport‚Äù runs the command directly in the console, bypassing your script ‚Äì and you should always enter the code in your script before executing it.\nRather than clicking on ‚ÄúImport‚Äù, click on the little Copy icon next to the ‚ÄúCode preview‚Äù field, and then cancel the dialog. Paste the copied code into your script, and then run it.\nThere is one more thing to modify. The code generated by RStudio often uses absolute paths. Try to modify it to use relative paths to ensure portability of your script.\n\n\n\n\n\n\n\nExercise 3.4 (Reading data) ¬†\n\nGo to the URL https://github.com/bihealth/RCrashcourse-book/Datasets\nDownload the following files:\n\niris.csv\nmeta_data_botched.xlsx\ntranscriptomics_results.csv\n\nAlternatively, you can download the whole repository as a ZIP file and unpack it.\nSave the files in the Datasets/ directory in your working directory ‚Äì or another location of your choice. From now on, I will assume that this is the location of your data files.\nRead the files using the appriopriate functions. Consult the table above for the correct function names, or use the RStudio data import feature. Make sure that you are using relative paths.\n\n\n\n\n\n\n\n3.2.6 Reading Excel files\nReading files sometimes requires diligence. This is especially true for Excel files ‚Äì they can contain multiple sheets, tables often do not start on the first row etc.\n The package readxl (which you hopefully successfully used to read the XLSX file in the previous exercise) contains several example files. They have been installed on your system when you installed the package. Manually finding these example files is annoying, and that is why the readxl package provides a convenience function, readxl_example(), that returns the absolute path to the file (yes, I know what I said about absolute paths; this is an exception).Reading excel files with readxl package\nreadxl_example(),read_excel()\n\nlibrary(readxl)\nfn &lt;- readxl_example(\"deaths.xls\")\nprint(fn)\n\n[1] \"/home/january/R/x86_64-pc-linux-gnu-library/4.4/readxl/extdata/deaths.xls\"\n\ndeaths &lt;- read_excel(fn)\n\nNew names:\n‚Ä¢ `` -&gt; `...2`\n‚Ä¢ `` -&gt; `...3`\n‚Ä¢ `` -&gt; `...4`\n‚Ä¢ `` -&gt; `...5`\n‚Ä¢ `` -&gt; `...6`\n\nhead(deaths)\n\n# A tibble: 6 √ó 6\n  `Lots of people`             ...2       ...3  ...4     ...5          ...6     \n  &lt;chr&gt;                        &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;    \n1 simply cannot resist writing &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;     &lt;NA&gt;          some not‚Ä¶\n2 at                           the        top   &lt;NA&gt;     of            their sp‚Ä¶\n3 or                           merging    &lt;NA&gt;  &lt;NA&gt;     &lt;NA&gt;          cells    \n4 Name                         Profession Age   Has kids Date of birth Date of ‚Ä¶\n5 David Bowie                  musician   69    TRUE     17175         42379    \n6 Carrie Fisher                actor      60    TRUE     20749         42731    \n\n\nIf you view this file using the head() function (or if you use the RStudio data import feature), you  will notice that the actual data in this file starts on line 5; the first 4 lines contain the text ‚ÄúLots of people simply cannot resist writing some notes at the top of their spreadsheets or merging cells‚Äù. To skip these lines, we need to use an argument to the read_excel function. If you look up the help file for the function (e.g.¬†using ?read_excel), you will find the following fragment:head()\nskipping lines with read_excel(skip=...)\nskip    Minimum number of rows to skip before reading anything,\n        be it column names or data. Leading empty rows are \n        automatically skipped, so this is a lower bound. Ignored\n        if range is given.\nTherefore, we can modify the code above to skip the first four lines:\n\ndeaths &lt;- read_excel(fn, skip=4)\nhead(deaths)\n\n# A tibble: 6 √ó 6\n  Name       Profession Age   `Has kids` `Date of birth`         `Date of death`\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;      &lt;dttm&gt;                  &lt;chr&gt;          \n1 David Bow‚Ä¶ musician   69    TRUE       1947-01-08 00:00:00.000 42379          \n2 Carrie Fi‚Ä¶ actor      60    TRUE       1956-10-21 00:00:00.000 42731          \n3 Chuck Ber‚Ä¶ musician   90    TRUE       1926-10-18 00:00:00.000 42812          \n4 Bill Paxt‚Ä¶ actor      61    TRUE       1955-05-17 00:00:00.000 42791          \n5 Prince     musician   57    TRUE       1958-06-07 00:00:00.000 42481          \n6 Alan Rick‚Ä¶ actor      69    FALSE      1946-02-21 00:00:00.000 42383          \n\n\n\n\n\n\n\n\n\nExercise 3.5 (Reading data with options) If you take a closer look at the file deaths.xls, you will notice that there is some extra text at the bottom of the file as well. How can you omit that part when reading? Hint: there are two ways to do that. If in doubt, look up the ‚Äúexamples‚Äù section of the readxl helpfile.\n(Solution)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#diagnosing-and-cleaning-data",
    "href": "day3-reading-your-data.html#diagnosing-and-cleaning-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.3 Diagnosing and cleaning data",
    "text": "3.3 Diagnosing and cleaning data\n\n3.3.1 Diagnosing datasets\nCongratulations! You are now proficient in reading data into R. Believe me or not, this alone is an important step forward ‚Äì many who try their hand at R get stuck at this point.\nHowever, reading data is only the first step. In most of the cases, the data requires some treatment: cleaning, transformation, filtering etc. In many projects that I have worked on as a bioinformatician, data import, diagnosis and cleaning took up the majority of time spent on the project. Unfortunately, this is necessary before any real fun with the data can start.\n Let us examine the file iris.csv that you have just read. The dataset comes from a famous paper by Ronald Fisher, who used it to demonstrate his newly developed method called linear discriminant analysis ‚Äì an early machine learning algorithm, if you will. The dataset contains measurements (in cm) of 150 flowers of three species of irises: Iris setosa, Iris versicolor and Iris virginica. The measurements are the sepal and petal length and width, four measurements in total. Each row consists of these four measurements, plus a column which contains the species name.The iris dataset\nI have doctored the file to mimick typical problems with imported data, especially in clinical trial settings. Humans who enter data make errors, this is normal and expected, all of us do. Before we analyse them, we need to correct them. Before we correct them, we need to find them.\nFirst, note that when you read the data using read_csv, the function conveniently shows what types of data were assigned to each column:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\")\n\nRows: 150 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): Sepal Length, Petal?Length, Species\ndbl (2): Sepal Width, Petal.Width\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis should already ring some alarm bells. First, it looks like the columns are not consistently named: we have Sepal Length and Petal?Length, and also Petal.Width.\n Secondly, we would expect that the measurements are imported as numbers. However, both sepal and petal lengths are imported as characters. We can confirm this by using the class function:class()\n\nclass(iris_data[[\"Sepal Length\"]])\n\n[1] \"character\"\n\nclass(iris_data[[\"Sepal Width\"]])\n\n[1] \"numeric\"\n\nclass(iris_data[[\"Petal?Length\"]])\n\n[1] \"character\"\n\nclass(iris_data[[\"Petal.Width\"]])\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nclass and typeof\n\n\n\nThe class function returns the class of an object, which is a higher-level classification of the object. An object can have multiple classes. The typeof function returns the internal storage type of the object, which is a lower level classification. For example, both tibbles and data frames have the type list, but their classes are different. Another example:i if mtx is a matrix of numbers, typeof(mtx) is double, and class(mtx) is matrix.\n\n\nNote that instead of using iris_data$Sepal Length (which will not work, because there is a space in the column name), we use the double bracket notation. An alternative would be to use quotes: iris_data$'Sepal Length'. This a reason why want to avoid spaces and special characters in column names (in a moment we will show you how to standardize column names). If you use tab-completion with RStudio, the quotes will be inserted automatically.\n We can also use the summary function on the whole dataset:summary()\n\nsummary(iris_data)\n\n Sepal Length        Sepal Width     Petal?Length        Petal.Width   \n Length:150         Min.   : 2.000   Length:150         Min.   :0.100  \n Class :character   1st Qu.: 2.800   Class :character   1st Qu.:0.300  \n Mode  :character   Median : 3.000   Mode  :character   Median :1.300  \n                    Mean   : 3.411                      Mean   :1.199  \n                    3rd Qu.: 3.375                      3rd Qu.:1.800  \n                    Max.   :36.000                      Max.   :2.500  \n   Species         \n Length:150        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nOps, another problem. Did you notice the maximum value for sepal widths? It is 36 cm. This cannot be, way too large for the flowers. Also, both the mean and median are around 3 cm, so this must be either an outlier or a clerical error.\n\n\n\n\n\n\nThe summary() functions\n\n\n\nUnder the hood, there is no single summary() function. Instead, different classes can have different types of summaries. Whenever you produce a result, always try the summary function with it.\n\n\n Alternatively, we can use the tidyverse glimpse function. Rather then providing a summary, this function uses a terse format to show the data type for each column and first few values:glimpse()\n\nglimpse(iris_data)\n\nRows: 150\nColumns: 5\n$ `Sepal Length` &lt;chr&gt; \"5.1\", \"4.9\", \"4.7\", \"4.6\", \"5\", \"5.4\", \"4.6\", \"5\", \"4.‚Ä¶\n$ `Sepal Width`  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, ‚Ä¶\n$ `Petal?Length` &lt;chr&gt; \"1.4\", \"1.4\", \"1.3\", \"1.5\", \"1.4\", \"1.7\", \"1.4\", \"1.5\",‚Ä¶\n$ Petal.Width    &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, ‚Ä¶\n$ Species        &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"seto‚Ä¶\n\n\n Another way how we can diagnose the dataset is to use the str function. This provides a more detailed summary for each column:str()\n\nstr(iris_data)\n\nspc_tbl_ [150 √ó 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Sepal Length: chr [1:150] \"5.1\" \"4.9\" \"4.7\" \"4.6\" ...\n $ Sepal Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal?Length: chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Sepal Length` = col_character(),\n  ..   `Sepal Width` = col_double(),\n  ..   `Petal?Length` = col_character(),\n  ..   Petal.Width = col_double(),\n  ..   Species = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n3.3.2 Using skim() to diagnose datasets\n Another option to diagnose the dataset is to use the skim function from the skimr package, which you installed (hopefully) yesterday. The skim package provides a more detailed summary of the dataset, including the number of missing values, the number of unique values and summary statistics. To use it, you need first to load the package:skim()\n\nlibrary(skimr)\n\nThis skim() function shows more than just the str function. In addition to column types, for character and factor columns it shows the unique values, ordered by their frequency. For numerical values, it shows their non-parametric summary statistics (range, median, quartiles):\n\nskim(iris_data)\n\n\nData summary\n\n\nName\niris_data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSepal Length\n0\n1\n1\n5\n0\n37\n0\n\n\nPetal?Length\n0\n1\n1\n4\n0\n47\n0\n\n\nSpecies\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSepal Width\n0\n1\n3.41\n3.16\n2.0\n2.8\n3.0\n3.38\n36.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.3\n1.80\n2.5\n‚ñá‚ñÅ‚ñá‚ñÖ‚ñÉ\n\n\n\n\n\n Hm, when you look at the output you might notice one more thing: for the last column, ‚ÄúSpecies‚Äù, skim() shows that there are six unique values (take a look at the n_unique column). However, how can that be? We know that there are only three species in this dataset. We can check this using the unique function:unique()\n\nunique(iris_data[[\"Species\"]])\n\n[1] \"setosa\"     \"Setosa\"     \"versicolor\" \"Versicolor\" \"virginica\" \n[6] \"Virginica\" \n\n\nThere it is. Looks like whoever typed the data2, sometimes used a lower-case species designation, and sometimes upper-case. However, for R (and for computers in general), ‚Äúversicolor‚Äù and ‚ÄúVersicolor‚Äù are two different things.\n2¬†I hate to say it, it was probably me. More information can be gained using the table() function:table()\n\ntable(iris_data[[\"Species\"]])\n\n\n    setosa     Setosa versicolor Versicolor  virginica  Virginica \n        45          5         42          8         46          4 \n\n\nHere we not only see the unique values, but also their frequency. The table() function is very useful for any categorical data, as you will see later.\n\n\n\n\n\n\nLower and upper case\n\n\n\nFor computers in general and R in particular, ‚Äúlowercase‚Äù and ‚ÄúUppercase‚Äù are two different things. Variables a and A are different, as are versicolor and Versicolor. This is called case sensitivity.\n\n\n\n\n3.3.3 Checking individual values\nWe have seen before that some measurement columns were imported as character vectors, while others were correctly imported as numbers. To understand why, we need to have a closer look at the individual columns.\nOne of the columns that has been imported as a character vector is the ‚ÄúSepal Length‚Äù. We will extract the column as a vector and then try to manually convert it to a number:\nas.numeric()\n\nsepal_length &lt;- iris_data[[\"Sepal Length\"]]\nas.numeric(sepal_length)\n\nWarning: NAs introduced by coercion\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0\n [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5\n [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2  NA 5.9 6.1\n [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5\n [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n[109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7  NA 6.0 6.9 5.6 7.7 6.3 6.7 7.2\n[127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\n\nTwo things to note: first, R issued a warning, ‚ÄúNAs introduced by coercion‚Äù. That means that some values could not be converted to numbers, and were replaced by the special value NA3. Second, there was no error. R happily allowed you to create a vector of numbers from a vector even though there were some problems.\n3¬†See Day 1 for more information on NA values.When you look at the output, you will easily spot the NA values. You could even figure out at which positions in the vector they occur. However, this is not the way to go ‚Äì what if you have not 150 values, but 150,000?  Instead, use the is.na function:is.na()\n\n# convert to numbers\nsepal_length_n &lt;- as.numeric(sepal_length)\n\nWarning: NAs introduced by coercion\n\n# how many are NA?\nsum(is.na(sepal_length_n))\n\n[1] 2\n\n# which are NA?\nwhich(is.na(sepal_length_n))\n\n[1]  70 119\n\n# show the values\nsepal_length[is.na(sepal_length_n)]\n\n[1] \"5,6\"   \"&gt; 7.7\"\n\n\nOne more problem remains: the value ‚Äú36‚Äù in the sepal width column. We can check which values are greater than 10 with a similar approach:\n\nsepal_width &lt;- iris_data[[\"Sepal Width\"]]\n\n# how many are greater than 10?\nsum(sepal_width &gt; 10)\n\n[1] 2\n\n# which are greater than 10?\nwhich(sepal_width &gt; 10)\n\n[1]  42 110\n\n# show the values\nsepal_width[sepal_width &gt; 10]\n\n[1] 23 36\n\n\n\n\n\n\n\n\n\nExercise 3.6 (Petal lengths) Repeat the steps above for the Petal length column.\n\n\n\n\n\n\n3.3.4 Diagnosing datasets: a checklist\nWhenever you read a data set, you should check the following things:\nColumn names. Are they consistent? Are they easy to type (no spaces, no special characters)? Are they in the correct order? Are they what you expect them to be? Do you understand what each column is?\nData types. Are the columns of the correct type (e.g. numerical columns imported as numbers)?\nCategorical variables. Do you see what you expect? Is the number of categories correct? Do you understand what each category is?\nNumerical variables. Do the summary statistics make sense? Are there any outliers? Missing values? Were the variables imported incorrectly as characters? Why?\nMissing values. Are there any missing values? Why? Are they important? How to deal with them?\n\n\n\n\n\n\nChecklist for importing data\n\n\n\n\nColumn names\nData types\nCategorical variables\nNumerical variables\nMissing values\n\n\n\n\n\n\n\n\n\n\nExercise 3.7 (Botched metadata) Load the file meta_data_botched.xlsx using the readxl package. Diagnose the problems.\n(Solution)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#mending-the-data",
    "href": "day3-reading-your-data.html#mending-the-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.4 Mending the data",
    "text": "3.4 Mending the data\n\n3.4.1 Correcting column names\nColumn names should contain only letters, numbers and underscores. While all other characters are also allowed (so a column name can be Ë±öËÇâ üòÄ \"temp.\"? (¬∞C)), it is way easier to work with columns with simple, standardized names.\nYou already know one way of dealing with this ‚Äì simply assign new names to the columns:\n\ncolnames(iris_data) &lt;- c(\"sepal_length\", \"sepal_width\", \"petal_length\",\n                      \"petal_width\", \"species\")\ncolnames(iris_data)\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\n However, there is a quicker way. The janitor package contains a function called clean_names that does exactly that, automatically and for all  columns. Install it with install.packages(\"janitor\") if you haven‚Äôt installed it yet.janitor packageclean_names()\n\nlibrary(janitor)\niris_data &lt;- clean_names(iris_data)\ncolnames(iris_data)\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\nMy advice is to use clean_names on every dataset you import.\n\n\n3.4.2 Correcting outliers\nIn the column with sepal widths, which now has the name sepal_width, we found two values that probably lack the decimal point: 23 and 36. They are at positions 42 and 110, respectively.\nIt is tempting to simply substitute the data in the original data frame:\n\n# works, but don't do it\niris_data$sepal_width[42] &lt;- 2.3\n\nUnfortunately, this is not a good idea, because this solution is a bit like manual editing in a spreadsheet. There is a better way. First, generalize the issue. What seems to be the problem? The values that are larger than 10 are probably missing a decimal point. The simplest solution is to divide them by 10. Using logical vectors, we can do that for all the values that are larger than 10.\n\n# numbers that are missing a decimal point\ntoo_large &lt;- iris_data$sepal_width &gt; 10\n\niris_data$sepal_width[too_large] &lt;- iris_data$sepal_width[too_large] / 10\n\nThis a bit hard to read, so we make it more explicit:\n\n# numbers that are missing a decimal point\nsepal_width &lt;- iris_data$sepal_width\ntoo_large &lt;- sepal_width &gt; 10\n\nsepal_width[too_large] &lt;- sepal_width[too_large] / 10\nsepal_width[too_large]\n\nnumeric(0)\n\n\nLooks good. Finally, we assign the corrected values back to the data frame:\n\niris_data$sepal_width &lt;- sepal_width\n\nAs the last step, we need to check whether our approach worked for all  values in the column. For this, we will use the any() function, which returns TRUE if any of the values in a logical vector is TRUE:any()\n\nany(iris_data$sepal_width &gt; 10)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\nExercise 3.8 Can you spot what the potential problem with this approach is? Hint: what would happen if the original value was 1.10? Can you think of a better approach? Is it actually possible to make sure that our guess is correct?\n\n\n\n\n\n\n3.4.3 Correcting categorical data\nIn the column ‚Äúspecies‚Äù, we found that there are two different values for the same species. There are many ways we can handle it. Obviously, we could manually assign the correct labels, but that would be a lot of work.\nA better way would be to somehow automatically convert all the values. In our case, the problem is quite simple: it is sufficient to put all the values in uniform lower case, so that Versicolor becomes versicolor.\n We can do that in one line of code thanks to the tolower function:tolower()\n\niris_data$species &lt;- tolower(iris_data$species)\n\n There is also a corresponding function, toupper, that converts all the letters to upper case.toupper()\nRemember to check that your changes were successful:\n\ntable(iris_data$species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nThis was a simple case, but what if we had more complex typos? For example, ‚Äúi.versicolor‚Äù, ‚Äúvers.‚Äù, ‚ÄúV‚Äù etc. In such cases, you would need to use a more sophisticated approach. This approach involves regular expressions, and will also help us to correct the sepal and petal lengths.\n\n\n3.4.4 Incorrectly imported numeric data\nWe have two columns which were imported as characters, but should be numbers. Their brand-new names are sepal_length and petal_length. All in all, there are 2 problems in the sepal length column and 5 in the petal length column, as shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\nColumn\nPosition\nOriginal\nCorrect\n\n\n\n\nsepal_length\n70\n5,6\n5.6\n\n\nsepal_length\n119\n&gt; 7.7\n7.7\n\n\npetal_length\n12\n1.6!\n1.6\n\n\npetal_length\n13\n1 .4\n1.4\n\n\npetal_length\n14\n1.1?\n1.1\n\n\npetal_length\n115\n5k.1\n5.1\n\n\npetal_length\n149\n5. 4\n5.4\n\n\n\n\n\nThe last column in the table above is our target ‚Äì this is how we would like to correct the data. How can we do that?\n\n\n\n\n\n\nCorrecting values\n\n\n\nOf course, without additional information we can only guess that 5,6 should be 5.6. Maybe there were two measurements, 5 and 6? Maybe 5. 4 should be 5.04, and not 5.4? In a real-world scenario, you would need to consult the author(s) of the data, or check your lab book.\n\n\nIt is tempting to simply substitute the data in the original data frame:\n\n# works, but don't do it\niris_data$sepal_length[70] &lt;- 5.6\n\nAs before, that is not a good idea. If you receive another version of the file, with added lines before the 70th, the position will change and you will have to correct the version manually again. Also, if there are new problems, like yet another typo in the data, you will have to spot it and manually add a line to correct it. Again, we need to generalize the issue.\nWhat is the problem on line 70? A comma instead of a decimal dot ‚Äì maybe someone who typed it was used to entering numbers in a German version of Excel, where a comma is used instead of a dot. So why not replace all the commas by a dot? This should do the trick not only for line 70, but also for possible future problems.\n We will use for that the function str_replace_all from tidyverse4. Before we do that, however, we will make a copy of the column. That will allow us to easily see the changes will make, check that everything is OK and only then replace the column in the data frame. Later on, you will learn more efficient ways of doing this.str_replace_all()4¬†In base R, you can use the gsub function. However, it has a slightly different syntax, which makes it less convenient when you start using pipes tomorrow.\n\n# make a copy\nsepal_length &lt;- iris_data$sepal_length\n\n# record the problematic places\nproblems &lt;- is.na(as.numeric(sepal_length))\n\nWarning: NAs introduced by coercion\n\nsepal_length[problems]\n\n[1] \"5,6\"   \"&gt; 7.7\"\n\n# replace the comma \nsepal_length &lt;- str_replace_all(sepal_length, \",\", \".\")\n\n# what was the result?\nsepal_length[problems]\n\n[1] \"5.6\"   \"&gt; 7.7\"\n\n\nIt worked! The str_replace_all is a general search-and-replace function. You can always use it with character vectors to replace stuff.\nHowever, we still have the problem with &gt; 7.7. You will find such values quite often in clinical settings, along with the counterpart &lt; 7.7. These may indicate that the measurement was out of range of an instrument. However, we cannot usually treat them as missing values, because they contain some information. Depending on whether or not we want to keep this information, we could either replace them by NAs, or decide to keep the value as is (in this case, change &gt; 7.7 to 7.7).\nIn the latter case, we can use the str_replace_all function again:\n\nsepal_length &lt;- str_replace_all(sepal_length, \"&gt; \", \"\")\n\nIn any case, the last point that remains is to convert the vector to numbers and assign it to the column holding sepal lengths. We do it in one go and also check if everything is OK:\n\n# finalize\niris_data$sepal_length &lt;- as.numeric(sepal_length)\n\n# check whether the column is numeric\nis.numeric(iris_data$sepal_length)\n\n[1] TRUE\n\n# check whether our problems are gone\niris_data$sepal_length[problems]\n\n[1] 5.6 7.7\n\n# check whether there are any NA's\nany(is.na(iris_data$sepal_length))\n\n[1] FALSE\n\n\n The new function, is.numeric(), checks whether the column sepal_length is indeed numeric. Finally, we make sure that no NA‚Äôs were produced in the conversion.as.numeric()\n\n\n\n\n\n\n\nExercise 3.9 Find the problems in the following vector and correct them:\nvec &lt;- c(\" 5\", \"5,6\", \"5.7\", \"5. 4\", \"&gt; 5.0\", \"6.O\")",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#regular-expressions",
    "href": "day3-reading-your-data.html#regular-expressions",
    "title": "3¬† Reading and Writing Files",
    "section": "3.5 Regular expressions",
    "text": "3.5 Regular expressions\n\n3.5.1 Introduction to regular expressions\n Regular expressions, regexps for short, are for finding certain patterns in the data and handling them automatically. They are quite old (going back at least to 1960‚Äôs) and have remained virtually the same for many decades. You will find them in almost all programming languages, and while details may differ, the principles are usually the same. The regexps in R are very similar to regular expressions in other programming languages such as Python. As you may guess, they have a different syntax then R ‚Äì they are something else altogether.Regular expressions\nWe will not go in depth in regular expressions in R here. Here, just a short primer.\n Before we start, let us introduce a new function called str_detect()5. The function is available as soon as you load the tidyverse package. This function is used to search for a pattern in a character vector. For every element that matches the pattern, it will return a TRUE, otherwise a FALSE. Say, we have a vector (or data frame column) containing sample names, and we want to find all the controls.str_detect()5¬†In base R, there is the grep() function. It is very similar, but has a different syntax. We will stick to str_detect() for now.\n\nsamples &lt;- c(\"ko_1_ctrl\", \"ko_2_ctrl\", \"ko_1_treat\", \"ko_2_treat\",\n            \"wt_1_ctrl\", \"wt_2_ctrl\", \"wt_1_treat\", \"wt_2_treat\")\nstr_detect(samples, \"ctrl\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\nsamples[str_detect(samples, \"ctrl\")]\n\n[1] \"ko_1_ctrl\" \"ko_2_ctrl\" \"wt_1_ctrl\" \"wt_2_ctrl\"\n\n\nIn case that someone did not pay attention to lower or upper case, we can tell str_detect to ignore the case:\n\nsamples &lt;- c(\"ko_1_ctrl\", \"ko_2_CTRL\", \"ko_1_treat\", \"ko_2_treat\",\n            \"wt_1_CTRL\", \"wt_2_ctrl\", \"wt_1_treat\", \"wt_2_treat\")\n# this does not work\nstr_detect(samples, \"ctrl\")\n\n[1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n# but this does\nstr_detect(samples, regex(\"ctrl\", ignore_case=TRUE))\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\nHere we are looking for a literal string ‚Äúctrl‚Äù. However, we can use regular expressions to search for more complex patterns. What if the sample names are more hap-hazard, like ko_1_control, ko_2_kontrol, ko_1_ctrl?\n\nsamples &lt;- c(\"ko_1_control\", \"ko_2_ctrl\", \"ko_1_trt\", \"ko_2_treatment\",\n            \"wt_1_Control\", \"wt_2_kontrol\", \"wt_1_Trtmt\", \"wt_2_treated\")\n\nOK, so we got control, ctrl, Control and kontrol. A pattern emerges:\n\nfirst letter is either k or c or C\nthen we might have an on (or not)\nnext always comes tr\nthen we might have an o\nthen we always have an l\nthen the word ends.\n\nAll this ‚Äúmights‚Äù and ‚Äúors‚Äù and ‚Äúeithers‚Äù and ‚Äúalways‚Äù can be encoded in a regular expression:\n\n\n\nPattern\nExplanation\n\n\n\n\n[kcC]\nmust have one of the letters k, c or C\n\n\no?\nzero or one o (i.e., ‚Äúthere might be an o‚Äù)\n\n\nn?\nzero or one n (i.e., ‚Äúthere might be an n‚Äù)\n\n\ntr\nmust have literally the string tr\n\n\no?\nzero or one o (i.e., ‚Äúthere might be an o‚Äù)\n\n\nl\nmust have literally the string l\n\n\n$\nhere must be the end of the string\n\n\n\nTaken together, this gives us the regular expression [kcC]o?n?tro?l$. This looks weird, but has the magic power of matching all the control-like strings in our vector. Let us try it:\n\nstr_detect(samples, \"[kcC]o?n?tro?l$\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\nsamples[str_detect(samples, \"[kcC]o?n?tro?l$\")]\n\n[1] \"ko_1_control\" \"ko_2_ctrl\"    \"wt_1_Control\" \"wt_2_kontrol\"\n\n\nThis works, but‚Ä¶ what does that all mean?\n\n\n\n\n\n\n\nExercise 3.10 (Quick question) Look at the table above. Can you think of other patterns that would match this regular expression? Would kotrl work? What about cotro?\n\n\n\n\n\n\n3.5.2 How regexp works\n A regular expression is a sequence of characters ‚Äì a string. However, some of these strings have a special meaning. Here is a short table, we will discuss these in more detail in a moment:regexp rules\n\n\n\nCharacter\nMeaning\n\n\n\n\na\na literal character a\n\n\n.\nany character\n\n\n[abc]\none of the characters a, b or c\n\n\n[^abc]\nany character except a, b or c\n\n\n[a-z]\nany character from a to z\n\n\n[0-9]\nany digit\n\n\n\nThese are the so-called ‚Äúatoms‚Äù. They stand for something that we want to match. In the simplest case (like the a above), they stand for themselves. Also, did you notice the square brackets? They have nothing to do with how we use square brackets in R.\nIn addition we can specify how many times we want to match the atom:\n\n\n\nQuantifier\nMeaning\n\n\n\n\n?\nzero or one\n\n\n*\nzero or more\n\n\n+\nat least one\n\n\n\nThen, we can ‚Äúanchor‚Äù the pattern in our string:\n\n\n\nAnchor\nMeaning\n\n\n\n\n^\nstart of the string\n\n\n$\nend of the string\n\n\n\nLet us put that together. Say we have the following strings:\n\nstrings &lt;- c(\"a\", \"ab\", \"ac\", \"ad\", \"bc\", \"abc\", \"bac\", \"cab\")\n\nTo match these which contain an a, we can use the regular expression a:\n\nstrings[str_detect(strings, \"a\")]\n\n[1] \"a\"   \"ab\"  \"ac\"  \"ad\"  \"abc\" \"bac\" \"cab\"\n\n\nTo match these which start with an a, we can use the regular expression ^a:\n\nstrings[str_detect(strings, \"^a\")]\n\n[1] \"a\"   \"ab\"  \"ac\"  \"ad\"  \"abc\"\n\n\nTo find these which start with an a followed by b or c, but not d, we can use the square brackets:\n\nstrings[str_detect(strings, \"^a[bc]\")]\n\n[1] \"ab\"  \"ac\"  \"abc\"\n\n\nOK, that was a lot. Take some time to digest it. Regulare expressions are what you call in German gew√∂hnungsbed√ºrftig ‚Äì they require some getting used to, but they are worth it.\n\n\n3.5.3 Usage of regular expressions in R\nMost commonly you will use regular expressions in R for two things:\n\nSearching for something in a character vector (like we did above)\nReplacing something in a character vector, for example using str_replace_all\n\nWhat we did not tell you when we introduced str_replace_all is that it actually, its second argument is a regular expression. Therefore, we can use it to find a pattern and replace it with something else. For example, we can unify the following messed up vector denoting the gender of patients:\n\ngender &lt;- c(\"m\", \"f\", \"m\", \"w\", \"frau\",\n            \"female\", \"other\", \"male\", \n            \"m√§nnlich\", \"x\", \"weiblich\")\n\nGerman and English are mixed here, but we can see a pattern: if the strings starts with m, it is male, if it starts with either f or w, it is female. And in remaining cases it is ‚Äúother‚Äù. We can clean up this mess with just three lines of code:\n\ngender &lt;- str_replace(gender, \"^m.*\", \"male\")\ngender &lt;- str_replace(gender, \"^[fw].*\", \"female\")\ngender &lt;- str_replace(gender, \"^[^mfw].*\", \"other\")\n\nNote that the ^ character has two different meanings on line 3 above. As the first character of a regular expression, it anchors it to the beginning of the string. However, inside the square brackets, it negates the selection. So, [^mfw] means ‚Äúany character except m, f or w‚Äù.\nThe .* idiom is a common one in regular expressions. It means ‚Äúzero or more of any character‚Äù. So, ^m.* matches both the string ‚Äúm‚Äù and the string ‚Äúmale‚Äù. And because we used ^, it will only match the strings where m is the first letter (so it does not match ‚Äúfemale‚Äù).\n\n\n\n\n\n\n\nExercise 3.11 (Quick question) What would happen if we ommited the ^ at the beginning of the strings above? For example, if we used [^mfw].* instead of ^[^mfw].*? Think first, then try it out.\n\n\n\n\nBut wait, if some characters have a special meaning, how can we replace them? For example, how can we replace a dot in a string? The following will not work as intended:\n\nvec &lt;- c(\"5.6\", \"5.7\", \"5.8\")\nstr_replace_all(vec, \".\", \",\")\n\n[1] \",,,\" \",,,\" \",,,\"\n\n\nSince the character . means ‚Äúany character‚Äù, every character will be replaced by a comma in the example above. In order to search or replace  special characters, we must escape them ‚Äì which in R means putting two backslashes6 in front of them in the regular expression.escaping characters in a regexp6¬†In R, it is two backslashes. In other programming languages, it is usually a single backslash.\n\nvec &lt;- c(\"5.6\", \"5.7\", \"5.8\")\nstr_replace_all(vec, \"\\\\.\", \",\")\n\n[1] \"5,6\" \"5,7\" \"5,8\"\n\n\n\n\n\n\n\n\n\nExercise 3.12 ¬†\n\nUse str_replace_all() to make the following uniform: c(\"male\", \"Male \", \"M\", \"F\", \"female\", \" Female\")\nUsing str_replace_all() and toupper(), clean up the gene names such that they conform to the HGNC (all capital letters, no spaces, no dashes): c(\"ankrd22\", \"ifng\", \"Nf-kb\", \" Cxcl 5\", \"CCL 6.\", \"ANK.r.d. 12\")\nWhat regular expression matches all of the ankyrin repeat genes (but not other genes) in the following vector: c(\"ANKRD22\", \"ank.rep.d. 12\", \"ANKRD-33\", \"ankrd23\", \"ANKEN\", \"MAPK\", \"ifng-1\", \"ANKA-REP-6\")? Ankyrin repeat domain genes are the first 4 in the vector.\n\n\n\n\n\n\n\n3.5.4 Correcting columns in iris_data with regular expressions\nLet us now turn to another column in the data frame, the petal lengths. Using the approach we have just learned, we can find the problematic values:\n\npetal_length &lt;- iris_data$petal_length\nproblems &lt;- is.na(as.numeric(petal_length))\n\nWarning: NAs introduced by coercion\n\nwhich(problems)\n\n[1]  12  13  14 115 149\n\npetal_length[problems]\n\n[1] \"1.6!\" \"1 .4\" \"1.1?\" \"5k.1\" \"5. 4\"\n\n\nWe could use for example str_replace_all(sepal_length, \"k\", \"\"), to change 5k.1 to 5.1, but that would not be a general solution. What if it is j in the new data? We should actually remove everything that is not a number and not a decimal dot. To this end, we will use the regular expressions.\nWe need to remove anything but numbers and decimal dots. We can use the square brackets for that:\n\npetal_length &lt;- str_replace_all(petal_length, \"[^0-9.]\", \"\")\n\nThe 0-9 means anything between 0 and 9, and the . is a literal dot. As you can see, you don‚Äôt have to escape the dot if it is already in the  square brackets. The ^ inside the square brackets negates the selection, so [^0-9.] means exactly what we wanted.[^0-9]\n\n# check the problems\npetal_length[problems]\n\n[1] \"1.6\" \"1.4\" \"1.1\" \"5.1\" \"5.4\"\n\n# convert to numbers\npetal_length &lt;- as.numeric(petal_length)\n\n# check for remaining NA's\nany(is.na(petal_length))\n\n[1] FALSE\n\n# assign\niris_data$petal_length &lt;- as.numeric(petal_length)\n\nDone! The iris data set is now clean.\n\nskim(iris_data)\n\n\nData summary\n\n\nName\niris_data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nspecies\n0\n1\n6\n10\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n‚ñÜ‚ñá‚ñá‚ñÖ‚ñÇ\n\n\nsepal_width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n‚ñÅ‚ñÜ‚ñá‚ñÇ‚ñÅ\n\n\npetal_length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n‚ñá‚ñÅ‚ñÜ‚ñá‚ñÇ\n\n\npetal_width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n‚ñá‚ñÅ‚ñá‚ñÖ‚ñÉ\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.13 (Correcting metadata) In Exercise¬†3.7, you have diagnosed the file meta_data_botched.xlsx. Now go ahead and correct the problems you have found.\n\n\n\n\n\n\n3.5.5 Just a quick look\nThe newly cleaned up data set begs for a quick look. We already created a simple plot before using the plot() function, and today I will show you how to use the ggplot2 package to create somewhat nicer plots. In this case, it is rather trivial. ggplot2\n\nlibrary(ggplot2)\nggplot(iris_data, aes(x=sepal_length, y=sepal_width, color=species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nAs you can see, the ggplot function takes two arguments: the data frame and something which is calles aesthetics and which is produced by the aes() function. The aes() function serves as a kind of link between the data and the plot, showing how what should be plotted. In this case, we tell aes() that the x-axis should be the sepal length, the y-axis the sepal width, and the color should be determined by the species.\nThen, a magical thing happens. Whatever is produced by ggplot() is not a number, but nonetheless can be added with a + sign to other stuff. In this case, we add a geom_point() function, which tells ggplot to plot points at the x, y coordinates (there are many others, and you will learn about them the day after tomorrow).\nThere, all done. See how I. setosa is different from the other two?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#writing-data",
    "href": "day3-reading-your-data.html#writing-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.6 Writing data",
    "text": "3.6 Writing data\n\n3.6.1 Keep your data organized\nWriting data is way easier than reading it. However, you always have to come up with a name for the file, and I would like you to follow a few points.\n\nNever overwrite the original file (we will discuss it in more detail shortly).\nCreate an output directory and save your files only there, do not mix the original and the output files.\nUse a meaningful name, so not output, but iris_cleaned. No spaces or special characters (use an underscore _ instead).\nThirdly, always version your files: add a date, your initials, a version number and a date, e.g.¬†iris_cleaned_2021-09-01_JW_v1.csv.\nDo not use the old Excel XLS format (file extension .xls). Use the newer XLSX format (file extension .xlsx).\n\nThis latter point warrants an explanation. There are two main problems with that format. Firstly, it is less portable then XLSX (the new one) ‚Äì so many programs can read XLSX, but not XLS (or, which is worse, they can, but read it incorrectly). Secondly, the number of rows and columns in an XLS file is severly limited (65536 rows and 256 columns). These limits are easily reached in modern bioinformatic datasets.\n\n\n3.6.2 Functions to use for writing data\nwrite_tsv(), write_csv(),write_xlsx()\n\n\n\n\n\n\n\n\n\nData type\nFunction\nPackage\nNotes\n\n\n\n\nTSV / TAB separated values\nwrite_tsv()\nreadr\nNo rownames!\n\n\nCSV / comma separated\nwrite_csv()\nreadr\nNo rownames!\n\n\nXLS (old Excel)\n\n\nJust don‚Äôt use it. No, seriously, don‚Äôt.\n\n\nXLSX (new Excel)\nwrite_xlsx()\nwritexl\nNo rownames!\n\n\n\nJust as in the case of reading data, there are several functions to write data. There are also functions in the base R that can be used for writing (e.g.¬†write.csv), but they are not recommended. We advise you to only use the readr package for writing TSV and CSV files, and the writexl package for writing XLSX files.\nHowever, keep in mind that row names are not exported with these packages. That is why we do not use row names in this course.\n\n\n\n\n\n\n\nExercise 3.14 (Writing data) ¬†\n\nIn your project directory, create the directory ‚ÄúData_clean‚Äù (if you want, you can use the R function dir.create() for that).\nWrite the cleaned iris_data to a new file in the ‚ÄúData_clean‚Äù directory. Use the write_csv function from the readr package. Use a meaningful name, version it and use a date.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#principles-of-data-management",
    "href": "day3-reading-your-data.html#principles-of-data-management",
    "title": "3¬† Reading and Writing Files",
    "section": "3.7 Principles of data management",
    "text": "3.7 Principles of data management\n\n3.7.1 Keeping originals\nAs every day, we end the day with some food for thought. Today, we would like to introduce a few important principles that you should follow in your everyday work with data ‚Äì not only in R, but in general.\nThe first rule is: always keep the originals. Whether you have received the data from someone, extracted them from a database such as RedCap or created it yourself, once you start working with R you should freeze your data, version it and not touch it anymore.\nLet me reiterate: do not touch the original data. If you absolutely have to edit it by hand, make a copy and add a suffix like _edited or your initials to the file. However, it is better to perform all editing operations in R.\nVersioning. Always include a date, and possibly your initials and version in the file name. If you ever submitted a manuscript of a scientific paper, you know how quickly versions can accumulate. It is absolutely crucial to know which version of the data was used for which analysis.\nData editing. Every edit operation on your data should be somehow recorded. This is not different from a lab notebook, where you write down every step of your experiment. When working with a spreadsheet this is hard to do, therefore‚Ä¶\n\n\n3.7.2 Use R for data editing\nOptimally, your mode of operation should never involve editing data directly in a spreadsheet. You should read your data in R and perform all operations here, recording them in a script. This has two huge advantages:\n\nReproducibility. You and others will be able to trace back all your operations. There will be no doubt whether and which data were edited and how.\nAutomation. When you get updated data (and you will), for example when new samples arrived, you will be able to re-run your script (maybe with a few changes) and get the updated results with minimal effort. Otherwise, you will have to repeat every. Single. Edit. Again.\n\nConsider the example of the iris.csv file. We have seen that one of the values of sepal width seems to be missing a decimal dot. You could edit that in the Excel or CSV file directly. You might even make a note of that somewhere. However, chances are that you were wrong ‚Äì maybe that was a flower wiht a 36 sepal width after all. This change influences your data and your analysis results and must be documented if you were to do science. If you did it in R, you probably have entered a comment in your script, something like this:\n\n# There seems to be a missing decimal dot in some of the sepal widths\n# Changing it to 1/10th of the value\n\nsepal_width &lt;- iris_data[[\"Sepal Width\"]]\nsel &lt;- which(sepal_width &gt; 10)\nsepal_width[sel] &lt;- sepal_width[sel] / 10\n\niris_data[[\"Sepal Width\"]] &lt;- sepal_width\n\nThis concisely informs the reader that a change was made, how and why.\nAnd what happens when Ronald Fisher raises from his grave and collects  another 150 flowers? You will get an updated data set with 300 flowers and import it again. In the best case scenario, you will find the problem again (or remember it). Then you will have to go into Excel and make the same edit again. And all the other edits as well. If you did it in R, you simply use your script again.undead Ronald Fisher\n\n\n3.7.3 Working with Excel and other spreadsheets\n We all use Excel, LibreOffice Calc or Google Sheets. They are great tools, no doubt about that. However, by now you can see that certain operations make working with R harder, and some even that can mess up your data.How to work with Excel\nIf possible, you should actually avoid working with Excel when working with scientific data. Excel does certain operations automatically (depending on the settings), so it can change your data without leaving a trace of the changes. This is the precise opposite of what you want in science.\n\n\n\n\n\n\nAvoid working with Excel\n\n\n\nExcel can automatically replace strings with dates, like changing MARCH9 into 9th of March. However, MARCH9 is a valid gene name. In fact, it turns out that a substantial proportion (about a third!) of all Excel files containing gene names and published as supplementary materials online contain gene names transformed to dates. Not only that, but even though that this has been discovered many years ago and even some genes were officialy renamed because of Excel, this is still a problem. And Excel is now able to recognize dates in many languages, exacerbating the problem (Abeysooriya et al. 2021).\n\n\nHowever, sometimes it is unavoidable to work with Excel.\nDon‚Äôt‚Äôs:\n\nUsing color and font to encode information, for example marking the different treatments with different background color or formatting controls in bold. While it is possible to read this information in R, it is not trivial. In addition, even outside of R formatting of cells can be lost when the data is passed around, and this would mean that the information is lost.\nComments in the same cells as values, for example 50 (measured twice). This prevents the columns to be interpreted as numbers and has to be dealt with in R. Create a separate column for comments, even it is just a single comment in a row.\nMeta-data information in the header. Very often, column names contain additional information, for example units, dates, encoding (like ‚Äú0 for male, 1 for female‚Äù). This makes it hard to both, import the data and actually use that information. Create a separate sheet in your spreadsheet for meta data, with one row per column name.\nAdding header and tail information. Just like in the example with deaths.xlsx file, additional lines in spreadsheets require you to carefully select what you wish to import. Avoid it if you can, use another sheet.\nMore than one table on a spreadsheet. A spreadsheet should contain a single table. Otherwise, just like in case of header and tail information, you will have to spend some time cautiously selecting the areas that you want to import.\nMerging cells. Merged cells are a nightmare to work with in R. If you have to, unmerge them before importing the data.\n\nDo‚Äôs:\n\nUse a single table per sheet. If you have more than one table, use more than one sheet.\nUse a single header row. If you have more than one header row, use create another sheet with meta-data on the column names.\nSwitch off automatic conversion. Excel can automatically change your data in many ways. For example, it can change gene names to dates. It can also change the decimal separator, which can mess up your data. Newer versions of Excel allow to switch off this behaviour, so do it (here are the instructions).\nControl your import. When importing data from CSV files to TSV files, Excel allows you to control which fields are imported as text, which as dates etc. Use this feature to make sure that your data is correctly imported.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#review",
    "href": "day3-reading-your-data.html#review",
    "title": "3¬† Reading and Writing Files",
    "section": "3.8 Review",
    "text": "3.8 Review\nThings that you learned today:\n\nReading and writing\n\nread_csv, read_tsv, read_delim, read_xls, read_xlsx\nreading a file from working directory\nreading a file from a different directory using relative paths\n\nPaths\n\nGet the current working directory with getwd()\nrelative and absolute paths\n\nDiagnosing datasets\n\ndata types: str, class, typeof\nsummaries: summary, glimpse\ncategorical data: unique, table\nskim() from the skimr package\ndiagnosis checklist\n\nCorrecting datasets\n\nconverting character vectors to numbers with as.numeric()\nchecking a vector with is.numeric()\nchecking a logical vector with any()\nreplacing values with str_replace_all()\n\nPrinciples of data management\n\nkeeping originals\nversioning\ndata editing\nworking with Excel\n\nOther\n\nusing help pages to find out about functions and their parameters\n\n\n\n\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann. 2021. ‚ÄúGene Name Errors: Lessons Not Learned.‚Äù PLoS Computational Biology 17 (7): e1008984. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html",
    "href": "day4-manipulating-your-data.html",
    "title": "4¬† Manipulating data frames",
    "section": "",
    "text": "4.1 Aims for today",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#aims-for-today",
    "href": "day4-manipulating-your-data.html#aims-for-today",
    "title": "4¬† Manipulating data frames",
    "section": "",
    "text": "Searching, sorting and selecting\nMatching and merging data\nPipes - writing readable code\nWide and long format",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#selecting-columns",
    "href": "day4-manipulating-your-data.html#selecting-columns",
    "title": "4¬† Manipulating data frames",
    "section": "4.2 Selecting columns",
    "text": "4.2 Selecting columns\n\n4.2.1 Selecting columns using square brackets\nAs you know, if we want the actual column as vector, we use the $ operator:\n\ndf &lt;- data.frame(ara=1:5, bera=6:10, cora=11:15, dora=16:20)\ndf$ara # same as df[[\"ara\"]]\n\n[1] 1 2 3 4 5\n\n\nJust a reminder: you don‚Äôt want to try to select a single column using square brackets (df[, 1]), because the behavior is different for data frames and tibbles. Better use df$a or df[[\"a\"]].\nWe will now discuss selecting and searching for more than one column. Quite often, you want to select only some columns from a data set, or maybe you want to change their order. As you learned on Day 2, you can select columns from a data frame using vectors. This is very similar to how you select elements from a matrix, or even a vector. You can use integers, negative integers, or column names.\n\n# select columns 1 to 2\ndf2 &lt;- df[ , 1:2]\n\n# select anything but column 2\ndf2 &lt;- df[ , -2]\n\n# select all columns in reverse order\ndf2 &lt;- df[ , ncol(df):1]\n\n# select columns ara and cora\ndf2 &lt;- df[ , c(\"ara\", \"cora\")]\n\n# select all columns ara and cora, but in reverse order\ndf2 &lt;- df[ , c(\"cora\", \"ara\")]\n\nThis is very similar to what we did when dealing with matrices, and actually similar to how we select elements from a vector. Note, however, that rather than using 4:1 in the line 8 above, we use ncol(df):1. This ensures that if data frame grows for some reason, we still get all the columns.\n\n\n4.2.2 Selecting columns using tidyverse\n Tidyverse has the select() function, which is a bit more explicit and readable. Most importantly, it also has extra features that make it easier to work with.select()\n\nlibrary(tidyverse)\n# select columns ara and cora\ndf2 &lt;- select(df, ara, cora)\n\n# select columns ara to cora\ndf2 &lt;- select(df, ara:cora)\n\n# select anything but column bera\ndf2 &lt;- select(df, -bera)\n\nCan you spot what is weird about the code above? Exactly! There are no quotes around the column names. One would expect that R should throw an error ‚Äì after all, there is no variable ‚Äúara‚Äù defined yet. However, this is an extra feature of tidyverse. It can be confusing at first, but you will soon get to like it.\n What‚Äôs more, you see the constructs like ara:cora or -bera. In the base R, ara:cora means ‚Äúlook up variables ara and cora and return all integers between these two values‚Äù. In Tidyverse, that means ‚Äúselect all columns from ara to cora‚Äù. Similarly, -bera means ‚Äúselect all columns except bera‚Äù. This is called tidy evaluation and it works only with some tidyverse functions.Tidy evaluation\n\n\n\n\n\n\nRemember!\n\n\n\nTidy evaluation only works with tidyverse functions!\n\n\n Another nice feature of select() is that you can use the a few specialized helper functions, saving you tedious regular expression and column searches. For example, how would you select for columns that end with a particular suffix? This is a common enough task in data science ‚Äì many clinical data sets have hundreds of columns and a systematic way of naming them, thus allowing to search for example all columns related to laboratory diagnostics. This can easily be done with the ends_with() function:ends_with()\n\ndf2 &lt;- select(df, ends_with(\"ora\"))\ncolnames(df2)\n\n[1] \"cora\" \"dora\"\n\n\n There are many more such functions, like starts_with(), contains(), matches(), one_of(), everything() and even last_col() (for selecting the last column). You can find them all in the help page for select().Other tidy selection functions\n\n\n\n\n\n\nTo quote or not to quote?\n\n\n\nThe fact that you don‚Äôt use quotes around column names in tidyverse is confusing for many. As a rule of thumb, at this stage you should use quotes around column names, unless:\n\nyou are using $ operator (e.g.¬†df$ara)\nyou are using select(), mutate() or filter() from tidyverse\nyou are using another tidyverse function that uses tidy evaluation (look up the help page if unsure)\n\nOnce you start to program your own functions, the tidy evaluation will be an additional source of confusion, but burn that bridge when you get to it.\n\n\n\n\n4.2.3 Renaming columns\nThe select function has one more useful feature: you can directly rename the variables in the same call.\n\ndf2 &lt;- select(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"Gamma\"\n\n\n However, there is another function which can be used for renaming specific columns, aptly named rename()1. The way its syntax works it is well suitable for working with pipes (see below), and is a great deal easier than renaming the columns using colnames().rename()1¬†Unfortunately, there are many functions in R that are named rename(). Someone should really rename them (badum-tss).\n\ndf2 &lt;- rename(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"bera\"  \"Gamma\" \"dora\" \n\n\nAs you can see, no selection was made here, only renaming. Again, no quotes are needed around the column names.\n\n\n\n\n\n\n\nExercise 4.1 ¬†\n\nRead the file ‚ÄòDatasets/transcriptomics_results.csv‚Äô\nWhat columns are in the file?\nSelect only the columns ‚ÄòGeneName‚Äô, ‚ÄòDescription‚Äô, ‚ÄòlogFC.F.D1‚Äô and ‚Äòqval.F.D1‚Äô\nRename the columns to ‚ÄòGene‚Äô, ‚ÄòDescription‚Äô, ‚ÄòLFC‚Äô and ‚ÄòFDR‚Äô\n\n(Solution)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#sorting-and-ordering",
    "href": "day4-manipulating-your-data.html#sorting-and-ordering",
    "title": "4¬† Manipulating data frames",
    "section": "4.3 Sorting and ordering",
    "text": "4.3 Sorting and ordering\n\n4.3.1 Sorting and ordering vectors\nBefore we jump to sorting, filtering and ordering data frames, we should first discuss these operations on vectors. This is important, because although data frames can easily be sorted with functions such as arrange() from tidyverse, you will have situations in which you need to sort a vector, a list or another data type. Secondly, understanding the concepts of ordering will aid you also with your work with data frames.\n The basic R function for sorting is, appropriately, sort(). It is as simple as it gets:sort()\n\nvec &lt;- round(rnorm(10), 2)\nvec\n\n [1]  0.24 -0.42 -1.42 -0.29  0.95  0.89 -0.08  1.48  0.61  1.14\n\nsort(vec)\n\n [1] -1.42 -0.42 -0.29 -0.08  0.24  0.61  0.89  0.95  1.14  1.48\n\nsort(vec, decreasing=TRUE)\n\n [1]  1.48  1.14  0.95  0.89  0.61  0.24 -0.08 -0.29 -0.42 -1.42\n\n\nThe rnorm() function generates a vector of random numbers from a normal distribution (you met it on Day 2). The round() function with parameter 2 rounds the numbers to two decimal places. The sort() with the decreasing=TRUE argument sorts the vector in descending order.\n Amazingly, it is hardly ever used. Mostly we need the order() function. Rather than returning the sorted vector, it returns the indices of the sorted vector:order()\n\nord &lt;- order(vec)\nord\n\n [1]  3  2  4  7  1  9  6  5 10  8\n\n\nSo, what does that mean? Take the first element of ord. It is 3. This means that the first value to appear in the sorted vector should be the element number 3 of the original vector, which is -1.42 and the smallest number in vec (check it!). Likewise, the second element of ord is 2, which points to -0.42, the second smallest element of vec.\nSo what happens if we use the vector ord to select element from the vector vec? We get the sorted vector, that‚Äôs what:\n\nvec[ord]\n\n [1] -1.42 -0.42 -0.29 -0.08  0.24  0.61  0.89  0.95  1.14  1.48\n\n\n\n\n\n\n\n\n\nExercise 4.2 (Reverse sorting) How can you get the reverse order vector for vec?\n\n\n\n\n\n\n4.3.2 Sorting character vectors\n We can also sort character vectors, but the behavior might not be exactly what you expect. Take a look:Sorting character vectors\n\nchvec &lt;- c(\"b\", \"a\", \"Zorro\", \"10\", \"Anton\", \"A\", \"2\", \"100\", \"zxy\")\nsort(chvec)\n\n[1] \"10\"    \"100\"   \"2\"     \"a\"     \"A\"     \"Anton\" \"b\"     \"Zorro\" \"zxy\"  \n\n\nOK, so what happened here? The sort() function sorts the vector in lexicographical order ‚Äì just like you sort words in a dictionary (or, in the olden days, in a phone book). For the most part that seems quite intuitive, however take a look at the numbers: 100 comes before 2.\nLexicographical order means that the words are sorted first by first letter, then by second etc. So a word starting with a 1 always comes before a word starting with a 2, even if it means a number that is larger than 2.\nThis has one important implication for all of us who work with data. Identifiers, especially sample and patient identifiers are quite often a combination of letters and numbers, e.g.¬†ID1 or Patient10. This can cause problems when sorting, as ID10 will come before ID2:\n\nchvec &lt;- c(\"ID1\", \"ID2\", \"ID3\", \"ID4\", \"ID10\", \"ID20\", \"ID100\")\nsort(chvec)\n\n[1] \"ID1\"   \"ID10\"  \"ID100\" \"ID2\"   \"ID20\"  \"ID3\"   \"ID4\"  \n\n\nThere are several solutions for that, but here is one with the tools that you already know. First, we will extract the numbers from the strings, then we will sort the vector based on these numbers:\n\n# extract numbers\nchvec_n &lt;- str_replace_all(chvec, \"ID\", \"\")\n\n# convert to numeric\nchvec_n &lt;- as.numeric(chvec_n)\n\n# get the order\nord &lt;- order(chvec_n)\n\n# sort the original vector\nchvec[ord]\n\n[1] \"ID1\"   \"ID2\"   \"ID3\"   \"ID4\"   \"ID10\"  \"ID20\"  \"ID100\"\n\n\n\n\n\n\n\n\n\nExercise 4.3 (Sorting by last name) Here is a vector of names. Can you think of how to sort it by last name?\npersons &lt;- c(\"Henry Fonda\", \"Bob Marley\", \"Robert F. Kennedy\", \n  \"Bob Dylan\", \"Alan Rickman\")\n(Solution)\n\n\n\n\n\n\n4.3.3 Sorting data frames with order()\nBy now it should be clear that you can use this method to order data from data frames (or matrices, or other data types). We will show it on example of the transcriptomics_results.csv file that you have read in the Exercise¬†4.1. I assume that you have read the file and selected the columns with something like this:\n\nlibrary(tidyverse)\ntr_res &lt;- read_csv(\"Datasets/transcriptomics_results.csv\")\ntr_res &lt;- select(tr_res, \n                 Gene=GeneName, Description,\n                 logFC=logFC.F.D1, FDR=qval.F.D1)\n\nThe transcriptomics data set comes from a vaccine study (Weiner et al. 2019) and show the changes in gene expression after vaccination with a particular adjuvanted fluad vaccine. logFC stands for log fold change, and FDR is the false discovery rate (resulting from raw p-values being corrected using the Benjamini-Hochberg procedure). We will sort the data frame by decreasing logFC:\n\nord &lt;- order(tr_res$logFC, decreasing=TRUE)\nhead(tr_res[ord, ])\n\n# A tibble: 6 √ó 4\n  Gene     Description                                            logFC      FDR\n  &lt;chr&gt;    &lt;chr&gt;                                                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Q5D1D6   tc|Q5D1D6_CERAE (Q5D1D6) Guanylate binding protein 1,‚Ä¶  4.85 1.80e-14\n2 ANKRD22  ref|Homo sapiens ankyrin repeat domain 22 (ANKRD22), ‚Ä¶  4.81 5.55e-15\n3 ETV7     ref|Homo sapiens ets variant 7 (ETV7), transcript var‚Ä¶  4.65 2.53e-15\n4 SERPING1 ref|Homo sapiens serpin peptidase inhibitor, clade G ‚Ä¶  4.52 5.95e-16\n5 CXCL10   ref|Homo sapiens chemokine (C-X-C motif) ligand 10 (C‚Ä¶  4.38 1.51e-10\n6 GBP1P1   ref|Homo sapiens guanylate binding protein 1, interfe‚Ä¶  4.11 2.33e-17\n\n\n\n\n\n\n\n\n\nExercise 4.4 (Sorting data frames with order()) ¬†\n\nSort the data frame tr_res by increasing FDR\nSort the data frame alphabetically by Gene\n\n\n\n\n\n\n\n4.3.4 Sorting data frames with tidyverse\n Tidyverse makes it simple to sort the data frames with the arrange() function:arrange()\n\ntr_res &lt;- arrange(tr_res, desc(logFC))\nhead(tr_res)\n\n# A tibble: 6 √ó 4\n  Gene     Description                                            logFC      FDR\n  &lt;chr&gt;    &lt;chr&gt;                                                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Q5D1D6   tc|Q5D1D6_CERAE (Q5D1D6) Guanylate binding protein 1,‚Ä¶  4.85 1.80e-14\n2 ANKRD22  ref|Homo sapiens ankyrin repeat domain 22 (ANKRD22), ‚Ä¶  4.81 5.55e-15\n3 ETV7     ref|Homo sapiens ets variant 7 (ETV7), transcript var‚Ä¶  4.65 2.53e-15\n4 SERPING1 ref|Homo sapiens serpin peptidase inhibitor, clade G ‚Ä¶  4.52 5.95e-16\n5 CXCL10   ref|Homo sapiens chemokine (C-X-C motif) ligand 10 (C‚Ä¶  4.38 1.51e-10\n6 GBP1P1   ref|Homo sapiens guanylate binding protein 1, interfe‚Ä¶  4.11 2.33e-17\n\n\n Note the use of desc() function ‚Äì rather than specifying an argument like decreasing=TRUE.desc()\nUsing arrange(), it is easy to sort by multiple columns:\n\ntr_res &lt;- arrange(tr_res, logFC, FDR)\nhead(tr_res)\n\n# A tibble: 6 √ó 4\n  Gene            Description                                      logFC     FDR\n  &lt;chr&gt;           &lt;chr&gt;                                            &lt;dbl&gt;   &lt;dbl&gt;\n1 DSC1            ref|Homo sapiens desmocollin 1 (DSC1), transcri‚Ä¶ -1.97 3.33e-4\n2 XLOC_010084     tc|Q6FUE3_CANGA (Q6FUE3) Similarity, partial (1‚Ä¶ -1.76 7.59e-3\n3 LRRC69          ref|Homo sapiens leucine rich repeat containing‚Ä¶ -1.68 5.06e-2\n4 ZMAT4           ref|Homo sapiens zinc finger, matrin-type 4 (ZM‚Ä¶ -1.67 4.61e-2\n5 PCSK6           ref|Homo sapiens proprotein convertase subtilis‚Ä¶ -1.63 3.13e-2\n6 ENST00000456460 Unknown                                          -1.59 2.53e-2\n\n\nIt is also allowed to use the expressions like logFC * FDR to sort by the sum of the two columns (that doesn‚Äôt make much sense in this context, but you get the idea):\n\ntr_res &lt;- arrange(tr_res, logFC * FDR)\n\nabs()\n\n\n\n\n\n\n\nExercise 4.5 (Sorting data frames with arrange()) The abs() function returns the absolute value of a number, for example abs(c(-3, 7)) returns 3, 7. Use the arrange() function to sort the data frame tr_res by the decreasing absolute value of logFC.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#modifying-and-filtering-data-frames-in-tidyverse",
    "href": "day4-manipulating-your-data.html#modifying-and-filtering-data-frames-in-tidyverse",
    "title": "4¬† Manipulating data frames",
    "section": "4.4 Modifying and filtering data frames in tidyverse",
    "text": "4.4 Modifying and filtering data frames in tidyverse\n\n4.4.1 Modifying with mutate()\n Yesteday we have been busy cleaning up data sets. To modify the columns of the data frame, we have been using simple assignment. This works, but there is another method in tidyverse. Just like with regular assignments, the mutate() function allows both, creating new columns and modifying existing ones.mutate()\nBelow we create a new column logFC_abs which contains the absolute value of the logFC column:\n\ntr_res &lt;- mutate(tr_res, logFC_abs = abs(logFC))\n\nArguably, this is not much simpler than using simple assignment:\n\ntr_res$logFC_abs &lt;- abs(tr_res$logFC)\n\nThe main difference, as you see, is that mutate() operates on data frames ‚Äì it takes a data frame as the first argument and returns a data frame. The advantages will be obvious later, when you start to use pipes (see below).\n\n\n4.4.2 Using ifelse() with mutate()\n Say, you would like to create a new column that contains ‚Äúup‚Äù for genes that are upregulated (i.e.¬†logFC &gt;= 0) and ‚Äúdown‚Äù for genes that are downregulated (i.e.¬†logFC &lt; 0). We could do it with simple assignment and logical vectors:ifelse()\n\n# default value\ntr_res$direction &lt;- \"up\"\n\n# create logical vector\nnegative &lt;- tr_res$logFC &lt; 0\n\n# change \"up\" to \"down\" where logFC &lt; 0\ntr_res$direction[negative] &lt;- \"down\"\n\nHowever, this is a frequent operation and a much more convenient way exists. It is often used in combination with mutate, although it also can work in a normal assignment:\n\ntr_res &lt;- mutate(tr_res, \n                 direction = ifelse(logFC &lt; 0, \"down\", \"up\"))\n\nThe ifelse() function takes three arguments: a logical vector, a value to return if the logical vector is TRUE, and a value to return if the logical vector is FALSE. In other words, it ‚Äúconverts‚Äù a logical vector replacing TRUE with the second argument and FALSE with the third.\n\n\n4.4.3 Filtering with logical vectors and filter()\nAs you have learned, you can filter data frames using logical vectors. This is relatively straightforward. For example, we might want to create a table with only genes that are significant, that is have an FDR &lt; 0.05:\n\nsmall_fdr &lt;- tr_res$FDR &lt; 0.05\nsignificant &lt;- tr_res[small_fdr, ]\n\nIf you use nrow(significant), you will find that there are 1478 significant genes in the data set.\n The same can be achieved using the filter() function from tidyverse:filter()\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05)\n\nSimlarly, we can use the str_detect() function (which you learned about yesterday) to select only genes that contain the word ‚Äúinterferon‚Äù in the description:\n\ninterferon &lt;- filter(tr_res, \n                     str_detect(Description, \"interferon\"))\n\nThere are 80 such genes.\nBut what if we want to set two conditions? In statistics it is common to set a threshold not only for the p-value or FDR, but also for an effect size. In our case, the measure of the effect size is the log2 fold change. Thus, we might want to select only genes that are at leas 2-fold upregulated or down-regulated. Twofold upregulation corresponds to log2-fold change greater than 1, and 2-fold downregulation corresponds to log2-fold change less than -1.\nOne way is to use filter() with two parameters:\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05, abs(logFC) &gt; 1)\n\n However, there is a more elegant and versatile way. We can achieve the same effect by combining two logical vectors with the & operator:& (logical operator)\n\nlarge_fc &lt;- abs(tr_res$logFC) &gt; 1\nsignificant &lt;- tr_res[small_fdr & large_fc, ]\n\nOr, in tidyverse,\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05 & abs(logFC) &gt; 1)\n\nThe & operator is the logical AND operator. It combines two logical vectors, returning (at the given position) TRUE only if both vectors are TRUE. Take a look:\n\nvec1 &lt;- c(TRUE, TRUE, FALSE, FALSE)\nvec2 &lt;- c(TRUE, FALSE, TRUE, FALSE)\nvec1 & vec2\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nAt the first position both vectors are TRUE, so the result is TRUE. At position 2, the first vector is TRUE and the second is FALSE, so the result is FALSE. And so on. Try it.\n\n\n\n\n\n\n\nExercise 4.6 (Logical vectors) ¬†\n\nCreate a logical vector that selects genes that are both significant (FDR &lt; 0.05) and contain the string ‚Äúinterferon‚Äù in the Description. How many are there? How many genes are significant and do not contain ‚Äúinterferon‚Äù in the Description column?\nHow many of the genes containing ‚Äúinterferon‚Äù in the Description are are guanylate binding proteins? You can recognize the GBPs by their Gene symbol ‚Äì it starts with ‚ÄúGBP‚Äù. (Solution) gical\n\n\n\n\n\n\n\n4.4.4 Using the %in% operator\n Another useful utility in searching data frames is the %in% operator. It compares two vectors; for each element of the first vector, it returns TRUE if that element is in the second vector, and FALSE otherwise.%in%\n\nvec1 &lt;- c(\"a\", \"b\", \"c\")\nvec2 &lt;- c(\"a\", \"b\", \"x\", \"y\", \"z\")\nvec1 %in% vec2\n\n[1]  TRUE  TRUE FALSE\n\n\nThe elements 1, of the result are TRUE, because both a and b are in vec2. The last element, however, is FALSE, because c is not in vec2.\nWith %in% we can for example select a specified subset of genes that we are interested in.\n\ngenes &lt;- c(\"GBP1\", \"GBP2\", \"GBP3\", \"GBP4\", \"GBP5\", \"ANKRD22\")\nfilter(tr_res, Gene %in% genes)\n\n# A tibble: 7 √ó 6\n  Gene    Description                         logFC      FDR logFC_abs direction\n  &lt;chr&gt;   &lt;chr&gt;                               &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 GBP4    ref|Homo sapiens guanylate binding‚Ä¶  2.25 2.59e-21      2.25 up       \n2 GBP5    ref|Homo sapiens guanylate binding‚Ä¶  2.92 1.39e-20      2.92 up       \n3 GBP3    ref|Homo sapiens guanylate binding‚Ä¶  2.63 1.62e-15      2.63 up       \n4 ANKRD22 ref|Homo sapiens ankyrin repeat do‚Ä¶  4.81 5.55e-15      4.81 up       \n5 GBP2    ref|Homo sapiens guanylate binding‚Ä¶  1.8  1.17e-13      1.8  up       \n6 GBP1    ref|Homo sapiens guanylate binding‚Ä¶  2.76 7.38e-12      2.76 up       \n7 GBP3    ref|Homo sapiens guanylate binding‚Ä¶  1.27 3.40e- 3      1.27 up       \n\n\n\n\n\n\n\n\n\nExercise 4.7 Which of these genes are significant? Use the & operator to combine the result from %in% with the result of FDR &lt; 0.05.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#combining-data-sets",
    "href": "day4-manipulating-your-data.html#combining-data-sets",
    "title": "4¬† Manipulating data frames",
    "section": "4.5 Combining data sets",
    "text": "4.5 Combining data sets\n\n4.5.1 Binding data frames with rbind() and removing duplicates\nThe simplest way to combine two data frames is to bind them. This can be done horizontally (when you have the same number of observations ‚Äì rows) or vertically, by stacking them on top of each other, when you have the same columns. The two functions are, respectively, cbind() and rbind(), and they work for both, matrices and data frames.\n Let‚Äôs start with stacking them on top of each other. For example, we might have generated two data frames with interesting results: one with interferones and the other with guanylate binding proteins, and at some point we decide to have them in one data frame. Stacking them can be done with the rbind() (‚Äúrow bind‚Äù) function:rbind()\n\ninterferon &lt;- filter(tr_res, \n                     str_detect(Description, \"interferon\"))\ngbp &lt;- filter(tr_res,\n              str_detect(Gene, \"GBP\"))\nresults &lt;- rbind(interferon, gbp)\n\nNote that this works only if the data frames have the same columns. If you were to try to bind two data frames with different columns, you would get an error:\n\na &lt;- data.frame(a=1:5, b=6:10)\nb &lt;- data.frame(a=11:15, c=16:20)\nrbind(a, b)\n\nError in match.names(clabs, names(xi)): names do not match previous names\n\n\nOK, but if you have checked the interferon and gbp data frames, you would have noticed that there are some genes that are both interferones and guanylate binding proteins. You did not check, because I haven‚Äôt told you yet how to do it. Here is how:\nintersect()\n\nintersect(interferon$Gene, gbp$Gene)\n\n[1] \"GBP1P1\" \"GBP2\"   \"GBP1\"  \n\n\n The intersect() function returns the elements that are common to two vectors. As you can see, there are three: GBP1P1, GBP2 and GBP1. We can get rid of them from the results table thanks to the function duplicated(). For every element of a vector, duplicated() returns TRUE if the element occured previously in the vector. So for the vector c(1, 2, 1) the result will be FALSE, FALSE, TRUE - because the second 1 is duplicated.duplicated()\nTo remove the elements which are duplicated, however, we need to use the negation ‚Äì ! to get TRUE for every element which is not duplicated. Here it is in action:\n\n# count the number of the duplicated results\nsum(duplicated(results$Gene))\n\n[1] 19\n\n# number of rows before removing dups\nnrow(results)\n\n[1] 96\n\n# remove duplicates\nresults &lt;- filter(results, !duplicated(Gene))\n\n# how many are left?\nnrow(results)\n\n[1] 77\n\n\n\n\n\n\n\n\n\nExercise 4.8 You might have been expecting that there are only three duplicates; after all, only four genes were in common between interferon and gbp- So where do these other duplicates come from? What are they?\n\n\n\n\n\n\n4.5.2 Binding data frames with cbind()\nCombining data frames (or matrices) horizontally is much easier, because you do not have to worry about column names. You just need to make sure that the number of rows is identical2.\n2¬†Strictly speaking, the recycling rules apply. See Day 1. This can be sometimes useful, but usually it is better to be more explicit.cbind()\n\ndf1 &lt;- data.frame(a = 1:4, b=11:14)\ndf2 &lt;- data.frame(c=21:24)\ncbind(df1, df2)\n\n  a  b  c\n1 1 11 21\n2 2 12 22\n3 3 13 23\n4 4 14 24\n\n\nWe will use cbind() tomorrow when running a principal component analysis (PCA).\n\n\n4.5.3 Merging data sets\nQuite often you will have to merge two data sets. For example, you might have one Excel file containing the covariates for the patients (like BMI, age, gender etc.) and another file containing the gene expression data. Both data frames must have some identifiers present that allow us to match the two data frames.\nThis is not a simple binding operation, because you do not have the guarantee that (a) the identifiers between these two data sets are in the same order or even that (b) both data frames have exactly the same sets of identifiers. Actually, the reverse is more common that not.\nIn order to merge two data frames we need first to identify by what to merge them. In the code below we will concoct two data frames that share some of the identifiers.\n\nids_x &lt;- paste0(\"ID\", sample(1:8, 6))\ndf_x &lt;- data.frame(ID=ids_x, val_x=rnorm(6))\ndf_x\n\n   ID      val_x\n1 ID2 -1.9279142\n2 ID5 -1.5509956\n3 ID4  1.7265118\n4 ID7  0.5656358\n5 ID6  0.6835853\n6 ID1  1.1412166\n\nids_y &lt;- paste0(\"ID\", sample(1:8, 6))\ndf_y &lt;- data.frame(ID=ids_y, val_y=rnorm(6, 10, 1))\ndf_y\n\n   ID     val_y\n1 ID4 10.992675\n2 ID2  9.898389\n3 ID1 12.815613\n4 ID8  8.851284\n5 ID6  9.860257\n6 ID5  9.042042\n\nintersect(ids_x, ids_y)\n\n[1] \"ID2\" \"ID5\" \"ID4\" \"ID6\" \"ID1\"\n\n\nsample()\nThe sample() function is very useful ‚Äì it generates a random sample by choosing elements from a vector. This is what is called sampling and comes in handy in many situations. Here we use it to create two sets of IDs that are similar, but not identical. As you can see from the output of intersect(), only 5 elements out of the total 6 are common between the two data frames.\nMerging can be done either with the base R function merge(), or with corresponding *_join functions from the tidyverse. Whether you prefer one or the other is a matter of preference; I like merge() and we will be using it in the examples below.\nInner join, full join\nFirst, however, let us think how we want to merge, or, more specifically, what to do with the identifiers that are present in one data frame, but not in the other? That depends only on what we want to achieve. Possibly we are not interested in any elements that are not present in both data frames. This is what is called in database lingo an inner join Alternatively, we might want to keep all possible elements, whether they are found in df_x or df_y only. This is called an full join.\nmerge()\n\n# inner join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\")\njoined\n\n   ID      val_x     val_y\n1 ID1  1.1412166 12.815613\n2 ID2 -1.9279142  9.898389\n3 ID4  1.7265118 10.992675\n4 ID5 -1.5509956  9.042042\n5 ID6  0.6835853  9.860257\n\n# full join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all=TRUE)\njoined\n\n   ID      val_x     val_y\n1 ID1  1.1412166 12.815613\n2 ID2 -1.9279142  9.898389\n3 ID4  1.7265118 10.992675\n4 ID5 -1.5509956  9.042042\n5 ID6  0.6835853  9.860257\n6 ID7  0.5656358        NA\n7 ID8         NA  8.851284\n\n\nAs you can see, in the first case there are just 5 rows ‚Äì as many as there are common elements between the ID columns of the two data frames. In the second case, we get all 7 possible IDs. Where the ID was present in one, but not the other data frame, the values were filled up with NA‚Äôs. To get the full join we used the parameter all=TRUE, standing for ‚Äúuse all IDs‚Äù.\nNote: it is possible to omit the by= parameter specifying by what column to join the data frames. However, do yourself a favor and always specify the column to join on explicitely.\nLeft join, right join\nThere are two more situations, allthough less common in practice. We might want to keep all elements of df_x, but discard any elements of df_y which are not present in df_x. This is called a left join. Or, vice versa, we will discard only these elements which are present in df_x, but not in df_y. This is called, you guessed it, a right join.\n\n# left join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all.x = TRUE)\njoined\n\n   ID      val_x     val_y\n1 ID1  1.1412166 12.815613\n2 ID2 -1.9279142  9.898389\n3 ID4  1.7265118 10.992675\n4 ID5 -1.5509956  9.042042\n5 ID6  0.6835853  9.860257\n6 ID7  0.5656358        NA\n\n# right join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all.y = TRUE)\njoined\n\n   ID      val_x     val_y\n1 ID1  1.1412166 12.815613\n2 ID2 -1.9279142  9.898389\n3 ID4  1.7265118 10.992675\n4 ID5 -1.5509956  9.042042\n5 ID6  0.6835853  9.860257\n6 ID8         NA  8.851284\n\n\nAs you can see, in case of the left join, there are missing values in the right column only, and in case of the right join ‚Äì only in the left column.\nLet us summarize it in a table:\n\n\n\n\n\n\n\n\n\n\nType of join\nall x\nall y\nmerge options\ntidyverse alternative\n\n\n\n\ninner join\nFALSE\nFALSE\nmerge(x, y)\ninner_join(x, y)\n\n\nleft join\nTRUE\nFALSE\nmerge(x, y, all.x=TRUE)\nleft_join(x, y)\n\n\nright join\nFALSE\nTRUE\nmerge(x, y, all.y=TRUE)\nright_join(x, y)\n\n\nfull join\nTRUE\nTRUE\nmerge(x, y, all=TRUE)\nfull_join(x, y)\n\n\n\nYou don‚Äôt have to remember the names (inner, left, right, full) ‚Äì just remember the principle: sometimes you need the common elements, sometimes you need all elements, and sometimes you need all elements from x or y only.\nDifferent identifier column names. There is one other thing that you should know, because it happens quite often. The two data frames might have identifiers in columns that have different names, for example ID in one and PatientID in the other. You can, of course, rename the columns, but you can also specify the columns to join on explicitly:\nmerge() with different column names\n\ndf_x &lt;- data.frame(ID=ids_x, val_x=rnorm(6))\ndf_y &lt;- data.frame(PatientID=ids_y, val_y=rnorm(6, 10, 1))\n\n# inner join by ID\njoined &lt;- merge(df_x, df_y, by.x=\"ID\", by.y=\"PatientID\")\n\nThe resulting data frame has the identifiers in the column name specified in the ‚Äúx‚Äù data frame, that is ‚Äì in this case ‚Äì in the ID column.\n\n\n4.5.4 Complex merges\nThe situation above is quite trivial. In real world, unfortunately, the situations can be quite complex.\nFirst, there can be duplicates. For example, one data frame contains the meta data on a group of lab animals, while the second contains the results of the experiments at two time points. This is not a problematic situation, but you should understand what happens here:\n\nmeta_data &lt;- data.frame(ID=paste0(\"ID\", 1:4),\n                        age=sample(1:3, 4, replace=TRUE),\n                        group=rep(c(\"control\", \"treated\"), each=2))\nmeta_data\n\n   ID age   group\n1 ID1   3 control\n2 ID2   1 control\n3 ID3   1 treated\n4 ID4   3 treated\n\ncrp &lt;- data.frame(ID=rep(paste0(\"ID\", 1:4), each=2),\n                           time=rep(c(\"day1\", \"day2\"), 4),\n                           CRP=rnorm(8))\ncrp\n\n   ID time        CRP\n1 ID1 day1  0.4921237\n2 ID1 day2 -1.1239875\n3 ID2 day1 -0.3332624\n4 ID2 day2 -0.4621417\n5 ID3 day1 -0.1111603\n6 ID3 day2 -0.4514105\n7 ID4 day1 -0.4508619\n8 ID4 day2  0.8336788\n\nmerged_data &lt;- merge(meta_data, crp, by=\"ID\")\nmerged_data\n\n   ID age   group time        CRP\n1 ID1   3 control day1  0.4921237\n2 ID1   3 control day2 -1.1239875\n3 ID2   1 control day1 -0.3332624\n4 ID2   1 control day2 -0.4621417\n5 ID3   1 treated day1 -0.1111603\n6 ID3   1 treated day2 -0.4514105\n7 ID4   3 treated day1 -0.4508619\n8 ID4   3 treated day2  0.8336788\n\n\nAs you can see, each line in the meta_data data frame is duplicated to fill up the information matching the ID in the crp data frame.\nHowever, chances are the situation is more complex. Imagine that apart from the above measurements, you have another set of measurements, say, of the albumin (ALB) levels. You would like to merge the two data frames. Naturally, you expect that you will get a data frame with 8 rows. The following will not work as expected:\n\nalbumin &lt;- data.frame(ID=rep(paste0(\"ID\", 1:4), each=2),\n                      time=rep(c(\"day1\", \"day2\"), 4),\n                      ALB=rnorm(8))\n\n# incorrect code!!!\ncrp_alb &lt;- merge(crp, albumin, by=\"ID\")\ncrp_alb\n\n    ID time.x        CRP time.y        ALB\n1  ID1   day1  0.4921237   day1 -1.0180195\n2  ID1   day1  0.4921237   day2 -1.1287590\n3  ID1   day2 -1.1239875   day1 -1.0180195\n4  ID1   day2 -1.1239875   day2 -1.1287590\n5  ID2   day1 -0.3332624   day1  1.0130927\n6  ID2   day1 -0.3332624   day2  0.7661092\n7  ID2   day2 -0.4621417   day1  1.0130927\n8  ID2   day2 -0.4621417   day2  0.7661092\n9  ID3   day1 -0.1111603   day1 -0.7651393\n10 ID3   day1 -0.1111603   day2 -1.1168553\n11 ID3   day2 -0.4514105   day1 -0.7651393\n12 ID3   day2 -0.4514105   day2 -1.1168553\n13 ID4   day1 -0.4508619   day1 -1.3267826\n14 ID4   day1 -0.4508619   day2  0.4038912\n15 ID4   day2  0.8336788   day1 -1.3267826\n16 ID4   day2  0.8336788   day2  0.4038912\n\n\nUh-oh, what happened here?\nYou see, the problem is that the merge() function merges the data frames by the column ID, but it does not take into account the time column. Since it observes that ID‚Äôs are duplicated in both data frames, it creates all possible pairs of the rows: CRP from day1 with ALB day2; but also CRP from day1 and ALB from day1, CRP from day2 and ALB from day1 and so on. For each identifier. That is why we are getting 4 (and not 2) rows for each identifier.\n This is a common problem, and the solution is to merge by more than one identifier. The important question is: what combination of identifiers uniquely identifies a row in the data frame? In our case, it is the combination of ID and time. We can specify that in the by= parameter:Merging by more than one ID\n\ncrp_alb &lt;- merge(crp, albumin, by=c(\"ID\", \"time\"))\ncrp_alb\n\n   ID time        CRP        ALB\n1 ID1 day1  0.4921237 -1.0180195\n2 ID1 day2 -1.1239875 -1.1287590\n3 ID2 day1 -0.3332624  1.0130927\n4 ID2 day2 -0.4621417  0.7661092\n5 ID3 day1 -0.1111603 -0.7651393\n6 ID3 day2 -0.4514105 -1.1168553\n7 ID4 day1 -0.4508619 -1.3267826\n8 ID4 day2  0.8336788  0.4038912\n\n\n\n\n4.5.5 Bringing it all together\nWe are now coming to the final exercise in this section. This exercise is based on data from the vaccination study (Weiner et al. 2019). One file contains meta-data for the samples used in transcriptomic analysis ‚Äì it only includes the subjects and time points for which the gene expression data is present. The other file is quite large and contains the results of several laboratory tests taken for many patients at many time points.\nNote that the data is not real, just based on real data.\n\n\n\n\n\n\n\nExercise 4.9 (Merging large data sets) The files expression_data_vaccination_example.xlsx and labresults_full.csv contain data from the same study. The first contains some meta-data for samples for which gene expression has been measured, and the latter contains the results of laboratory tests for many patients in the trial. The goal here is to connect the RNA expression data with the laboratory values, so that one can analyze the relationship between the expression of certain genes and, say, the observed levels of inflammation.\n\nRead CSV file and the first sheet (‚Äútargets‚Äù) from the XLSX file.\nWhich columns contain the ID the subjects? Are there any subjects in common? (Hint: use the intersect() function)\nWhat other columns are common between the two data frames? Is the column with the ID of the subject sufficient to identify each row?\nWe are interested only in the following information: SUBJ, ARM (group), time point, sex, age, test name and the actual measurement. Are the measurements numeric? Remember, you can use expressions like [ , c(\"ARM\", \"sex\") ] or select(df, ARM, sex) to select the desired columns from a data set.\nUse the subjects and / or other information to merge the two data frames however you see fit. Note that there are multiple time points per subject and multiple measurements per subject and time point.\n\nSee Solution",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#pipes-in-r",
    "href": "day4-manipulating-your-data.html#pipes-in-r",
    "title": "4¬† Manipulating data frames",
    "section": "4.6 Pipes in R",
    "text": "4.6 Pipes in R\n\n4.6.1 Too many parentheses\nRemember how functions work? Each function takes some arguments and returns a value. And we can use that value to plug in back into another function. This allows us to write terse code, for example:\n\nvec &lt;- c(1, NA, 3, 10)\nwhich(is.na(as.numeric(vec)))\n\n[1] 2\n\n\nThis is quite easy to understand. Unfortunately, this can quickly become unreadable. Let us consider cleaning up the data from yesterday. We can do it one by one, like this:\n\nlibrary(tidyverse)\nlibrary(janitor)\niris_data &lt;- read_csv(\"Datasets/iris.csv\")\niris_data &lt;- clean_names(iris_data)\niris_data$species &lt;- tolower(iris_data$species)\niris_data$petal_length &lt;- str_replace_all(iris_data$petal_length ,\"[^0-9.]\", \"\") \niris_data$petal_length &lt;- as.numeric(iris_data$petal_length)\niris_data$sepal_length &lt;- str_replace_all(iris_data$sepal_length, \",\", \".\")\niris_data$sepal_length &lt;- str_replace_all(iris_data$sepal_length ,\"&gt; \", \"\") \niris_data$sepal_length &lt;- as.numeric(iris_data$sepal_length)\n\nThat is a lot of code. However, we could do it all on one line:\n\niris_data &lt;- \n  mutate(mutate(mutate(clean_names(read_csv(\"Datasets/iris.csv\")),\n  petal_length = as.numeric(\n  str_replace_all(petal_length, \"[^0-9.]\", \"\"))),\n  sepal_length = as.numeric(str_replace_all(\n  str_replace_all(sepal_length, \",\", \".\"), \"&gt; \", \"\"))),\n  species = tolower(species))\n\nIf you spend some time deciphering this, you will see that it does precisely the same operations as the previous code fragment. However, it is completely unreadable and unmaintainable3.\n3¬†I got a headache just trying to figure out the correct number of parentheses in this code.\n\n4.6.2 Pipes\n Fortunately, there is a dirty trick that results in clean and readable code: the pipe operator |&gt; (pronounced ‚Äúpipe‚Äù; in older code you might see ‚Äú%&gt;%‚Äù instead4).Pipe operator |&gt;4¬†The idea of the pipe operator was popularized by the magrittr package, which is a part of Tidyverse, as %&gt;%. R users found it so cool that with R version 4.1.0, the pipe operator |&gt; was included in the base R. There are some differences between |&gt; and %&gt;%, but for our purposes, they are equivalent.\nPipe operator allows for writing very clean, very convenient and very readable code. The basic idea behind the pipe operator is as follows: a |&gt; f(b) is the same as f(a, b). We can say it pipes the value of a to the function f. And if a is already the result of a function, say g(a), then g(a) |&gt; f(b) is the same as f(g(a), b). We build a pipe from the output of g() to the input of f().\nSo, for example, the following two lines are equivalent:\n\nvec &lt;- c(1, NA, 3, 10)\nas.numeric(vec)\n\n[1]  1 NA  3 10\n\nvec |&gt; as.numeric()\n\n[1]  1 NA  3 10\n\n\nPipe takes whatever is on its left side and inserts it in the parentheses of the function on its right side. Now think for a moment what it does to constructs like f1(f2(f3(f4(f5(x))))). Now compare this:\n\nwhich(is.na(as.numeric(vec)))\n\n[1] 2\n\nvec |&gt; as.numeric() |&gt; is.na() |&gt; which()\n\n[1] 2\n\n\nThe two lines are equivalent. However, the latter is much more readable: it shows how the value goes into as.numeric(), then the output of as.numeric() goes into is.na(), and the resulting logical vector goes into which().\nSee how that works for our iris dataset:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\") |&gt;\n        clean_names() |&gt;\n        mutate(species = tolower(species)) |&gt;\n        mutate(petal_length = str_replace_all(petal_length ,\"[^0-9.]\", \"\")) |&gt;\n        mutate(petal_length = as.numeric(petal_length)) |&gt;\n        mutate(sepal_length = str_replace_all(sepal_length, \",\", \".\")) |&gt;\n        mutate(sepal_length = str_replace_all(sepal_length ,\"&gt; \", \"\")) |&gt;\n        mutate(sepal_length = as.numeric(sepal_length))\n\nIt is just as clear and readable as the first example, but with some additional benefits.\nFirst, think what happens if you want to change the name of the variable, say, from iris_data to flowers. In the first example, you would have to change every single occurence of iris_data to flowers. Sure, it would not be all too bad if you can do it automatically with a search-and-replace, but what if the variable name was a?\nSecond, the pipe facilitates development. You build a pipe line by line, and each time you execute the code, all operations are repeated. This is a good thing ‚Äì you avoid the situation where you forget to execute one line and the code does not work.\nThere is an alternative to the above code which uses pipes, but not all on the same line:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\") |&gt;\n        clean_names() |&gt;\n        mutate(species = tolower(species))\n\niris_data$petal_length &lt;- iris_data$petal_length |&gt; \n        str_replace_all(\"[^0-9.]\", \"\") |&gt;\n        as.numeric()\n\niris_data$sepal_length &lt;- iris_data$sepal_length |&gt;\n        str_replace_all(\",\", \".\") |&gt;\n        str_replace_all(\"&gt; \", \"\") |&gt;\n        as.numeric()\n\nWhether you prefer this one or the previous is a matter of both, taste and context. The second is more appropriate if for each column you have to do a lot of manipulations.\nOne limitation is that in the form that is used here5, the value passed on from function to function is always the first argument. This is why we are doing the substitutions with the Tidyverse function str_replace_all() (which takes the character vector as the first argument) rather than the base R function gsub() (which takes the pattern as the first argument).\n5¬†This limitation can be circumvented either by using only named arguments, or by using a placeholder, _. See here for an explanation.\n\n\n\n\n\n\nExercise 4.10 (Botched meta data) Go back to the code that you have used yesterday for cleaning up the dataset from file meta_data_botched.xlsx. Use pipes to make it more readable.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#review",
    "href": "day4-manipulating-your-data.html#review",
    "title": "4¬† Manipulating data frames",
    "section": "4.7 Review",
    "text": "4.7 Review\nThings that you learned today:\n\nSelecting columns in data frames:\n\nusing square brackets\nusing select() from tidyverse\ntidy evaluation\nrenaming columns with rename()\n\nSorting, ordering, filtering\n\nsort() to sort a vector\norder() to get the order of a vector\norder() to sort a data frame\narrange() from tidyverse to sort a data frame\nfiltering with logical vectors\nfiltering with filter() from tidyverse\nhandling duplicates with duplicated()\ncombining logical vectors with &\nusing %in% operator\n\nMerging data frames\n\nrbind() and cbind()\nmerge() inner, left, right and full joins\nmerging by more than one column\n\nPipes in R\n\nusing |&gt; operator\nbuilding pipelines\n\nOther\n\nusing rnorm() to generate random numbers\nusing round() to round numbers\nsample() for generating random samples\n\n\n\n\n\n\nWeiner, January, David JM Lewis, Jeroen Maertzdorf, Hans-Joachim Mollenkopf, Caroline Bodinham, Kat Pizzoferro, Catherine Linley, et al. 2019. ‚ÄúCharacterization of Potential Biomarkers of Reactogenicity of Licensed Antiviral Vaccines: Randomized Controlled Clinical Trials Conducted by the BIOVACSAFE Consortium.‚Äù Scientific Reports 9 (1): 20362. https://www.nature.com/articles/s41598-019-56994-8.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html",
    "href": "day5-visualization-and-statistics.html",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "",
    "text": "5.1 Aims for today",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#aims-for-today",
    "href": "day5-visualization-and-statistics.html#aims-for-today",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "",
    "text": "working R markdown / Quarto\nbasic visualisations with R and ggplot2\nsimple statistics with R\nfinal example: principal component analysis",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#r-markdown",
    "href": "day5-visualization-and-statistics.html#r-markdown",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.2 R markdown",
    "text": "5.2 R markdown\n\n5.2.1 What is R markdown?\nR markdown is a remarkably simple to use and yet powerful tool. The basic idea is as follows: you write a document in a simple text format ‚Äì for example, a manuscript, a report, or even a thesis. In that document, you include R ‚Äúcode chunks‚Äù. These code chunks are executed by R, and the output is included in the document ‚Äì including figures, numbers or tables. And then, from this single simple text document you can create a PDF, a Word document, an HTML page (or a whole website!), a book, a slide presentation and much, much more.\n\n\n\n\n\nflowchart LR\n    A(R markdown / Quarto\\ndocument\\nwith R code) --&gt;|knitr| B(Markdown\\nwith R code output)\n    B --&gt; C[LaTeX]\n    C --&gt; CC[PDF]\n    B --&gt; D[Word]\n    B --&gt; E[HTML]\n    B --&gt; F[Presentation]\n    B --&gt; G[Book]\n\n\n\n\n\n\nIn fact, this book you are reading now has been entirely written in R markdown1. You can go to the github repository for this book and see the R markdown/Quarto files that generated it ‚Äì for example, here is the page you are reading right now. It is also possible to create a scientific paper completely in R markdown (here is an example of such a paper: Weiner 3rd, Obermayer, and Beule (2022)).\n1¬†Actually, in its successor called ‚ÄúQuarto‚Äù.Why is this so great?\n\nReproducibility: It allows you to keep the methods, analysis itself and the analysis results in one place.\nAvoiding errors: Whenever you re-run your analysis, R automatically updates all your figures, tables, results, etc. in your document. This prevents you from forgetting to update a figure or a number in your manuscript.\nFlexibility: You can easily change the output format of your document; you can update your figures or your tables easily without having to resort to manual editing.\nBibliography. As you have seen in this book, it is not hard to include bibliography in your document, in any style you desire. You can use a free package manager such as Zotero or Mendeley to manage your bibliography, and produce the bibliography in the format that R markdown uses.\nFocus on content. The minimal formatting required in R markdown allows you to focus on the content of your document, not on the formatting.\n\nWhat are the disadvantages?\n\nSteeper learning curve. You have to learn all the stuff first. Luckily, when it comes to R markdown, it is quite easy and there are plenty of resources, and after today you will know all the most important stuff.\nNo fine control over the layout. While you can easily use simple formatting commands, you will have to resort to more complex tools to control things like font size of the chapter headers2. Many people consider it to be a good thing ‚Äì this is the boring part that the computers should take care of, you shouldn‚Äôt worry about formatting but about your text, but sometimes it is annoying. However, you can always generate a Word file with R markdown that uses a Word template, and then fine tune it in Word.\nCollaboration. Collaborating with markdown is not as easy as with a Word document, because most people can‚Äôt use markdown. Often that means that you need to communicate with your co-authors using Word, and then patiently type in their changes into your markdown document.\n\n2¬†Actually, you can control the format for PDF and HTML output very precisely, but then you have to learn LaTeX and CSS, respectively. For Word, the only option is to use a Word file with pre-defined styles as a template.\n\n\n\n\n\nR markdown vs Quarto\n\n\n\n\nR markdown is older and more widely supported\nQuarto is newer, slightly more complex, with additional features and generally better looking\n\nDocuments created in Quarto can largely be processed with R markdown and vice versa, only some visual bells & whistles might be lacking.\nBoth Quarto and R markdown are available if you have installed RStudio. Standalone R installations without RStudio may require additional packages (e.g.¬†rmarkdown for R markdown) or programs (quarto for Quarto).\n\n\n\n\n\n\n\n\n\nExercise 5.1 (Create an R markdown document) Go to the ‚ÄúFile‚Äù menu in Rstudio and choose ‚ÄúNew File‚Äù -&gt; ‚ÄúR markdown‚Äù. Enter a title and click on ‚ÄúOK‚Äù. Save the file in your project directory and take a look at it. Rstudio has created a simple R markdown file for you so that you might get an idea of how it works.\nClick on the ‚Äúknit‚Äù icon in the toolbar to render the document. What do you see? How does it relate to the content of the document? Try changing a few words in the document and click on ‚Äúknit‚Äù again. What happens?\n\n\n\n\n\n\n5.2.2 Markdown basics\nThe ‚ÄúR‚Äù in R markdown stands for ‚ÄúR‚Äù, the programming language, combined with markdown. But what is markdown?\nMarkdown is a very lightweight formatting system, much like what many of us are using in emails or messengers, stressing words by surrounding them with stars etc., but with a few extra features. The idea is that you can write the text in a very simple way and it remains readable even with the formatting marks (take a look!). Basic markdown formatting\n\n\n\nCode\nOutput\n\n\n\n\n**bold**\nbold\n\n\n*italic*\nitalic\n\n\n3^2^\n32\n\n\nlog~2~\nlog2\n\n\n`code`\ncode\n\n\n[link to website](https://cubi.bihealth.org)\nlink to website\n\n\n\n\nHere is another feature: lists. Lists in markdown\n\n\nCode:\n- item 1\n- item 2\n  - subitem 1\n  - subitem 2\n- item 3\n\nResult:\n\nitem 1\nitem 2\n\nsubitem 1\nsubitem 2\n\nitem 3\n\n\n\n\nThere is much more to it (look up for example markdown guide from Rstudio or the Quarto markdown documentation), but you don‚Äôt need it right now. Just keep in mind that you can always take a look at the markdown source of this document to see how things can be done.\n\n\n\n\n\n\nMathematical formulas\n\n\n\nUsing a special syntax, it is possible to include virtually any mathematical formula in R, both inline variant (like \\(\\sigma=\\sqrt{\\frac{(x_i-\\bar{x})^2}{n}}\\)) or as a stand-alone block:\n\\[\\sigma=\\sqrt{\\frac{(x_i-\\bar{x})^2}{n}}\\]\nWhen you convert the R markdown document to Word, you will even be able to edit the formulas natively in Word.\nThere are several formulas in this book, if you are interested, look up the book quarto sources on github or check this guide.\n\n\n\n\n\n\n\n\n\nExercise 5.2 (Formatting markdown) ¬†\n\nTry some formatting commands in markdown. For example, try to make a word bold or italic. Try to create a list. Insert a link to a website.\nThis one is a bit harder, because I did not show it to you (on purpose). First download any kind of image from the internet and save it in your project directory. Then, using whatever means necessary (Google, the markdown guide, the Rstudio help), try to figure out how to insert an image in your document.\n\n\n\n\n\n\n\n5.2.3 R markdown header\nYou might have noticed that at the top of the first R markdown file you created there is a block of text that might look something like this:\n---\ntitle: \"Untitled\"\nauthor: \"JW\"\ndate: \"`‚Äãr Sys.Date()`\"\noutput: html_document\n---\nThis is a block with meta-information about your document. For example, it specifies the title, author and date. Note that date is updated automatically every time the document is rendered by executing the r command Sys.Date() (you will learn about the inline chunks in a moment). Naturally, you can run the command in your console or script. `Sys.Date()¬¥\n\nSys.Date()\n\n[1] \"2024-10-15\"\n\n\nThe output: specifies the type of output you would like to have.\n\n\n\n\n\n\n\nExercise 5.3 (Choosing output format) When editing your R markdown document, click on the little ‚ñº arrow next to the ‚ÄúKnit‚Äù icon in RStudio. Choose the PDF format. Observe what happens to the header of your document.\n\n\n\n\n\n\n5.2.4 R code chunks\nIn between the markdown text, you can include R code chunks. These are executed consicutively by R and the output is included in the document. Every single fragment of code in this book is a code chunk.\nEach chunk starts with a ```{r} and ends with a ``` (these should  be placed on separate lines). You can also add the chunks by clicking on the little green plus sign in the Rstudio toolbar and choosing ‚ÄúR chunk‚Äù. Here is an example of a code chunk:R chunks\n\n\nR markdown:\n```{r}\nx &lt;- 1:10\nprint(x)\nplot(x)\n```\n\n\n\nOutput:\n\nx &lt;- 1:10\nprint(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nplot(x)\n\n\n\n\n\n\n\n\n\n\nBy default, the code itself is also shown in the document. You can configure this by clicking on the little cogwheel ‚öô icon to the right of the chunk start. You will notice that configuring the options means essentially adding stuff like echo=FALSE to the chunk start (echo=FALSE  means that the code itself is not shown in the document).echo=FALSE\n\n\nR markdown:\n```{r echo=FALSE}\nx &lt;- 1:10\nprint(x)\nplot(x)\n```\n\n\n\nOutput:\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode chunks run in one environment\n\n\n\nCode chunks share their environment. That is, if you define a variable in one chunk, you will be able to use it in the subsequent chunks.\nThink of all the chunks as consecutive fragments of your script file!\n\n\nThe code chunks can also be inline, that is, you can put directly a code chunk in your sentence. For example, when I write that the \\(\\pi\\) constant is equal to 3.1415927, I am using an inline code chunk:\nFor example, when I write that the $\\pi$ constant is \nequal to `‚Äãr pi`, I am using an inline code chunk:\nThe point is not to save typing. The point is to update your document whenever you update your analysis. For example, if you add samples to your data and re-run the analysis, the number of samples that you have written in your method section must reflect the change. If you use an inline code chunk, it will do so automatically, without you needing to painstakingly update each number, each figure, each result in your manuscript.\n\n\n\n\n\n\n\nExercise 5.4 (Put your code in your markdown) ¬†\n\nGo back to Day 1, the water lillies example in Section 1.6.1. Copy the code to the R markdown, including the plot and Exercise¬†1.11.\nWrite a brief summary including inline code chunks that show the number of days of the simulation used.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#visualizations-with-r",
    "href": "day5-visualization-and-statistics.html#visualizations-with-r",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.3 Visualizations with R",
    "text": "5.3 Visualizations with R\n\n5.3.1 Basic principles of visualization\nBefore we launch into creating our own plots, let me give you a few tips about visualizations in general.\nEach plot is a way of communicating with your reader. You want to tell them something with that plot. Therefore, before you code anything, think what you want to tell the reader with the plot. What is the message, what is the story? How to best get your point across? What is that point?\nThis is really helpful, because we (and I do mean we all) have the tendency to want to document, to ‚Äúshow our data‚Äù: show everything that we got, all the results, all the measurements, all the experiments. Very often this results in cluttered, unreadable plots that do not really communicate anything apart from your prowess in R. While this may (or may not) be needed for things like supplementary data, usually it is not very effective.\nThe next step is to think how to communicate the message. Again, we all have a knee-jerk reflex to use the plots that we know how to do. This is why I would like to urge you to first take a piece of paper and a pencil and sketch a plot that you think would best communicate your message. Don‚Äôt worry about the details or how to implement it, just think how you would like it to look.\nIn addition, you can always look at how other people have visualized similar data and get inspired. Not to copy, but to see which way of visualizing is convincing for you, which plots are clear and fantastically communicative, and which are hard to read and to understand.\nFinally, when creating the plot, think about accessibility. Thus, ask yourself. Are your fonts and symbols large enough to be read by a person with normal vision even if the plot is scaled down to fit a multi-panel illustration? Are the colors color-blind friendly3?\n3¬†Around 5%-8% of all males and 0.5%-1% of all femals are color-blind. When I was a young post-doc I created an interface to single-nucleotide polymorphism data in a sequencing project. I showed synonymous mutations with green arrows and non-synonymous mutations with red arrows. Very proud, I presented it to the twenty or so members of the consortium. One of them stood up and said: ‚ÄúI am color-blind. I can‚Äôt see shit on your plot‚Äù.\n\n\n\n\n\nVisualization principles\n\n\n\n\nMessage: think what the message of your plot is\nDesign: make a sketch on how to best communicate the message\nInspiration: look at other people‚Äôs plots for inspiration\nSimplicity: less is more\nAccessibility: make your fonts large and colors color-blind friendly\n\n\n\n\n\n5.3.2 Base R vs ggplot2\nWe already had simple examples of plots in R (for example on day one, in the Water Lily example - Section 1.6.1). We even had a look at the ggplot2 package on Day 3 (Section 3.5.5). Why are there so many plotting systems in R?\nActully, the situation is even more complex than you might imagine. Partly that is because there are so many different types of plots that it is hard to make a single system that would be good for all of them. Partly because when R was first created, many modern graphical formats and features did not exist yet. But the main problem, as usual, is that anybody can write their own. And so they did.\nFortunately for us, the two graphic systems ‚Äì base R and ggplot2 ‚Äì are sufficient even for the most sophisticated plots. It is useful to know them both, however. Base R is simple and allows very quick plotting of simple tasks. Moreover, many base R statistical functions have built-in plotting capabilities (e.g.¬†you can simply call plot(model) for a linear regression model to get all relevant plots). ggplot2, on the other hand, is more complex, but it is also working on a much higher level and takes care of many things automatically.\nHere is a table comparing the basic features of the two systems.\n\n\n\n\n\n\n\nBase R\nggplot2\n\n\n\n\nSimple to use for simple task, gets very complex for complex tasks\nMore complex, but more powerful and makes complex tasks easier\n\n\nMany packages\nMany packages\n\n\nPlots need many lines to be customized\nPlots are customized in one line\n\n\nLow-level, with absolute feature control\nHigh-level, with automatic feature control\n\n\n\nMost of what can be done with ggplot2 can be done with base R, but it often requires many more lines of code. On the other hand, it is easier to develop a de novo approach with base R, because programming new features in ggplot is not for the faint of heart.\nLet us plot a simple scatter plot with both systems and show how customization will look like in both. We will use the iris data set4.\n4¬†Not the doctored version we have been loading in the previous days, but the built-in original data set. If you have created an iris variable in your environment, type rm(iris) to remove it.Base R:\n\ncolors_map &lt;- c(setosa=\"red\", versicolor=\"green\", virginica=\"blue\")\ncolors &lt;- colors_map[iris$Species]\nplot(iris$Sepal.Length, iris$Sepal.Width, \n     col=colors, pch=19, xlab=\"Sepal length\", \n     cex=1.5,\n     ylab=\"Sepal width\", main=\"The iris data set\")\nabline(-2.8, 1.1, col=\"red\")\nlegend(\"topright\", legend=unique(iris$Species), \n       col=colors_map[unique(iris$Species)], pch=19)\n\n\n\n\n\n\n\n\nNote that in the code above the col= parameter simply takes a vector describing colors. Color management must be done by the user ‚Äì you cannot simply say ‚Äúchoose the viridis palette‚Äù (or similar). You also need to remember some weird things, like that the pch=19 means ‚Äúfilled circles‚Äù (in contrast, pch=1 means ‚Äúempty circles‚Äù, and if you need triangles, you have to use pch=2)5. And cex is the size of the points.\n5¬†For many years I had a piece of paper hanging over my desk with all the pch values written on it and symbols scrawled with pencil next to them.Legend drawing is a completely separate function that slaps the legend on top of the existing plot, whatever it is. The abline() function draws a line on the plot, also not really caring what was drawn before.\nGgplot2:\n\nlibrary(ggplot2)\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size=3) +\n  geom_abline(intercept = -2.8, slope = 1.1, color = \"red\") +\n  scale_color_viridis_d() +\n  labs(x = \"Sepal length\", y = \"Sepal width\", \n       title = \"The iris data set\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this example you see why ggplot2 is so popular. The code is much cleaner and does not require you calling separate functions for each task. The parameters have explicit names, easy to remember. Legend and abline are directly connected to the plot, and the color scale is chosen with a single command, scale_color_viridis_d(). There is no need to separately change colors for the legend and the plot. The theme_minimal() function changes a whole range of parameters such as the font face used or the background color to achieve a ‚Äúminimal look‚Äù. scale_color_viridis_d(), theme_minimal()\nNoteworthy is also the labs() function, which allows you to change the labels of the axes and the title of the plot in one go (you can also use individual functions like xlab(), ylab() and ggtitle()). labs(), xlab(), ylab(), ggtitle()\nHowever, some operations can be challenging. If you want your legend placed directly on top of the drawing in the right upper corner, you have to specify the coordinates manually. And if you want triangles‚Ä¶ well, you still have to use the numbers, for example using geom_point(shape=2) for triangles6.\n6¬†And here the punchline: I still have this piece of paper with symbols scrawled on it.7¬†You basically have to calculate the LOESS lines manually and then manually plot them with lines() and / or polygon().Then again, some complex operations are easy. For example, to add per-group LOESS lines to the plot with confidence intervals, you would simply add geom_smooth(method=\"loess\"). In base R, the same operation requires about a dozen lines of code7.\n\n\n\n\n\n\n\nExercise 5.5 (Ggplot2) Take the code for the ggplot2 plot above and make the following modifications:\n\nTry adding a geom_smooth() layer\nRemove scale_color_viridis_d() layer. What happens?\nAdd the following layer: scale_color_manual(values=colors_map). What happens?\nChange the color of all points to red (use the color parameter in geom_point())\nIf you wanted to show different shapes for different species rather than different colors, where would you change the plot? Hint: where is the species mentioned in the code?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size=3) +\n  scale_color_manual(values=colors_map) +\n  # LOESS smoothing, separate for each Species\n  geom_smooth() +\n  geom_abline(intercept = -2.8, slope = 1.1, color = \"red\") +\n  labs(x = \"Sepal length\", y = \"Sepal width\", \n       title = \"The iris data set\") +\n  theme_minimal()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Esthetics and information channels\nThe last point in the exercise was sneaky. The answer is: you would need to change the aes() function call. Rather than use color=Species, you would have to use shape=Species.\nThe aes() function ‚Äì and the concept of plot esthetics ‚Äì is at the core of ggplot2. The idea is that a plot has different information channels, such as x position, y position, color, shape, size, etc. Each of these can be mapped or linked to a column in the input data frame. This is the job of aes(). The geom functions such as geom_point() then use these mapping to actually draw the plot.\n                    +------------+\nPetal Width ------&gt; |            | -------&gt; x position\nPetal Length -----&gt; |  aes()     | -------&gt; y position\nSpecies ----------&gt; |            | -------&gt; color\n                    +------------+\nThis is why the layers (geom_point(), scale_color_viridis_d(), etc.) are being added to the plot using the + operator. Each layer adds another piece of information on how to display the plot. Then, the plot is displayed all in one go.\nThe assumption is also that this information is mapped automatically. You do not need (or should not need) to specify which precisely color which groups get, or which symbols are used for plotting. Rather, you chose a particular information channel (‚Äúuse different shapes depending on species‚Äù) and ggplot2 takes care of the rest.\nOf course, it is still possible to manually specify the colors, shapes etc. For example, and as you have seen in the previous exercise, you can use the same manual mapping of colors as above by using scale_color_manual(values=colors_map)8.\n8¬†I do not really show here how to use the various scale_color_.... and scale_fill_... functions. There are so many use cases, and so many options, that it is hard to give a general overview. Use the google, Luke, or take a look at the R Graphics Cookbook.\n\n5.3.4 Boxplots and violin plots\nFor continuous data, violin plots combined with boxplots or boxplots alone are the method of choice. In the following, we will use the sepal lengths of the iris dataset9. First, a boxplot. We have created a boxplot a while ago on day 2 using the R base function boxplot() (Section 2.4.3). Now we will do the same with ggplot2. geom_boxplot()\n9¬†Not the doctored version we have been loading in the previous days, but the built-in original data set. If you have created an iris variable in your environment, type rm(iris) to remove it.\nlibrary(ggplot2)\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe thick line in the middle of the box is the median, the box itself goes from the lower quartile to the upper quartile (so its vertical size is the interquartile range), and the whiskers show the range of the data (excepting the outliers). The points shown are outliers. All in all, it is a non-parametric representation ‚Äì that is, we do not assume that the data is normally distributed and we can use it for any kind of continuous data.\nHowever, boxplots are still not perfect, as they do not show the actual distribution of the data. Here is a better method ‚Äì we create a so-called violin plot which extrapolates a distribution from the data points.  In addition, we overlay that with a boxplot to show the quartiles and outliers.geom_violin()\n\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_violin() + \n  geom_boxplot(width=0.1)\n\n\n\n\n\n\n\n\nFinally, in cases where we do not have too many data points we might want to show them directly on the plot (we can combine it with a boxplot or a violin plot). Without loading any additional packages, you can use geom_jitter() to show the points:\n\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_boxplot(outlier.shape=NA) +\n  geom_jitter(alpha=.5)\n\n\n\n\n\n\n\n\nHowever, this is not the optimal way to show the data points, because it is hard to see the actual distribution. A particularly fine way of showing the actual points is implemented by the ggbeeswarm package and its geom_beeswarm() function. geom_beeswarm(), ggbeeswarm\n\nlibrary(ggbeeswarm)\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_boxplot(outlier.shape=NA) +\n  geom_beeswarm(alpha=.5)\n\n\n\n\n\n\n\n\nOK, but what if we want to show how the different variables (Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) differ between the species? One possibility would be to produce four figures like the one above, one after another. But maybe we would like to have all box plots on one figure? Yes, that is totally doable!\nFirst, however, we need to prepare a data frame which contains the values of all the variables in one column. We could use the pivot_longer() function from the tidyverse here, but I haven‚Äôt shown you this one yet, so if you would like to learn more about it, read the Appendix ‚ÄúWide vs long data‚Äù.  Instead, we will do it by hand.pivot_longer()\n\nlibrary(tidyverse)\ndf &lt;- data.frame(species = rep(iris$Species, 4),\n  measurement = rep(c(\"Sepal.Length\", \"Sepal.Width\",\n                   \"Petal.Length\", \"Petal.Width\"), each=150),\n  value = c(iris$Sepal.Length, iris$Sepal.Width,\n            iris$Petal.Length, iris$Petal.Width)\n)\ndim(df)\n\n[1] 600   3\n\n\nMake sure that you understand what is happening above. We are creating a data frame which has three columns: species, measurement and value; in the value column, we first put all the sepal lengths, then the sepal width etc. The resulting data frame is therefore four times longer than the initial one (with \\(4 \\times 150 = 600\\) rows), which is why we need to repeat the Species column four times10.\n10¬†If you have followed the ‚Äúrecycling‚Äù part on day 1 closely, you might think that it is not necessary to use rep() here ‚Äì and you are absolutely right! However, I urge you to do such operations explicitely until you are very comfortable with R, because it is easy to make mistakes with recycling otherwise.One possible solution is to assign the measurement to the x position channel, and assign the species to another channel, for example the color:\n\nggplot(df, aes(x=measurement, y=value, color=species)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nOK, that is already not bad. But now let me now to introduce you to the facet_wrap() function. You add it to the plot like another layer, but what it does is very clever: it separates the plot into subplots based on an additional variable ‚Äì in this case, we use the measurement column. facet_wrap()\n\nggplot(df, aes(x=species, y=value)) +\n  geom_boxplot() +\n  facet_wrap(\"measurement\", ncol=4, scales=\"free_y\")\n\n\n\n\n\n\n\n\nThere is a lot one can do with facet_wrap() (and its cousin, facet_grid()).  Here we see two additional parameters. The ncol parameter specifies the number of columns ‚Äì setting it to two would generate a \\(2 \\times 2\\) grid of subplots. The scales=\"free_y\" parameter means that the y axis is not identical on all subplots ‚Äì it makes sense here, because the different types of measurements are in different ranges; but in some other cases, you might want to omit it, or set the \\(x\\) scales to be free instead.facet_grid()\nOne more thing. You might be tempted to create a plot with the so-called bar plots. This might look something like this: Bar plots\n\n\n\n\n\n\n\n\n\nI am sure you have seen a plot like that many times, and that you might be tempted to recreate it. Don‚Äôt. This plot is a terrible way to show continuous data. Bar plots in general should only be used in very special cases, like when you show proportions or absolute counts (say, number of votes). In all other cases you should use a boxplot combined with a violin plot or actual data points. If you want to know more about this, check this section in the Appendix.\n\n\n\n\n\n\nShow actual data\n\n\n\nIf possible, always strive to show the actual data points (with geom_beeswarm() or geom_jitter()) in addition to the summary statistics. If not, at least show the distribution (with geom_violin()). Never use a bar plot.\n\n\n\n\n5.3.5 Heatmaps\nA very common type of figure in high throughput data setting and one which is hard to achieve with other tools is the heatmap. There are numerous packages and functions (including the base R function heatmap()), however we will use the pheatmap() function from the pheatmap package. pheatmap()\nFirst, however, we need some data. For starters, we take our beloved iris data set; however we will chose only 10 flowers from each species. You haven‚Äôt learned the following idiom yet, but here is how to do it efficiently in tidyverse:\n\nlibrary(tidyverse)\n\niris_sel &lt;- iris |&gt;\n            group_by(Species) |&gt;\n            slice_sample(n=10) |&gt;\n            ungroup()\ntable(iris_sel$Species)\n\n\n    setosa versicolor  virginica \n        10         10         10 \n\n\nThe group_by() function groups the data by the Species column, which means that the subsequent functions will affect each group separately. The slice_sample() is a specialized function only for this purpose ‚Äì it randomly selects rows from a data frame. Finally we remove the grouping with the ungroup() function. group_by(), slice_sample(), ungroup()\n\nlibrary(pheatmap)\niris_mtx &lt;- t(as.matrix(iris_sel[, 1:4]))\npheatmap(iris_mtx)\n\n\n\n\n\n\n\n\nAs you can see, we have first converted the first four columns of the matrix into a data frame, then transposed it (‚Äúflipped‚Äù so the rows become columns and vice versa) and finally plotted it with pheatmap(). pheatmap(),t()\nHowever, we do not see the species information on the plot. We can add it using a special data frame that contains the species information. We will also add a few more parameters to the pheatmap() function to make the plot more readable.\n\niris_species &lt;- data.frame(species=iris_sel$Species)\nfoo &lt;- pheatmap(iris_mtx, labels_col=iris_sel$Species,\n  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(100)\n)\n\n\n\n\n\n\n\n\n\n\n5.3.6 Output formats\nWe will not be spending time here on the details of how to create different output formats for plots; however, there is one thing that we want to mention.\nIn general, there are two types of graphical files: raster (bitmap) and vector graphics. Raster graphics are made of pixels, like photos, and are good for complex images. Vector graphics are made of lines and shapes, like drawings, and are good for plots.\nThe two graphics below look identical, however, in the HTML version of this book, the left one is a raster image, and the right one is a vector graphics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry magnifying these images ‚Äì right click, select ‚Äúopen image in a new tab‚Äù (or similar) and then zoom in. You will see that the left image gets pixelated, while the right-hand one remains sharp:\n\n\n\nRaster vs vector graphics\n\n\nThe advantage of vector graphics is not only that you can zoom in as much as you want and the image will not get pixelated. First and foremost, you can edit them using an appropriate tool. Many people use the commercial Adobe Illustrator for this task, but there are also free and open source tools like Inkscape. Both these latter tools will do the job in a scientific setting ‚Äì they will allow you to change the colors, the fonts, the line thickness, the text shown etc. of your plots.\nTherefore, if possible, you should always save your plots in a vector format. You can always convert them to a raster format later, but once a plot is in a raster format, converting it to a vector format is practically impossible. The only exception is when you have an image with millions of data points ‚Äì sometimes this may challenge your computer‚Äôs memory (vector graphics is more computationally intensive than raster graphics).\n\n\n\n\n\n\nVector graphics\n\n\n\nIf possible, use a vector graphic format for your plots. You can always convert them to a raster format later, while the opposite is not true.\n\n\nR can produce vector graphics in the PDF and in the SVG format. Both formats can be edited in Inkscape or Illustrator, but the SVG format is also suitable for HTML pages, because SVG is a standard related to HTML ‚Äì in fact, all the plots in this book are in SVG format.\nTo choose SVG or PDF format, you have the following options:\nUse Rstudio. In the right hand ‚ÄúPlots‚Äù tab on the lower right panel, you can click on ‚ÄúExport‚Äù and choose one of the available formats, including PDF or SVG.\nUsing R markdown or Quarto. You can either use global document options or per-chunk options.\nFor global options, include the following code at the beginning of your R markdown file, just after the header:\n```{r include=FALSE}\nknitr::opts_chunk$set(dev=\"svg\")\n```\nAlternatively, insert this into the header:\nknitr:\n  opts_chunk:\n    dev: svg\nIf you want to make sure that one specific chunk produces SVG, you can always set the option in the given chunk:\n```{r dev=\"svg\"}\nplot(1:10)\n```\nThe only problem with this method is that if you create a Word document, the output will invariably be a raster image in your document.\n\n\n\n\n\n\nWord and R plots\n\n\n\nWhen you create a Word document with Rmarkdown, never copy your plots from that file! Either create a standalone file (see below for instructions or use the ‚ÄúExport‚Äù button in Rstudio), or copy the SVG graphics from the HTML file.\n\n\nDirectly creating graphics in your script. If you want your script to produce a file with the plot, you can do that in one of the many ways. Two of them are shown below, one using svg()11, and the other one using pdf() or cairo_pdf()12. Both commands need to be finalized with dev.off():\n11¬†If you experience problems with SVG generated in R, you can try to use the svglite() function from the svglite package which produces a more standard-compliant SVG files.12¬†The cairo_pdf() supports a wider range of characters, including Unicode characters, and allows font embedding. Long story short, use it if your fonts are garbled.\n# producing an SVG image file\nsvg(\"test.svg\", width=14, height=7)\nplot(1:10)\ndev.off()\n\nThis produces an SVG file ‚Äútest.svg‚Äù with the nominal size 14 x 7 inches (read more in the Appendix regarding image and font sizes).\n\n# produce a PDF file\npdf(\"test.pdf\", width=14, height=7)\nplot(1:10)\ndev.off()\n\nThis produces a PDF file ‚Äútest.pdf‚Äù with the nominal size 14 x 7 inches.\n\n\n\n\n\n\nVector graphics with Microsoft Office\n\n\n\nIf you rely heavily on Microsoft Office for your work, you might find another vector graphic format more useful ‚Äì the Windows Meta File (WMF). The function is called win.metafile() and used in the same way as the pdf() function. The problems are that (a) you can only use it on Windows and (b) it does not support transparency.\n\n\n\n\n\n\n\n\nRemember!\n\n\n\nUse SVG format for your plots whenever possible.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#basic-statistics-with-r",
    "href": "day5-visualization-and-statistics.html#basic-statistics-with-r",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.4 Basic statistics with R",
    "text": "5.4 Basic statistics with R\n\n5.4.1 Statistics with R\nR is a powerful tool for statistics. It has a staggering number of packages that can be used for statistical analysis. I would venture the guess that if someone somewhere came up with a statistical method, then there is an R package for it13.\n13¬†Almost the same goes for bioinformatics and visualization.One interesting fact about statistics is that is is way harder than programming in R. In fact, after some initial learning curve, you will find that it is much harder to understand, say, which statistical test you should use than how to actually run it in R. You will also see that it is harder to understand the plots that you have produced then to actually produce them.\nNonetheless, it takes a while to get into the ‚ÄúR mindset‚Äù of doing statistics. It is done very much differently than in UI-based software like Graphpad Prism or Excel.\n\n\n\n\n\n\n\nExercise 5.6 (P values) P-values are one of the most basic concepts in statistics, and part of the language of science ‚Äì it is hard to find a scientific paper without any p-values.\n\nTake a piece of paper or open a text editor and write down a one- or two-sentence explanation of what a p-value is.\nOnly when you have done this, read the section ‚ÄúClarifications about p-values‚Äù in this Wikipedia article. Did you get it right? If yes, you are in the minority of scientists.\n\n\n\n\n\n\n\n5.4.2 Descriptive statistics\nYou have already seen some basic statiscs in R (see Day 1, for example  Tip¬†1.1). Here is a quick reminder of how to get some basic statistics for a vector of numbers:Descriptive statistics\n\nx &lt;- rnorm(1000)\n\n# mean\nmean(x)\n\n[1] -0.03872947\n\n# median\nmedian(x)\n\n[1] -0.04420835\n\n# standard deviation\nsd(x)\n\n[1] 1.048951\n\n# Some basic statistics in one go\nsummary(x)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.49291 -0.76969 -0.04421 -0.03873  0.68598  3.19717 \n\n\nFor a vector of numbers, the summary() function returns a vector with six numbers. Four of them should be self-explanatory: the minimum, median, mean, and maximum. The other two are the first and the third quartile.\nThe first number is the 1st quartile, also called ‚Äúthe lower quartile‚Äù or  25th percentile, which means that 25% of the data are below that number. The second is the 75th percentile (or the upper quartile), which means that 75% of the data are below the number. Together, between the first and the second number, there are 50% of the data. You can check it for yourself:Quartiles\n\ns &lt;- summary(x)\nsum(x &lt; s[2]) / length(x)\n\n[1] 0.25\n\nsum(x &gt; s[5]) / length(x)\n\n[1] 0.25\n\nsum(x &gt; s[2] & x &lt; s[5]) / length(x)\n\n[1] 0.5\n\n\nOf particular interest is the so called interquartile range (IQR), which is the difference between the upper and lower quartile.  Just like the median is a robust, non-parametric measure of the ‚Äúcenter‚Äù of the data, the IQR is a robust, non-parametric measure of the ‚Äúspread‚Äù of the data. While median corresponds to the parametric mean, IQR corresponds to the standard deviation.Interquartile range\nIn non-normally distributed data, for example count data (when we count things) median and IQR are often more informative than mean and standard deviation. We will see that later when we come to visualizing data with bar plots.\nYou can get the IQR using the IQR() function: IQR()\n\nIQR(x)\n\n[1] 1.45567\n\n\nFor data frames, you can either use the summary() function or use the skim() function from the skimr package (see Section 3.3.1 and Section 3.3.2).\n\n\n5.4.3 Simple tests\nMost everyday statistical tests are available in base R without the need for loading any additional packages. Here we will show you some of them.\nt-test. The Student‚Äôs test is one of the most common tests in all statistics. It compares two groups of data. In its simplest form, the function t.test() takes two vectors of numbers. t.test()\n\n# simulate two vectors with different means\na &lt;- rnorm(15, mean=1, sd=1)\nb &lt;- rnorm(15, mean=3, sd=1)\n\n# perform the t-test\nt.test(a, b)\n\n\n    Welch Two Sample t-test\n\ndata:  a and b\nt = -6.2053, df = 27.836, p-value = 1.082e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.768422 -1.394013\nsample estimates:\nmean of x mean of y \n0.9100471 2.9912649 \n\n\nThe actual output of the t.test() function is a list with a lot of information, but when printed to the console it shows a human-readable summary.\nIt is often useful to extract and format the p-value from the output of the t.test() function. You can save the t.test output to a variable and then access the element p.value:\n\nres &lt;- t.test(a, b)\nres$p.value\n\n[1] 1.082117e-06\n\n\nTo show the p-value in a more readable format, you can use the format.pval  function which converts the number to a string with the correct number of significant digits:format.pval(..., digits=2)\n\nformat.pval(res$p.value, digits=2)\n\n[1] \"1.1e-06\"\n\n\nThis allows us to include the p-value in a sentence, for example like this:\n\"The p-value of the t-test \nwas `‚Äãr format.pval(res$p.value, digits=2)`.\"\nThis will be rendered as: ‚ÄúThe p-value of the t-test was 1.1e-06.‚Äù\nIf we doo many tests or want to save the test results in a spreadsheet, it might be useful to use the tidy() function from the broom package. tidy()\n\nlibrary(broom)\ntidy(res)\n\n# A tibble: 1 √ó 10\n  estimate estimate1 estimate2 statistic    p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    -2.08     0.910      2.99     -6.21 0.00000108      27.8    -2.77     -1.39\n# ‚Ñπ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nThe tidy() function returns a data frame with the test results. It understands many different statistical tests, not only the t-test.\n\n\n\n\n\n\n\nExercise 5.7 (T-test) Use either the builtin iris dataset or the cleaned-up version of the doctored data set from Day 3. Perform a t-test to compare the sepal length between Iris setosa and Iris versicolor, as well as between Iris versicolor and Iris virginica.\n\n\n\n\nWilcoxon test. The Wilcoxon test is a non-parametric version of the t-test, and almost as powerful. Use it via wilcox.test(): wilcox.test()\n\nwilcox.test(a, b)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  a and b\nW = 9, p-value = 1.251e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nAs you can see, the p-value is quite similar to the t-test, but we did not have to worry whether the data is normally distributed or not14.\n14¬†The t-test assumes normality, while the Wilcoxon test does not. However, despite that, the t-test is quite robust to non-normality, which means that it will work quite well even if the data is not quite normally distributed.Paired tests. Both the t-test and the Wilcoxon test can be used in a paired version. The paired variant is a special case of the regular t-test or Wilcoxon test where  the two groups are not independent. For example, you might have measured the same individuals before and after a treatment. You are not allowed to use a regular test if the data is paired, because one of the most fundamental and important assumptions is not met ‚Äì the assumption of independence. Also, in many cases, using a paired test will give you more statistical power, as we will see shortly.Paired t-test with paired=TRUE\nConsider this example. We first start with randomized vector a, then build vector b by adding a fixed effect and an error term:\nboxplot()\n\na &lt;- rnorm(15, mean=1, sd=1)\nb &lt;- a + 0.5 + rnorm(15, mean=0, sd=.5)\nboxplot(a, b)\n\n\n\n\n\n\n\n\nWe see on the boxplot that the groups, if treated independently, are not really different. However, we can visualize it much better using a paired plot, in which a line is connecting the two values for each individual. We will use ggplot2 for that purpose:\n\ndf &lt;- data.frame(value=c(a, b),\n                 group=rep(c(\"A\", \"B\"), each=15),\n                 id=1:15)\nggplot(df, aes(x=group, y=value, group=id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nThe new parameter here is the group aesthetic, group, which tells ggplot2 that the points which have identical id belong together. The geoms, such as geom_line() which connects data points with lines, work group-by-group, and therefore lines connect only points within one group. geom_line(), group aesthetic\nOn this plot we can clearly see that for the majority of individuals, the value in the second measurement (group B) is higher than in the first measurement (group A). We can confirm this with a paired t-test:\n\nt.test(a, b, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  a and b\nt = -4.1849, df = 14, p-value = 0.0009172\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.7638627 -0.2461963\nsample estimates:\nmean difference \n     -0.5050295 \n\n\nThe paired=TRUE argument tells R that the two vectors are paired. We get a p-value of 0.00092, whereas with regular t-test we would have gotten a p-value of 0.21. Similarly, if you were to run the Wilcoxon paired test, the p value would be 0.0034, whereas in the regular Wilcoxon test it would be 0.17.\n\\(\\chi^2\\) (Chi-square) test. If we have two categorical vectors, we can  use the \\(\\chi^2\\) test. Consider the results of the gene expression analysis we have looked at yesterday. We can define two vectors: ‚Äúsignificant/non-significant‚Äù and ‚Äúinterferon/noninterferon‚Äù. These can be logical or character vectors, it doesn‚Äôt matter:\\(\\chi^2\\) test\n\nlibrary(tidyverse)\ntr_res &lt;- read_csv(\"Datasets/transcriptomics_results.csv\")\ntr_res &lt;- select(tr_res, \n                 Gene=GeneName, Description,\n                 logFC=logFC.F.D1, FDR=qval.F.D1)\ninterferon &lt;- grepl(\"interferon\", tr_res$Description)\nsignificant &lt;- tr_res$FDR &lt; 0.01\ntable(significant, interferon)\n\n           interferon\nsignificant FALSE  TRUE\n      FALSE 40725    59\n      TRUE    834    21\n\n\nAbove we constructed a contingency table. Rows show the two  values of significant vector, columns ‚Äì same for the interferon vector. In other words, most (40725) genes are neither significant nor interferons. However, out of 855 significant genes, as much as 21 have ‚Äúinterferon‚Äù in their description. And vice verse, out of the 80 interferon genes, more than a third are significant. But is this statistically significant? To answer this, we can use the chisq.test(). Contingency table with table(x, y)chisq.test()\n\nchisq.test(significant, interferon)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  significant and interferon\nX-squared = 221.44, df = 1, p-value &lt; 2.2e-16\n\n\nYep, looks like it. In fact, the above was a simple case of gene set enrichment analysis.\nWith this, we conclude this simple statistics part. If you are interested in more, take a look at the Appendix: more stats and visualizations ‚Äì I have included a few quite common statistical applications there.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#pca-and-scatter-plots",
    "href": "day5-visualization-and-statistics.html#pca-and-scatter-plots",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.5 PCA and scatter plots",
    "text": "5.5 PCA and scatter plots\n\n5.5.1 Principal component analysis\nIn this last section we will combine a lot of things (and introduce a few new concepts, despite it being Friday afternoon15).\n15¬†Assuming, of course, that you started this course on a Monday‚Ä¶One of the basic plots in statistics is the scatter plot, with two continuous variables. They are easy enough to generate in R, but we will do it with a twist, introducing you to a new statistical concept: principal component analysis (PCA). PCA, Principal component analysis\nRember how we plotted the iris data set two days ago (Section 3.5.5)? It was easy to see the differences between the groups, and if we wanted, we could have plotted all four variables on two plots. But what if we had thousands of variables? For example, expression of thousands of genes for hundreds of samples?\nOne of the possible approaches is to use a technique like PCA. PCA converts the data replacing the original variables with a new, smaller set, that however covers all the variance in the data. While the old variables are clearly defined, the new ones, called ‚Äúprincipal components‚Äù, result from combining them, so they do not correspond to something specific16. However, they have a few nice properties:\n16¬†Components are the linear combinations of the original variables. So for a given sample, we basically add up expression of all genes, but each gene has a different weight.\nMost of the variance of the samples sits in the first components, then the next biggest share sits in the second etc. That means that by looking at the first few we are likely to get a very good idea of the overall differences in the data.\nThe components are orthogonal, that is, they are not correlated with each other. This means that if, say, two groups correspond to one component, then it is quite likely that they will only correspond to that component. And thus we are able to say: ‚Äúthis component explains differences between treatments, and this explains gender‚Äù.\nThe analysis is unsupervised, that is, we do not need to tell the algorithm which samples belong to which group. It will find the differences on its own (if there are any).\n\n\n\n5.5.2 The data\nIn the following, we will be using a set of measurements of laboratory values (such as white blood cell count, hemoglobin, etc.) from a clinical vaccination study. There were three treatments - placebo and two vaccines, one with an adjuvant (Fluad) and one without (Agrippal), and samples were taken on different time points, starting with a screening time point and base line, then on to D0, D1 etc. The data is in the wide format, that is, each row corresponds to a single sample ‚Äì that is, a single patient and a single time point, and each column corresponds to a single measurement.\n\nlibrary(tidyverse)\nlabdata &lt;- read_csv(\"Datasets/labresults_wide.csv\")\ndim(labdata)\n\n[1] 1732   31\n\nhead(labdata[,1:10])\n\n# A tibble: 6 √ó 10\n  SUBJ.TP       ACA   ALB   ALP   ALT  BILI    CA CREAT   CRP   GGT\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 4081.SCREEN  2.24  46.1  79.4  25.7  14.6  2.37  83.0 0.224  27.9\n2 4081.BL      2.27  43.8  77.7  24.0  10.1  2.36  77.0 0.352  26.3\n3 4081.D0      2.28  44.1  76.2  21.0  15.4  2.36  80.4 0.208  24.8\n4 4081.D1      2.25  42.0  74.2  22.4  23.6  2.31  74.5 0.452  24.7\n5 4081.D2      2.18  39.9  65.9  22.2  20.3  2.18  75.0 0.342  21.6\n6 4081.D3      2.15  37.8  62.8  20.5  18.3  2.12  69.7 0.333  20.4\n\n\nThe last line above uses head() to show the first few lines of the data frame, and with [,1:10] we limit the output to the first ten columns17.\n17¬†If you want to know more on what the abbreviations mean, take a look at the labresults_full.csv file. This is the down-side of the wide format: no place to store extra information about the columns!As you can see, the first column identifies the sample by giving both the subject and the time point. In the following, we only want one time point (D1), and we could filter the data set by combining filter() with a suitable grepl() call, for example, filter(labdata, grepl(\"D1$\", TP)). Instead, we will use this as an opportunity to show you the separate() function. separate()\n\nlabdata &lt;- labdata |&gt;\n  separate(SUBJ.TP, into=c(\"SUBJ\", \"TP\"), sep=\"\\\\.\") |&gt;\n  filter(TP == \"D1\")\n\nThe separate() function takes a column name, the names of the new columns and a separator. The separator is a regular expression, and since the dot normally matches any character, we need to escape it with two backslashes. The resulting data frame has two columns named SUBJ and TP instead of one called SUBJ.TP, and we could directly filter the data frame for the desired time point.\nThere is one more thing that we need to take care of, unfortunately. There are some NA values in the data set, and we need to remove them. Rather than figure out how to handle them, we will simply remove all samples that have a missing value anywhere in the data set with one function: drop_na()\n\nlabdata &lt;- labdata |&gt; drop_na()\n\nThe problem we are facing now is that the data set does not contain any interesting meta-data, like any information about the actual treatment group! Lucky for us, we have that information in another file. The expression_data_vaccination_example.xlsx file contains the meta-data (as well as matching RNA expression data, but we will not use that here for now).\nWe will combine both data sets using an inner join with merge().\n\n# read the meta-data\nlibrary(readxl)\nmeta &lt;- read_excel(\"Datasets/expression_data_vaccination_example.xlsx\",\n                   sheet=\"targets\") |&gt;\n                   filter(Timepoint == \"D1\")\n                   \ncombined &lt;- merge(meta, labdata, by=\"SUBJ\")\nhead(combined[,1:10])\n\n  SUBJ          USUBJID Batch       ID Timepoint ARMCD      ARM AGE SEX PLACEBO\n1 4023  CRC987X-4023-17     C F4023.D1        D1     F    FLUAD  22   F   FALSE\n2 4028 CRC987X-4028-120     C A4028.D1        D1     A AGRIPPAL  29   F   FALSE\n3 4034 CRC987X-4034-128     C A4034.D1        D1     A AGRIPPAL  17   M   FALSE\n4 4066 CRC987X-4066-222     A P4066.D1        D1     P  PLACEBO  28   M    TRUE\n5 4119 CRC987X-4119-117     C F4119.D1        D1     F    FLUAD  27   F   FALSE\n6 4122 CRC987X-4122-185     A P4122.D1        D1     P  PLACEBO  26   F    TRUE\n\ndim(combined)\n\n[1] 60 41\n\n\n\n\n\n\n\n\nCheck your results!\n\n\n\nCheck your results frequently. Do you get the number of rows you expect? How does the data look like? Use dim(), head(), tail(), summary(), View() all the time.\n\n\nTo run the PCA, we need, however, only the numeric columns. If you take a look at the combined data frame with, for example, View, you will see that we need the columns from ACA (adjusted calcium) to WBC (white blood cell count). We will select these columns and convert the resulting data frame to a matrix.\n\ncombined_mtx &lt;- select(combined, ACA:WBC) |&gt; \n  as.matrix()\nhead(combined_mtx[,1:5])\n\n          ACA      ALB      ALP      ALT      BILI\n[1,] 2.049224 40.83864 72.74828 18.60445  9.781505\n[2,] 2.124255 41.17438 61.13931 15.05414  6.878661\n[3,] 2.203683 41.05199 56.41416 19.88788 11.611140\n[4,] 2.160399 39.91228 40.65232 32.52709 12.058742\n[5,] 2.100984 38.20202 69.14200 35.36937  9.516149\n[6,] 2.198283 38.35306 81.23705 10.83461  8.167515\n\n\n\n\n5.5.3 Running the PCA\nThe PCA is a built-in function in R, and it is called prcomp(). We will only use a single parameter, scale. (mind the dot!) to tell PCA to scale the gene expression before working its magic: prcomp()\n\n# the actual PCA is just one line!\npca &lt;- prcomp(combined_mtx, scale.=TRUE)\nis.list(pca)\n\n[1] TRUE\n\nnames(pca)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nThe object returned by prcomp() is a list, and the element that we are particularly interested in is called x. This is a matrix of the principal components, PC1, PC2 etc. There are quite a few principal components, and each one has as many elements as there are samples in the data set:\n\ncolnames(pca$x)[1:10]\n\n [1] \"PC1\"  \"PC2\"  \"PC3\"  \"PC4\"  \"PC5\"  \"PC6\"  \"PC7\"  \"PC8\"  \"PC9\"  \"PC10\"\n\ndim(pca$x)\n\n[1] 60 30\n\ndim(combined)\n\n[1] 60 41\n\n\nAs you can see, the number of rows of pca$x is the same as the number of samples in the combined data frame. In other words, for each sample, we have a bunch of new values that are the principal components.\nWe won‚Äôt be interested in more than the first few components, but we want to see them in connection with the covariates. Therefore, we will use cbind() to combine the combined data frame with the first few columns of the pca$x matrix. We will use the ggplot2 package to plot the data. cbind()\n\nlibrary(ggplot2)\npca_df &lt;- cbind(combined, pca$x[,1:10])\nggplot(pca_df, aes(x=PC1, y=PC2, color=SEX)) +\n  geom_point(cex=3)\n\n\n\n\n\n\n\n\nOn the plot above, we have plotted the first two principal components, PC1 and PC2. You can clearly see that the first component separates the samples by sex: males have a different value of PC1 then females. We can see it clearly on a boxplot: geom_boxplot()\n\nggplot(pca_df, aes(x=SEX, y=PC1)) +\n  geom_boxplot(outlier.shape=NA) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nHowever, because geom_boxplot() also puts a dot where outliers are, we use outlier.shape=NA to suppress them. geom_jitter()\n\n\n\n\n\n\n\nExercise 5.8 (Vaccines) Repeat the plot above, but instead of SEX, use the ARM column which corresponds to the study arm, i.e.¬†which vaccine (or placebo) was administered. Can you tell the groups apart? Try it with other components: PC3 vs PC4, PC5 vs PC6 etc. Any luck?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, the PC4 seems to show differences between the three groups, with Fluad having the lowest values, and Placebo the highest.\n\nggplot(pca_df, aes(x=PC3, y=PC4, color=ARM)) +\n  geom_point(cex=3)\n\n\n\n\n\n\n\nggplot(pca_df, aes(x=ARM, y=PC4)) +\n  geom_boxplot(outlier.shape=NA) +\n  geom_beeswarm()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.4 Interpreting the PCA\nOK, but what does that mean? How does the PCA ‚Äúknow‚Äù who is male, and who is female? Where does this information come from?\nAs mentioned before, every principal component is derived from the actual values of the different variables. The values are summed up for each component, but not every variable contributes equally. We can wee how each value contributes to the component by directly looking at another member of the pca object, rotation. Again, it is a matrix, but now each row corresponds to a variable, and each column to a principal component:\n\nhead(pca$rotation[,1:5])\n\n             PC1          PC2         PC3         PC4          PC5\nACA  -0.17887595  0.330789811 -0.12168346  0.08619332 -0.017461193\nALB  -0.26645587  0.054470599 -0.18559187 -0.20262134 -0.061104850\nALP  -0.08216838  0.139728838  0.06991512 -0.16908259 -0.006265994\nALT  -0.12070502 -0.008253212  0.16125243  0.11857842  0.390570215\nBILI -0.16847944 -0.233759742 -0.01457699 -0.26068360 -0.042323962\nCA   -0.26605009  0.280096277 -0.17364117 -0.02687325 -0.047787207\n\n\nThese numbers are loadings, that is, the weights of each variable in the calculation of the given principal component. If the loading is close to 0, then the given variable does not contribute much to the component. If, however, it is very large, or very small (negative), then the variable contributes a lot.\nTo understand why the male samples are separated from the female samples on the plot above, we will focus on PC1 and sort it by the decreasing absolute loadings.\n\npc1 &lt;- pca$rotation[,1]\nord &lt;- order(abs(pc1), decreasing=TRUE)\nhead(pc1[ord])\n\n       HGB        HCT        RBC        ALB         CA      CREAT \n-0.3645930 -0.3362286 -0.3076459 -0.2664559 -0.2660501 -0.2550704 \n\n\nFirst value that we find is HGB with a value of -0.36. The fact that it is negative means that when the actual value of the variable is high, the resulting PC1 will be low and vice versa. That is, samples with a high value of this variable will be likely to be shown on the left side of the plot, and samples with a low value on the right side.\nNow, male samples are on the left side of the plot, and female samples on the right side. What is HGB? It turns out that it corresponds to hemoglobin. Since the loading is low, it means that males should have a low value of hemoglobin compared to females. We can check it with a boxplot:\n\n\n\n\n\n\n\n\n\nIndeed, that seems to be the case! And, of course, it makes perfect sense biologically.\n\n\n\n\n\n\n\nExercise 5.9 (Other top loadings) In the table below, you will find three more variables. Check their loadings. Make corresponding box plots and check if the values correspond to what you know about human biology.\n\n\n\nVariable\nDescription\n\n\n\n\nESR\nErythrocyte sedimentation rate\n\n\nCREAT\nCreatinine\n\n\nCA\nCalcium\n\n\n\n\n\n\n\nThe purpose of this exercise was not only to show you the mechanics of PCA. This is a useful method, and quite often it is worthwile to run it before you do anything else with the data. PCA shows you the possible groupings even before you even start to think about the actual analysis.\nHowever, another point of this whole story was to demonstrate an important aspect of bioinformatic analyses. Think how much work it was to pummel the data into a shape which was suitable for the PCA, and how much work to actually figure out what it means ‚Äì compared with the actual analysis, which was just a single line of code. This is a common situation in bioinformatics: the actual analysis is easy, but the data preparation and interpretation are hard and time consuming.\n\n\n\n\n\n\n\nExercise 5.10 (Top loadings for vaccines) Which PC is the most important for separating the vaccine groups? Which variables contribute the most to this component? Can you explain why?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#afterword",
    "href": "day5-visualization-and-statistics.html#afterword",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.6 Afterword",
    "text": "5.6 Afterword\n\n5.6.1 Where to get R packages\nThere are three main sources of R packages: CRAN, Bioconductor and github.\nCRAN is the resource you are using when using install.packages(). It features more than 20,000 packages from all possible areas of science, statistics and even entertainment. Packages in CRAN are rigorously tested for formal requirements: they must have a certain structure, all functions and data sets must be documented, and the package must pass a series of tests, including testing whether the packages can be installed on a variety of operating systems. However, the packages are never tested for their actual correctness, i.e.¬†whether what they claim to do is what they actually do. There is no manual review process involved.\nHere are a few packages on CRAN that might be of interest to you at this stage:\n\ncolorDF) ‚Äì a package that I wrote for displaying colorized data frames in the console. It has also a summary function that provides information similar to that of skimr::skim() (colorDF::summary_colorDF()).\ntinytable ‚Äì package for generating awesome tables with R and R markdown.\ngt ‚Äì another package for creating tables in R markdown; they can be really complex and have a professional look. Good for manuscripts and publications.\ntidylog ‚Äì a package for logging your data manipulation steps; makes functions like filter() more explicit.\ncolorout ‚Äì adds colors to your console.\nformatdown ‚Äì a bunch of functions to properly format numbers and more in Rmarkdown/Quarto, similar to the format.pval() base R function but better looking (e.g.¬†\\(6.022 \\times 10^{23}\\)).\ntrackdown is an attempt to allow multi-user collaboration on R markdown documents. You work in R markdown / Quarto on your computer, but then upload a document to Google Docs, which is then edited by your collaborators; and then you can download the changes and apply them to your R markdown document automatically. Unfortunately, the hardest part is to set up the authentication with Google.\nrenv ‚Äì a package that helps you manage your R environment, including the packages you have installed. The idea is that rather than using a single global library on your computer, you install necessary packages into a local, project-specific library. This has the advantage that for a given project, you always work with the same versions of the packages, and helps to ensure reproducibility of your code.\nRColorBrewer ‚Äì a very popular set of color palettes for R graphics. You can use them directly in ggplot2 by calling scale_fill_brewer() or scale_color_brewer().\n\nBioconductor is both a framework and repository for packages which are mostly related to bioinformatics and biomedicine. The framework differs somewhat from, say, tidyverse, so that at times you will run into problems; for example, the filter() function from tidyverse will not work on the DataFrame object from Bioconductor; and vice versa, loading a Bioconductor package may mask the filter() function from tidyverse with its own variant (that works in a different way), causing infuriating errors in your code.\nBioconductor packages are rigorously checked and manually reviewed, and thus provide a higher level of quality assurance than CRAN packages.\nTo install a Bioconductor package, you need to install the BiocManager package from CRAN, and then use the BiocManager::install() function to actually install your packages.\nGithub is a general repository for code, not only R but also many other languages. Anyone can create a repository on Github and publish their R package there. In fact, for many packages on CRAN or in Bioconductor there exists a Github repository that contains also development versions of the packages. Github is simply very convenient for developing software ‚Äì and not only software. If you are reading this book, chances are that you are doing so over Github.\nInstalling github packages may depend on the specific package, but usually you can use the install_github() available when you load the devtools package. For example, to install the development version of colorDF, you can type devtools::install_github(\"january3/colorDF\") (in this case, it is the same version as the one on CRAN).\nThere is no formalized check for the quality of the packages on Github. Individual developers may or may not include some QC, but there is no formal requirement for it. Therefore, the software quality may vary considerably.\n\n\n5.6.2 Where to go from here\nThere are numerous resources for R on the web, allow me to recommend a few.\n\nStackOverflow ‚Äì this is a general Q & A forum for programmers and other like them. Most of the questions you have someone has previously asked here, including many by yours truly.\nOnline R books\n\nHands-On Programming with R by Garrett Grolemund. A very different approach to teachin R to beginners than the one that I prefer, but maybe you will find it better. Worth checking in any case.\nR for Data Science (tidyverse) by Hadley Wickham. The tidyverse book, written by the main tidyverse developer. This book also starts with introducing R.\nR Cookbook by James JD Long. The cookbook format splits the information in small ‚Äúrecipies‚Äù for common problems, both in base R and in tidyverse.\nR Graphics Cookbook by Winston Chang. The cookbook format splits the information in small ‚Äúrecipies‚Äù for common problems, both in base R and in ggplot2.\nR markdown: the definitive guide by Yihui Xie, J. J. Allaire and Garrett Grolemund. A comprehensive guide to R markdown written by the people who created it.\nR Workflow by Frank Harrell is somewhat hermetic, but there are many good ideas in it and a lot about reproducible scientific work with Quarto.\nR manuals from CRAN. Be warned ‚Äì they tend to be rather comprehensive.\n\nOther books:\n\nThe R Book by Michael J. Crawley ‚Äì this is a magnificent book on statistics. It uses very conservative R language (no tidyverse at all), but it discusses at length even the more complex statistical issues. I recommend this book to every person willing to learn statistics with R.\nComputational Genomics with R by Altuna Akalin (available online) ‚Äì this is a book on bioinformatics with R and BioConductor. It features both an introduction to genomics and a terse introduction to R. The R flavor presented is somewhat different from the one I prefer (for example, the author uses = instead of &lt;-), but the main focus of the book is elsewhere ‚Äì on specific bioinformatics tasks.\nRegression and other stories by Andrew Gelman, Jennifer Hill and Aki Vehtari. This book focuses on doing bayesian statistics with the RStan package. It is both, a very good introduction to R and a very good introduction to statistics, with thoughtful explanations and a lot of examples. However, keep in mind that the focus of the book is bayesian statistics.\n\nLarge Language Models (LLMs): I have good experiences with LLMs (ChatGPT, Perplexity AI) for learning programming languages. While complex tasks may be out of reach for them, they are very good at explaining basics. ‚ÄúHow do I do X in R?‚Äù or ‚ÄúWhy doesn‚Äôt the following code work?‚Äù seem to work quite well. RStudio allows you also to use Copilot which is an LLM model that watches the code you write and tries to guess what you are trying to do. Just remember, that the one thing that LLMs don‚Äôt know how to say is ‚ÄúI don‚Äôt know‚Äù. If something doesn‚Äôt make sense, or if it is beyond the scope of their learning, they will ‚Äúhallucinate‚Äù ‚Äì give good sounding advice which is totally bonkers.\n\n\n\n5.6.3 Famous last words\nWe are now at the end of Five Days of R, but your journey just started. Here is my last advice to you.\n\n\n\n\n\n\nGet on with R\n\n\n\nStart working with R, right now, for all your projects; for statistics, data management and even preparing reports and documents.\nAt first, doing the same task in R will take much more time then the same task in programs you used before. You will feel that you are wasting your time.\nYou are not. Sooner then you think, it will pay off.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#review",
    "href": "day5-visualization-and-statistics.html#review",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.7 Review",
    "text": "5.7 Review\n\nR markdown / Quarto\n\ncreating R markdown documents\nbasic markdown formatting (bold, italic etc.)\nincluding R chunks\ncreating output in different formats\nchoosing an output format\n\nVisualisations\n\nbasic scatterplots with ggplot()\nunderstanding esthetics (aes())\nbox plots with geom_boxplot() and violin plots with geom_violin()\nheatmaps\nvector vs raster graphics\n\nBasic statistics\n\nt-test and Wilcoxon test\npaired tests\n\\(\\chi^2\\) test\nprincipal component analysis with prcomp()\n\nRunning a principal component analysis\n\n\n\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022. ‚ÄúVenn Diagrams May Indicate Erroneous Statistical Reasoning in Transcriptomics.‚Äù Frontiers in Genetics 13: 818683. https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.818683/full.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "appendix-wide-long.html",
    "href": "appendix-wide-long.html",
    "title": "Appendix: Wide vs long data",
    "section": "",
    "text": "Wide and Long format\nConsider a data set with laboratory measurement for a number of patients. We can measure, for example, the concentration of calcium, of C-reactive protein, number of white blood cells in billions per liter etc., etc.\nWe have many ways to represent the data in a data frame. For example, we can put all measurements for a patient in one row, and have a column for each measurement type. This results in a nice, compact format, where each row corresponds to one patient, and each column to one measurement. Here is how it might look like:\nSUBJ\nCA\nCRP\nWBC\n\n\n\n\n4,081.00\n2.31\n0.45\n12.20\n\n\n4,368.00\n2.14\n0.62\n6.18\n\n\n4,601.00\n2.26\n0.60\n5.03\n\n\n4,224.00\n2.25\n0.32\n7.62\n\n\n4,323.00\n2.04\n0.60\n5.76\nThis is called the wide format. It is easy to view, and suitable for some applications ‚Äì like principal component analysis. It is also easy to filter the data by patients, or by certain measurement parameters. However, we can‚Äôt use it for many other applications, for example for plotting and many statistical applications.\nAnd what if we want to include additional information about the measurements? For example, when the measurement was made, in what units, who conducted the test? We could add columns for each of these, but then we would have a mess.\nSUBJ\nLBORRES_CA\nLBORRESU_CA\nLABTECH_CA\nLBORDATE_CA\nLBORRES_CRP\nLBORRESU_CRP\nLABTECH_CRP\nLBORDATE_CRP\nLBORRES_WBC\nLBORRESU_WBC\nLABTECH_WBC\nLBORDATE_WBC\n\n\n\n\n4,081.00\n2.31\nmmol/L\nFrank N. Stein\n2022-04-19\n0.45\nmg/L\nMr. Hyde\n2021-09-01\n12.20\n10*9/L\nFrank N. Stein\n2022-03-27\n\n\n4,368.00\n2.14\nmmol/L\nDr. Jekyll\n2021-09-29\n0.62\nmg/L\nFrank N. Stein\n2022-02-08\n6.18\n10*9/L\nFrank N. Stein\n2021-09-11\n\n\n4,601.00\n2.26\nmmol/L\nDr. Jekyll\n2021-12-01\n0.60\nmg/L\nDr. Jekyll\n2021-09-03\n5.03\n10*9/L\nMr. Hyde\n2022-01-18\n\n\n4,224.00\n2.25\nmmol/L\nFrank N. Stein\n2022-04-12\n0.32\nmg/L\nDr. Jekyll\n2022-02-23\n7.62\n10*9/L\nMr. Hyde\n2022-05-11\n\n\n4,323.00\n2.04\nmmol/L\nMr. Hyde\n2022-06-14\n0.60\nmg/L\nMr. Hyde\n2022-05-08\n5.76\n10*9/L\nMr. Hyde\n2021-12-24\nYeah, no, this is not going to work. It is hard to view manually, but for computing applications it is even worse. We cannot easily convert it to a matrix, and we still can‚Äôt use it for ggplot2 or linear modeling. It is, quite probably, the worst of both worlds1.\nEnter the long format. In the long format, each row corresponds to one measurement, and one measurement only:\nSUBJ\nLBTEST\nLBTESTCD\nLBORRES\nLBORRESU\nLBORDATE\nLABTECH\n\n\n\n\n4,081.00\nCalcium\nCA\n2.31\nmmol/L\n2022-04-19\nFrank N. Stein\n\n\n4,081.00\nC Reactive Protein\nCRP\n0.45\nmg/L\n2021-09-01\nMr. Hyde\n\n\n4,081.00\nWhite Blood Cells\nWBC\n12.20\n10*9/L\n2022-03-27\nFrank N. Stein\n\n\n4,368.00\nCalcium\nCA\n2.14\nmmol/L\n2021-09-29\nDr. Jekyll\n\n\n4,368.00\nC Reactive Protein\nCRP\n0.62\nmg/L\n2022-02-08\nFrank N. Stein\nThis is by far the most advantageous format for transfering and storing data. It is also the most flexible format for data analysis. We can easily convert it to the wide format (possibly losing some information, like units or who conducted the test), or to a matrix, or to a plot. We can easily filter the data, or do statistics on it.\nHowever, as you can see above, some of the information might be duplicated many times ‚Äì for example, if we have a column with patient‚Äôs name, it will appear many times next to each measurement of that patient.\nTo sum it up:\nLong advantages:\nWide advantages:\nI recommend using the long format in most situations, however there are always occasions where we need to move from one option to the other. For example, we might want to do a PCA ‚Äì so we need to convert to the wide format. Or the wide format is more suitable for manual data entry ‚Äì and we have to convert it to the long format for analysis.\nIt is crucial to be fluent in switching between both formats.",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#wide-and-long-format",
    "href": "appendix-wide-long.html#wide-and-long-format",
    "title": "Appendix: Wide vs long data",
    "section": "",
    "text": "1¬†Actually, that is not true. Even worse is adding information directly into the column names, like CA (mmol/L, 2021-09-01, Dr. Jekyll). Close second comes adding information in the values directly, like 7.9 mmol/L (2021-09-01, Dr. Jekyll), but at least here we can attempt to automatically parse the values ‚Äì as long as there are no typos‚Ä¶\n\n\n\n\n\n\neasier to filter, process, visualize, do statistics with\nfocused on measurement (‚Äúpatient ID‚Äù or equivalent is a covariate, and so is measurement type)\nsafer\n\n\n\ngroups data by a covariate (‚Äúpatient ID‚Äù)\ncan be easier to manage (each column one measurement type)\nless duplication of information (smaller files)\n\n\n\n\n\n\n\n\n\nUse long format\n\n\n\nIn general, you should probably always use the long format as the main format for your data. It is easier to work with, and you can always convert it to wide format if you need to.\nThere are exceptions to this rule ‚Äì typically when it comes to high-throughput data (like RNA-Seq).",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-wide-to-long",
    "href": "appendix-wide-long.html#converting-from-wide-to-long",
    "title": "Appendix: Wide vs long data",
    "section": "Converting from wide to long:",
    "text": "Converting from wide to long:\nFirst, let‚Äôs read an example wide data set:\n\nlibrary(tidyverse)\nwide &lt;- read_csv(\"Datasets/wide_example.csv\")\nwide\n\n# A tibble: 4 √ó 5\n  subject sex   control cond1 cond2\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 M         7.9  12.3  10.7\n2       2 F         6.3  10.6  11.1\n3       3 F         9.5  13.1  13.8\n4       4 M        11.5  13.4  12.9\n\n\nThis is clearly a wide format ‚Äì each row corresponds not to one observation (one measurement), but one subject. We want to convert this to long format, where each row corresponds to one observation (and therefore, for each subject, there are three rows).\npivot_longer()\n\npivot_longer(wide, cols=control:cond2,\n  names_to=\"condition\", values_to=\"measurement\")\n\n# A tibble: 12 √ó 4\n   subject sex   condition measurement\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1       1 M     control           7.9\n 2       1 M     cond1            12.3\n 3       1 M     cond2            10.7\n 4       2 F     control           6.3\n 5       2 F     cond1            10.6\n 6       2 F     cond2            11.1\n 7       3 F     control           9.5\n 8       3 F     cond1            13.1\n 9       3 F     cond2            13.8\n10       4 M     control          11.5\n11       4 M     cond1            13.4\n12       4 M     cond2            12.9\n\n\nNote that we must put quotes around condition and measurement in the code above. These are column names, but the columns don‚Äôt exist yet.",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-long-to-wide",
    "href": "appendix-wide-long.html#converting-from-long-to-wide",
    "title": "Appendix: Wide vs long data",
    "section": "Converting from long to wide",
    "text": "Converting from long to wide\nHere is another example data set, this time in long format:\n\nlong &lt;- read_csv(\"Datasets/long_example.csv\")\nlong\n\n# A tibble: 9 √ó 5\n  subject sampleID sex   condition measurement\n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n1       1 ID000001 M     control           7.9\n2       1 ID000002 M     cond1            12.3\n3       1 ID000003 M     cond2            10.7\n4       2 ID000004 F     control           6.3\n5       2 ID000005 F     cond1            10.6\n6       2 ID000006 F     cond2            11.1\n7       3 ID000007 F     control           9.5\n8       3 ID000008 F     cond1            13.1\n9       3 ID000009 F     cond2            13.8\n\n\nAs you see, there are three subjects again, each with three measurements (one control, one for condition 1, and one for condition 2). We want to convert it to wide format, so we expect one row per subject, in total three rows.\nWe can use the pivot_wider() function to do this. We need to specify which column contains the values that should be spread out, and which column contains the names of the new columns. pivot_wider()\nHowever, watch out. The first thing we might want to try does not give the expected result:\n\n## not what we wanted!!! Why?\npivot_wider(long, names_from=condition, values_from=measurement)\n\n# A tibble: 9 √ó 6\n  subject sampleID sex   control cond1 cond2\n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 ID000001 M         7.9  NA    NA  \n2       1 ID000002 M        NA    12.3  NA  \n3       1 ID000003 M        NA    NA    10.7\n4       2 ID000004 F         6.3  NA    NA  \n5       2 ID000005 F        NA    10.6  NA  \n6       2 ID000006 F        NA    NA    11.1\n7       3 ID000007 F         9.5  NA    NA  \n8       3 ID000008 F        NA    13.1  NA  \n9       3 ID000009 F        NA    NA    13.8\n\n\nThe problem is in the sampleID column. Given that names of the variables are in the condition column, and measurement is in the measurement column, R considers all the remaining columns to be the identifier columns. But the column sampleID contains only unique values, so they must be put in separate rows, as above.\nTo fix this, we need to tell R not to use sampleID as an identifier column; instead, only the subject column should be used as an identifier. We can also throw in the sex column if we want to keep it, since it has only 1 value per subject.\npivot_wider(id_cols=...)\n\n## Instead: \npivot_wider(long, id_cols=c(subject, sex),\n                  names_from=condition, values_from=measurement)\n\n# A tibble: 3 √ó 5\n  subject sex   control cond1 cond2\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 M         7.9  12.3  10.7\n2       2 F         6.3  10.6  11.1\n3       3 F         9.5  13.1  13.8\n\n\n\n\n\n\n\n\n\nExercise 1 (Converting to long format) Convert the following files to long format:\n\nlabresults_wide.csv\nThe built-in iris data set (data(iris))\ncars.xlsx (tricky! hint: how do you tell which value in the long format belongs to which row in the wide format?)\n\nClean up and convert to long format (what seems to be the problem? How do we deal with that?):\n\nmtcars_wide.csv",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-more-stats.html",
    "href": "appendix-more-stats.html",
    "title": "Appendix: More statistics and visualizations",
    "section": "",
    "text": "Correlations\nCorrelation is a measure of how two variables change together. There are many different variants, the most popular being the Pearson correlation and the Spearman correlation. Both can be calculated using the cor() function in base R and tested for significance using the cor.test(). Here they are in action on the iris dataset:\ncor(iris$Sepal.Length, iris$Petal.Length)\n\n[1] 0.8717538\n\ncor.test(iris$Sepal.Length, iris$Petal.Length, method=\"spearman\")\n\nWarning in cor.test.default(iris$Sepal.Length, iris$Petal.Length, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nS = 66429, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8818981\nThe warning message above indicates that there are duplicated values in the data, which makes the Spearman correlation test less reliable.\nThe cor() function can also calculate the correlation matrix ‚Äì that means, correlate each variable with each. This is useful for visualizing the relationships between variables. Here is how you can do it:\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\nThe diagonal of the matrix is always 1, because a variable is always perfectly correlated with itself. The matrix is symmetric, because the correlation between \\(x\\) and \\(y\\) is the same as the correlation between \\(y\\) and \\(x\\). We can visualize this matrix using pheatmap:\nlibrary(pheatmap)\nM &lt;- cor(iris[,1:4])\npheatmap(M, scale=\"none\")\nThe scale=\"none\" parameter is used to avoid scaling the data by rows or by columns ‚Äì it does not make sense for a symmetrical matrix.\nAs you can see, the one oddball in the iris dataset is the Sepal.Width variable, which is not very correlated with the other three.\nThere is another package to visualize correlation matrices, called corrplot. It is more flexible and can be used to visualize the correlation matrix in many different ways. Here is an example:\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(M, method=\"color\")\nThere are many cool ways how corrplot can visualize the correlation matrix. You can check them out in the documentation of the package.\ncorrplot(M, method=\"ellipse\", \n         type=\"upper\", tl.pos=\"d\")\ncorrplot(M, add = TRUE, type = 'lower', \n         method = 'number',\n         col=\"black\", diag = FALSE, \n         tl.pos = 'n', cl.pos = 'n')",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "appendix-more-stats.html#correlations",
    "href": "appendix-more-stats.html#correlations",
    "title": "Appendix: More statistics and visualizations",
    "section": "",
    "text": "Correcting for multiple testing\nWhen you run multiple tests, you increase the chance of finding a false positive. If two data sets do not differ, and you run a test a 100 times, on average 5 of those tests will show a significant difference at the 0.05 level. This is called the multiple testing problem.\nTherefore, in order to trust the results of your tests, you need to correct for multiple testing. There are basically two main approaches to this:\n\nFamily wise error rate (FWER) correction, which controls the chance of making at least one false positive. Basically, we want the corrected p-values to mean just what the regular once do ‚Äì the probability of making a false positive. The most popular method for this is the Bonferroni correction, which divides the significance threshold by the number of tests.\nFalse discovery rate (FDR) correction, which controls the proportion of false positives among all significant results. The most popular method for this is the Benjamini-Hochberg (BH) correction.\n\nThe former is very conservative, which means that while indeed you make sure that the corrected p-values are what you think they are, you are also introducing a huge number of type II errors - false negatives.\nThe BH correction is more relaxed, and is often used in high-throughput experiments in biology. Since it is not really a p-value it is good to refer to it as a q-value or FDR value. An FDR of, say, 0.05 means that among the results which have an FDR of 0.05 or less, at most 5% are expected to be false positives.\nBoth of these corrections can be done with the p.adjust() function in R. Say, we make a number of comparisons using the iris dataset:\n\nlibrary(tidyverse)\nsv &lt;- iris |&gt; filter(Species != \"setosa\") |&gt;\n  mutate(Species=factor(Species))\npvals &lt;- sapply(1:4, function(i) {\n  t.test(sv[,i] ~ sv$Species)$p.value\n})\nnames(pvals) &lt;- colnames(sv)[1:4]\npvals\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n1.866144e-07 1.819483e-03 4.900288e-22 2.111534e-25 \n\n\nOK, so a lot has happened above that you have not seen before. First, why do we convert Species to a factor? The reason is that the Species is already a factor in the original data frame, but it has three levels: ‚Äúsetosa‚Äù, ‚Äúversicolor‚Äù, and ‚Äúvirginica‚Äù. When we filter out the ‚Äúsetosa‚Äù species, the levels remain unchanged, and the t.test function will complain that we have too many groups. Therefore, we need to convert the Species to a factor with only two levels.\nThe sapply function is a way to apply a function to each value of a vector or list. Here, we apply an anonymous function, that is, defined without giving it a name, to every value in the vector 1:4. The anonymous function takes as parameter a single value from the vector, and returns the p-value of the t-test between the corresponding column of the sv data frame and the Species variable.\nAnother new thing that you have not seen previously is the ~ sign. Rather than running a t-test on two vectors, we run it on a formula. We will cover formulas in a moment, but basically here it means for the t-test that the Species vector defines the groups, while column sv[,i] defines the variable to be tested.\nWe run 4 comparisons, and assuming that there were no differences between the species, we would expect 5% of the tests to be significant at the 0.05 level ‚Äì which means that the probability of having at least one false positive in 4 tests is \\(1 - (1 - 0.05)^4 = 0.185\\). Let‚Äôs see if we can do something about it:\n\np.adjust(pvals, method=\"bonferroni\")\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n7.464578e-07 7.277934e-03 1.960115e-21 8.446138e-25 \n\np.adjust(pvals, method=\"BH\")\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n2.488193e-07 1.819483e-03 9.800575e-22 8.446138e-25 \n\n\nAs you can see, the corrected p-values are higher than the original ones, and the BH correction is less conservative (i.e., the p-values are smaller) than the Bonferroni correction.",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "appendix-more-stats.html#linear-models-with-lm",
    "href": "appendix-more-stats.html#linear-models-with-lm",
    "title": "Appendix: More statistics and visualizations",
    "section": "Linear models with lm()",
    "text": "Linear models with lm()\n\nSimple linear models\nWe end with an example of linear regression, and the most important reason for that is the introduction of formulas in R.\nA formula is a weird little construct. It contains variables, and links them using the ~ (tilde) sign. On the left side of the ~ are the dependent variables (the ‚Äúy‚Äù), on the right side are the independent variables (the ‚Äúx‚Äù, or covariates).\nDepending on the particular function, the formula can mean different things and will have different syntax. For example, in a package like the DESeq2 from Bioconductor, there will be nothing on the left side ‚Äì because DESeq2 understands that the formula applies to every single gene in the input matrix.\nHere we will use the lm() function, which is the basic linear regression  included in base R. Somewhat similar to tidyverse, you can use column names of a data frame in the formula, and specify the data frame with the data parameter. We will use it to model regression of the mathematical formlm()\n\\[ y = a + b \\cdot x + \\epsilon \\]\nwhere \\(a\\) is the intercept, \\(b\\) is the slope, and \\(\\epsilon\\) is the error.\nAs an example, we will use the mtcars dataset, which contains information about, you guessed it, cars (it is quite old ‚Äì it comes from 1974). In the data frame, there are two columns that we will use: mpg (miles per gallon, so fuel usage given in the american way), and hp (horsepower). We will try to predict the miles per gallon based on the horsepower. However, rather than model mpg, we will use its inverse ‚Äì gallons per mile, gpm, multiplied by 100 (so, effectively, gallons per 100 miles).\n\nmtcars$gpm &lt;- 100/mtcars$mpg\nmodel &lt;- lm(gpm ~ hp, data=mtcars)\n\nThe lm() function returns a model object, which contains a lot of information of no immediate use for us. To actually know the coefficients and p-values, it‚Äôs best to use either the summary() function, or the tidy() function from the broom package.\n\nsummary(model)\n\n\nCall:\nlm(formula = gpm ~ hp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.19776 -0.56724 -0.07017  0.24239  3.12691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.741786   0.456521   6.006 1.37e-06 ***\nhp          0.018277   0.002827   6.464 3.84e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 30 degrees of freedom\nMultiple R-squared:  0.5821,    Adjusted R-squared:  0.5682 \nF-statistic: 41.79 on 1 and 30 DF,  p-value: 3.839e-07\n\n\nWe have two rows in the ‚ÄúCoefficients‚Äù table, because we had two coefficients in our model: \\(a\\), the intercept, and \\(b\\), the slope. The \\(b\\) coefficient answers the question: how much more miles per gallon do we get if we reduce horse power by 1?\nWe can plot how the model fits our data with ggplot2:\n\nlibrary(ggplot2)\na &lt;- coefficients(model)[1]\nb &lt;- coefficients(model)[2]\nggplot(mtcars, aes(x = hp, y = gpm)) +\n  geom_point() +\n  geom_abline(intercept = a, slope = b)\n\n\n\n\n\n\n\n\ncoefficients()\nThe coefficients() function extracts the vector with the coefficients from the model.\nBut wait. The intercept, \\(a\\), is the fuel usage when the car‚Äôs horsepower is 0. Logically, the fuel usage of a car with 0 horsepower should be precisely 0, and not almost 3. Any value other than 0 simply doesn‚Äôt make sense. We can tell lm() that the intercept should be 0 quite easily:\n\nmodel_0 &lt;- lm(gpm ~ 0 + hp, data=mtcars)\nsummary(model_0)\n\n\nCall:\nlm(formula = gpm ~ 0 + hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6238 -0.5268  0.9824  1.4068  2.7063 \n\nCoefficients:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nhp 0.033703   0.001725   19.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.576 on 31 degrees of freedom\nMultiple R-squared:  0.9249,    Adjusted R-squared:  0.9225 \nF-statistic: 381.7 on 1 and 31 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see, we have now only one coefficient ‚Äì because we forced the other one to be 0 with the 0 + syntax.\n\n\nAdditional covariates\nThe nice thing about this type of approach is that it can be easily extended to model much more complex situations. For example, what else does the fuel usage depend on? One of the columns in the mtcars dataset is the weight of the car. We can add it to the model like this:\n\nmodel_2 &lt;- lm(gpm ~ 0 + hp + wt, data=mtcars)\nsummary(model_2)\n\n\nCall:\nlm(formula = gpm ~ 0 + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0183 -0.4441  0.1447  0.5905  1.1068 \n\nCoefficients:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nhp 0.007443   0.002364   3.148   0.0037 ** \nwt 1.330059   0.113669  11.701 1.05e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6791 on 30 degrees of freedom\nMultiple R-squared:  0.9865,    Adjusted R-squared:  0.9856 \nF-statistic:  1096 on 2 and 30 DF,  p-value: &lt; 2.2e-16\n\n\nAgain we are using the 0 + syntax to force the intercept to be 0 ‚Äì which makes sense, since a car with no horsepower and no weight should use no fuel. The summary() function shows us that the weight of the car is also significant in predicting the fuel usage, although the \\(p\\)-value for the hp coefficient is now much higher. Well, there is a correlation between horsepower and weight.\nBut which model is better? If you look at the summaries above, you will find that the R-squared value is given. This is a measure of how well the model fits the data. The closer it is to 1, the better the model. For model_0, the R-squared is 0.92, and for model_2 it is 0.99.\nHowever, adding more variables to the model will always increase the fit, leading to the situation we call overfitting, because while increasing the fit to this particular dataset we will be decreasing the models predictive power.\n\nmodel_huge &lt;- lm(gpm ~ 0 + hp + wt + qsec + drat + disp + cyl, data=mtcars)\nsummary(model_huge)\n\n\nCall:\nlm(formula = gpm ~ 0 + hp + wt + qsec + drat + disp + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6976 -0.4062  0.1508  0.3457  1.3327 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)   \nhp    0.004814   0.004045   1.190  0.24479   \nwt    1.031644   0.329317   3.133  0.00425 **\nqsec -0.011301   0.072827  -0.155  0.87788   \ndrat  0.201028   0.272197   0.739  0.46680   \ndisp  0.002103   0.003253   0.646  0.52370   \ncyl   0.062671   0.164361   0.381  0.70608   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6913 on 26 degrees of freedom\nMultiple R-squared:  0.9879,    Adjusted R-squared:  0.9851 \nF-statistic:   353 on 6 and 26 DF,  p-value: &lt; 2.2e-16\n\n\nOne way we can avoid overfitting is by using another measure of model fit, AIC (Akaike Information Criterion). The AIC() function calculates the AIC for a model, which is a measure of how well the model fits the data, but penalizes the number of parameters. The lower the AIC, the better the model.\n\nAIC(model_0)\n\n[1] 122.8965\n\nAIC(model_2)\n\n[1] 69.97504\n\nAIC(model_huge)\n\n[1] 74.53647\n\n\nIn the above examples we have been using continuous variables, but we can use almost anything with linear models. For example, we can ask how the Sepal.Length of the iris dataset depends on the Species:\n\nmodel_iris &lt;- lm(Sepal.Length ~ Species, data=iris)\nsummary(model_iris)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nIn fact, the above model is equivalent to an ANOVA test. The individual \\(p\\)-values above are actually not of immediate interest, since in ANOVA we want to first test if there is any difference between the groups, and only then test which groups differ. This can be done with the anova() function:\n\nanova(model_iris)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is much more to ANOVA, the lm() function, and to linear models in general. If you are interested, I would recommend reading the R Book by Michael J. Crawley, which is a great resource for learning statistics in R.\n\n\nInteractions\nConsider the following example. We have two strains, knockout (KO) and wild type (WT), and we measure something (let‚Äôs say, the expression of a gene) in two conditions, control and treatment.\nWe want to assess the impact of the strain on how an animal reacts to the treatment. Clearly, we can‚Äôt just compare the treatment in WT with the treatment in KO ‚Äì maybe the gene expression in KO is generally higher, independent of the treatment.\nIt would be also incorrect to compare the control with the treatment separately in the WT and KO strains. Chances are, we will get a significant difference in one, but not the other case ‚Äì but that does not mean that there is a difference between the strains. Maybe the change is really present in both strains, but we failed to detect it in one case. Here is what I mean:\n\n\n\n\n\n\n\n\n\nAs you can see, the treatment has similar effect on both WT and KO (actually, I made sure of it when generating the data set), but it is not significant in the WT (p = 0.053) and significant in the KO (p = 0.00021).\nInstead, what we need to figure out is whether the difference between the groups in KO is different from the difference between the groups in WT. This ‚Äúdifference of differences‚Äù is called an interaction. We can write it like this:\n\\[\\textrm{Interaction} = (\\textrm{treated}_{\\textrm{KO}} - \\textrm{control}_{\\textrm{KO}}) -\n(\\textrm{treated}_{\\textrm{WT}} - \\textrm{control}_{\\textrm{WT}})\\]\nInteraction will allow us to find strain-specific differences even if both, there is an effect of the strain and there is an effect of the treatment. For example, the average expression can be generally higher in one strain.\nWe should now generate a new data frame with the interaction term:\n\ndfi &lt;- data.frame(nn = 1:(n*4),\n  strain = rep(c(\"WT\", \"KO\"), each=2*n),\n  group = rep(c(\"control\", \"treatment\"), each = n)) |&gt;\n  mutate(strain = factor(strain, levels=c(\"WT\", \"KO\"))) |&gt;\n  mutate(value = \n    c(rnorm(n, mean=10, sd=1),\n      rnorm(n, mean=10, sd=1),\n      rnorm(n, mean=12, sd=1),\n      rnorm(n, mean=14, sd=1)))\nggplot(dfi, aes(x = group, y = value)) +\n  geom_boxplot() +\n  geom_beeswarm() +\n  facet_wrap(~ strain)\n\n\n\n\n\n\n\n\nAs you can see, there is an effect of the strain (the KO has a higher expression on average), and an effect of the treatment ‚Äì but only in the KO strain.\nWe can easily run an ANOVA with interaction term in R. As before, we use a formula, but now instead of adding (with a +) the two terms, strain and group, we multiply them (with a *):\n\nmodel1 &lt;- lm(value ~ strain * group, data=dfi)\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: value\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nstrain        1 64.368  64.368 43.3669 1.156e-07 ***\ngroup         1  3.044   3.044  2.0507  0.160758    \nstrain:group  1 15.938  15.938 10.7381  0.002329 ** \nResiduals    36 53.434   1.484                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the ANOVA of the model above, we see the strong effect of the strain, and although we do not see a significant overall effect of the group (because there is no effect in the WT strain), we do see a significant interaction ‚Äì that is the coefficient strain:group.\nLet us now take a look at a negative example. In the data frame below, we have an effect of the strain, but also an effect of the group ‚Äì in both strains, the expression goes up. However, the effect is identical in both strains ‚Äì the expression shifts up by the same amount in both strains. In other words, there is no interaction between strain and group.\n\ndfni &lt;- data.frame(nn = 1:(n*4),\n  strain = rep(c(\"WT\", \"KO\"), each=2*n),\n  group = rep(c(\"control\", \"treatment\"), each = n)) |&gt;\n  mutate(strain = factor(strain, levels=c(\"WT\", \"KO\"))) |&gt;\n  mutate(value = \n    c(rnorm(n, mean=10, sd=1),\n      rnorm(n, mean=12, sd=1),\n      rnorm(n, mean=13, sd=1),\n      rnorm(n, mean=15, sd=1)))\nggplot(dfni, aes(x = group, y = value)) +\n  geom_boxplot() +\n  geom_beeswarm() +\n  facet_wrap(~ strain)\n\n\n\n\n\n\n\n\n\nmodel2 &lt;- lm(value ~ strain * group, data=dfni)\nanova(model2)\n\nAnalysis of Variance Table\n\nResponse: value\n             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nstrain        1 99.952  99.952 121.5962 4.279e-13 ***\ngroup         1 26.923  26.923  32.7534 1.630e-06 ***\nstrain:group  1  1.582   1.582   1.9249    0.1738    \nResiduals    36 29.592   0.822                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA shows now a significant effect of the group (p = 1.6e-06), and a strong effect of the strain (p = 4.3e-13), but no effect of the interaction (p = 0.17).",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "appendix-more-stats.html#sec-image-sizes",
    "href": "appendix-more-stats.html#sec-image-sizes",
    "title": "Appendix: More statistics and visualizations",
    "section": "Image sizes in R markdown",
    "text": "Image sizes in R markdown\nUnlike in other programs, you can not simply drag images to change their size or aspect ratio in R, and you cannot click on a text and choose the font size. What can you do when you want to change the size of the image, to make the fonts larger relative to the image size etc.?\nFor font size, it is possible to change individual font sizes (and font types) in both basic R and ggplot2, however these operations tend to be annoying (see here for a description of how to do it in ggplot2).\nHowever, quite often what you want to achieve is simply make the overall fonts larger in an image. If you are using a vector graphic output (as you should), it does not really matter whether the font size is actually 12, 22 or 32 points ‚Äì you can always scale the image to the desired size without losing quality. What matters is how large the fonts are in relation to the nominal image size in inches.\nThe good news is: you can change the nominal size of an image in R very easily. You have already seen it done in the section 5.3.6 Output formats when generating images with svg() or pdf(). However, you can change this size also in your R Markdown document by providing chunk options fig.width and fig.height:\n\n\n```{r fig.width=4,fig.height=3}\nggplot(iris, \n  aes(x = Sepal.Length, \n      y = Sepal.Width, \n      color = Species)) +\n  geom_point(size=3) +\n  scale_color_viridis_d() +\n  labs(x = \"Sepal length\", \n       y = \"Sepal width\", \n       title = \"The iris data set\") +\n  theme_minimal()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n```{r fig.width=8,fig.height=8}\nggplot(iris, \n  aes(x = Sepal.Length, \n      y = Sepal.Width, \n      color = Species)) +\n  geom_point(size=3) +\n  scale_color_viridis_d() +\n  labs(x = \"Sepal length\", \n       y = \"Sepal width\", \n       title = \"The iris data set\") +\n  theme_minimal()\n```\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the only difference between these two chunks are the values next to fig.width and fig.height. Since the left-hand figure is smaller, even though in reality the fonts have the same sizes in points, they appear much larger.",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "appendix-more-stats.html#sec-combining-plots",
    "href": "appendix-more-stats.html#sec-combining-plots",
    "title": "Appendix: More statistics and visualizations",
    "section": "Combining several plots into one",
    "text": "Combining several plots into one\nQuite often you will want to combine several plots into one. This can be done in R in several ways, different for base R and ggplot2.\n\nBase R\nFor base R, the key function is par(). This function has many applications, but the one that we focus on here is the mfrow parameter, which specifies how many columns and how many rows you wish to have on the output. Then you create the subsequent plots as you would normally do, and they get placed in the grid. Here is an example with one row and two columns:\n\npar(mfrow=c(1, 2))\nhist(iris$Sepal.Length)\nboxplot(iris$Sepal.Length ~ iris$Species)\n\n\n\n\n\n\n\n\nThere are ways and packages to fine tune this, for example setting different sizes of columns and rows in the grid above ‚Äì‚Äì look up the layout() function if you need it.\n\n\nggplot2\nFirst, quite often in ggplot2 you can simply use facet_wrap() or facet_grid() to combine several plots into one. However, these plots are then of the same type.\nIf you want to combine different types of plots, there are two packages that might interest you. The cowplot package allows you to combine multiple plots with the plot_grid() function.\n\nlibrary(cowplot)\n\n# we save the plots into a variable \n# instead of printing them to the screen\np1 &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram()\n\np2 &lt;- ggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot()\n\n# plot both plots with labels in one row\nplot_grid(p1, p2, \n  ncol = 2,\n  labels = c('A', 'B'), \n  rel_widths = c(0.6, 0.4),\n  label_size = 12)\n\n\n\n\n\n\n\n\nThere are no limits ‚Äì you can overlay plots, you can change the relative sizes of the plots, and you can even add base R plots1:\n1¬†However, you need to install the package gridGraphics to do that.\n# use the formula notation to create a base R subplot\np3 &lt;- ~hist(iris$Sepal.Width)\n\n# create a one column, two-row subplot\np13 &lt;- plot_grid(p1, p3, \n  ncol = 1,\n  labels = c('A', 'B'), \n  label_size = 12)\n\n# put the subplots together\n# no need for a label for p13, \n# it already has labels\nplot_grid(p13, p2, \n  labels = c(\"\", \"C\"),\n  ncol=2)\n\n\n\n\n\n\n\n\nAnother package that is very popular is patchwork. It is more elegant and allows a lot of extras. This comes at the cost of a bit more complex syntax.\n\nlibrary(patchwork)\np3 &lt;- ggplot(iris, aes(x=Sepal.Width)) +\n  geom_histogram()\n(p1 / p3) | p2",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "appendix-more-stats.html#sec-barplots",
    "href": "appendix-more-stats.html#sec-barplots",
    "title": "Appendix: More statistics and visualizations",
    "section": "Boxplots and violin plots vs bar plots",
    "text": "Boxplots and violin plots vs bar plots\nBar plots are quite common in scientific literature, despite the fact that they actually should be avoided in most scenarios (‚ÄúKick the Bar Chart Habit‚Äù 2014). Bar plots should actually only be used when showing count or proportion data, and the \\(y\\) axis in this case should always start at zero. In all the other applications, box plots or, better, violin plots are preferred.\nThe advantages of the box plots and violin plots over bar plots are evident when the data is not normally distributed. Let us construct a small example.\n\nn &lt;- 250\nx &lt;- rnorm(n, mean=20, sd=1)\ny &lt;- rbeta(n, 2, 22) * 20  + 18.65\n\nThe vectors x and y are normally distributed and beta distributed, respectively, and they have been on purpose manipulated such that the standard deviations calculated with sd() and means calculated with mean() are the same. Here is a ggplot2 code that produces a bar chart. Don‚Äôt worry too much about the syntax below: it is here for demonstration purposes only, and, hopefully, you will rarely need bar plots in practice. The function geom_bar() is the one responsible for the bar plots; geom_errorbar() adds the error bars; coord_cartesian() is here to limit the y-axis to a certain range. geom_bar(), geom_errorbar(), coord_cartesian()\n\ndf &lt;- data.frame(value=c(mean(x), mean(y)), \n                 group=c(\"x\", \"y\"),\n                 sd=c(sd(x)/sqrt(n), sd(y)/sqrt(n)))\nggplot(df, aes(x=group, y=value)) + \n  geom_errorbar(aes(ymin = value - sd, ymax = value + sd), width = 0.2) +\n  geom_bar(stat=\"identity\", width=0.5, fill = \"skyblue\") +\n  labs(x=\"Group\", y=\"Value\") +\n  coord_cartesian(ylim = c(19.5, 20.5))\n\n\n\n\n\n\n\n\nWow, that looks really different! Unfortunately, the above figure is a lie. It suggests something that it is not true.\nFirstly, instead of standard deviations, which show the true spread of the data, I used the standard error of the mean (SEM) equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\), which shows the precision of the mean estimate and gets very small for large data sets even when the spread (standard deviation) is large ‚Äì but it looks good on a figure. Secondly, the y-axis does not start at zero, which is not acceptable for bar plots.\nAnd last but not least, one should not use a bar plot for continuous data, because it does not show their real distribution. Let us now produce a box plot and a violin plot.\nBefore, however, let me introduce yet another useful package, cowplot2. With this package, we can create composite plots that consist of several individual plots. Basically, first you create the ggplot plots and save them to a variable; then you use cowplot function plot_grid() to put the plots . cowplot\n2¬†There is also the newer patchwork package, which is more elegant and flexible, but the syntax requires a bit of getting used to.plot_grid()\n\nlibrary(cowplot)\ndf &lt;- data.frame(values = c(x, y), \n                 group = c(rep(\"x\", n), rep(\"y\", n)))\np1 &lt;- ggplot(df, aes(x=group, y=values)) +\n  geom_boxplot() +\n  labs(x=\"Group\", y=\"Value\")\n\n# violin plot\np2 &lt;- ggplot(df, aes(x=group, y=values)) +\n  geom_violin() +\n  geom_boxplot(width=0.1) +\n  labs(x=\"Group\", y=\"Value\")\n\n# plot_grid puts the different plots together \n# ncol=2 -&gt; two columns\n# labels=... -&gt; labels for the plots\nplot_grid(p1, p2, ncol=2, labels=c(\"A\", \"B\"))\n\n\n\n\n\n\n\n\nAs you can see, the violin plots show a completely different story. The group y only looks larger, because the mean is driven by the long upper tail of the distribution. The medians are practically identical (median of x is 19.97, median of y is 20.01), and the distributions largely overlap.\nSimilarly, if you were to run a t-test, which assumes normal distribution, the p-value would have been 0.00044; while in a Wilcoxon test, which does not make this assumption, the p-value would have been 0.02.\n\n\n\n\n‚ÄúKick the Bar Chart Habit.‚Äù 2014. Nature Methods 11: 113. https://doi.org/10.1038/nmeth.2837.",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions to Exercises",
    "section": "",
    "text": "Day 1",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "solutions.html#day-1",
    "href": "solutions.html#day-1",
    "title": "Solutions to Exercises",
    "section": "",
    "text": "Water lilies (Exercise¬†1.12)\n\n# plots with a blue color\nplot(days, area, type=\"b\", col=\"blue\")\n\n\n\n\n\n\n\n# plots with filled circles\nplot(days, area, type=\"b\", col=\"blue\", pch=19)\n\n\n\n\n\n\n\n# plots with labels\nplot(days, area, type=\"b\",\n  xlab=\"Day\", ylab=\"Area\", main=\"Area of water lilies\")\n\n\n\n\n\n\n\n# limits on the y axis and x axis\nplot(days, area, type=\"b\",\n  xlim=c(0, 8), ylim=c(0, 1))\n\n\n\n\n\n\n\n\nFormula for area when the growth is by 75% per day:\n\\[a(n) = 0.01 \\times 1.75^{n-1}\\]\n\narea_slow &lt;- 0.01 * 1.75^(days - 1)\nplot(days, area, type=\"b\", col=\"blue\")\n\n# add another data series to the plot\nlines(days, area_slow, type=\"b\", col=\"red\")",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "solutions.html#day-2",
    "href": "solutions.html#day-2",
    "title": "Solutions to Exercises",
    "section": "Day 2",
    "text": "Day 2\n\nMatrices (Exercise¬†2.3)\n\n# generate random readouts\nreadouts &lt;- runif(48)\n\n# create the matrix\nres_mtx &lt;- matrix(readouts, nrow=6)\n\n# always check your results!\ndim(res_mtx)\n\n[1] 6 8\n\n\nTo change the ‚Äúborders‚Äù to NA:\n\nres_mtx[1,] &lt;- NA\nres_mtx[,1] &lt;- NA\nres_mtx[nrow(res_mtx), ] &lt;- NA\nres_mtx[, ncol(res_mtx)] &lt;- NA\n\nAbove, we could have used res_mtx[6, ] &lt;- NA instead of res_mtx[nrow(res_mtx), ] &lt;- NA, but the latter is more general and will keep working if you decide to use a matrix with a different layout.\nRow and column names:\n\nrow_names &lt;- c(\"control\", \"low\", \"medium\", \"high\")\nrow_names &lt;- paste0(\"Inh1_\", row_names)\nrow_names &lt;- c(NA, row_names, NA)\nrownames(res_mtx) &lt;- row_names\n\ncol_names &lt;- c(\"control\", \"low\", \"high\")\ncol_names &lt;- rep(col_names, 2)\n\ninh &lt;- rep(c(\"Inh2_\", \"Inh3_\"), each=3)\ncol_names &lt;- paste0(inh, col_names)\ncol_names &lt;- c(NA, col_names, NA)\ncolnames(res_mtx) &lt;- col_names\n\nAbove, we use NA for row names and column names of the border row / column. We could have used another value as well, it doesn‚Äôt matter.\nSelecting from the matrix:\n\n# wells with inhibitor 3\nres_mtx[ 2:5, 5:6 ]\n\n             Inh3_control  Inh3_low\nInh1_control    0.1176258 0.4576290\nInh1_low        0.2526631 0.2369185\nInh1_medium     0.7776389 0.3400686\nInh1_high       0.3190512 0.6485127\n\n# inhibitor 1 as well as 2\ninh1 &lt;- c(\"Inh1_low\", \"Inh1_medium\", \"Inh1_high\")\ninh2 &lt;- c(\"Inh2_low\", \"Inh2_high\")\nres_mtx[ inh1, inh2 ]\n\n             Inh2_low  Inh2_high\nInh1_low    0.4650854 0.68540332\nInh1_medium 0.6189359 0.10531005\nInh1_high   0.6080377 0.02720906\n\n\n\n\nData frames (Exercise¬†2.5)\n\nmtx &lt;- matrix(rnorm(15), nrow=5)\ndf &lt;- as.data.frame(mtx)\n\ncolnames(df) &lt;- c(\"X\", \"Y\", \"Z\")\nrownames(df) &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\n\ndf$A &lt;- rep(\"A\", 5)\n\ndf$sequence &lt;- seq(0, 1, length.out=5)\n\ndf\n\n           X          Y          Z A sequence\na -0.3551887 -0.3379952  1.4349657 A     0.00\nb  0.5389208  0.2618027 -0.3364332 A     0.25\nc -0.1786603  1.0218908 -2.1030068 A     0.50\nd -0.8444127  1.0125655 -0.3731203 A     0.75\ne -0.3119014 -0.7316247  0.2766870 A     1.00",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "solutions.html#day-3",
    "href": "solutions.html#day-3",
    "title": "Solutions to Exercises",
    "section": "Day 3",
    "text": "Day 3\n\nDeaths.xlsx (Exercise¬†3.5)\nIf you look at the file, you will find that there are 4 lines with gibberish (which you can skip with skip=4), then the header row, then 10 lines with actual data, and finally a few lines with gibberish again. Thus, we need to read 1 + 10 = 11 lines, starting with line 5, and finishing with line 16.\n\nfn &lt;- readxl_example(\"deaths.xls\")\n\n# one way: use the n_max argument\ndeaths &lt;- read_excel(fn, skip=4, n_max=11)\n\n# another way: use the excel range specification\ndeaths &lt;- read_excel(fn, range=\"A5:F16\")\n\n\n\nDiagnosing meta_data_botched.xlsx (Exercise¬†3.7)\n\nbotched &lt;- read_excel(\"Datasets/meta_data_botched.xlsx\")\ncolnames(botched)\n\n[1] \"SUBJ\"       \"time point\" \"AGE\"        \"SEX\"        \"PLACEBO\"   \n[6] \"ARM\"       \n\n\nFirst thing that we see is that one of the columns is called time point with a space in between. This is neither conforming to the other column names (which are ALL CAPS) nor it is a good idea to have spaces in column names.\n\nsummary(botched)\n\n      SUBJ       time point            AGE                SEX           \n Min.   :4023   Length:122         Length:122         Length:122        \n 1st Qu.:4232   Class :character   Class :character   Class :character  \n Median :4429   Mode  :character   Mode  :character   Mode  :character  \n Mean   :4473                                                           \n 3rd Qu.:4701                                                           \n Max.   :4973                                                           \n   PLACEBO              ARM           \n Length:122         Length:122        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\nlibrary(skimr)\nskim(botched)\n\n\nData summary\n\n\nName\nbotched\n\n\nNumber of rows\n122\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntime point\n0\n1\n2\n5\n0\n8\n0\n\n\nAGE\n0\n1\n2\n6\n0\n34\n0\n\n\nSEX\n0\n1\n1\n8\n0\n12\n0\n\n\nPLACEBO\n0\n1\n1\n5\n0\n10\n0\n\n\nARM\n0\n1\n1\n16\n0\n20\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSUBJ\n0\n1\n4472.51\n287.12\n4023\n4232\n4429\n4701\n4973\n‚ñá‚ñá‚ñá‚ñÖ‚ñÜ\n\n\n\n\n\nOK, so we see several problems. First, the column ‚ÄúAge‚Äù is not numeric, but we would expect it to be. Let‚Äôs take a closer look:\n\nage_n &lt;- as.numeric(botched$AGE)\n\nWarning: NAs introduced by coercion\n\nbotched$AGE[ is.na(age_n) ]\n\n [1] \"32 ,\"   \"23 yrs\" \"28 ,\"   \"30 ,\"   \"3 7\"    \"23 yrs\" \"32 yrs\" \"40 yrs\"\n [9] \"35 yrs\" \"22 ,\"   \"30 ,\"  \n\n\nRight, so someone added extra text or spaces to the the actual values. And what does ‚Äú3 0‚Äù mean? It could be 30, but it also could mean ‚Äú3 years 0 months‚Äù ‚Äì who knows?\nNext, the SEX column. skim() shows that it has 12 unique values, but this is not the number we expect. What are these unique values?\n\nunique(botched$SEX)\n\n [1] \"Mann\"     \"M\"        \"m√§nnlich\" \"F\"        \"female\"   \"Female\"  \n [7] \"Herr\"     \"Male\"     \"male\"     \"Frau\"     \"Femal\"    \"Weiblich\"\n\n\nMaybe the data was entered by hand by different people, and they did not agree beforehand on how to enter the data. We see a similar problem also in the columns PLACEBO, ARM and time point:\n\nunique(botched$PLACEBO)\n\n [1] \"TRUE\"  \"yes\"   \"T\"     \"1\"     \"true\"  \"FALSE\" \"false\" \"0\"     \"no\"   \n[10] \"F\"    \n\nunique(botched$ARM)\n\n [1] \"Placebo\"          \"placebo\"          \"P\"                \"Plazebo\"         \n [5] \"control\"          \"Placibo\"          \"F\"                \"fluad vaccine\"   \n [9] \"A\"                \"Fl.\"              \"agrippal vaccine\" \"agrippal\"        \n[13] \"Agrippal vaccine\" \"grippal\"          \"AGRP.\"            \"Fuad\"            \n[17] \"Fluad\"            \"Agrippal\"         \"FLUAD\"            \"Fluad vaccine\"   \n\nunique(botched[[\"time point\"]])\n\n[1] \"D0\"    \"D1\"    \"Day1\"  \"D 0\"   \"day 1\" \"day 0\" \"D 1\"   \"Day0\" \n\n\n\n\nCorrecting meta_data_botched.xlsx (Exercise¬†3.13)\nLet‚Äôs start with the easiest one, the time point. First, we should fix the name of the column:\n\n# either works\nbotched &lt;- rename(botched, TIMEPOINT=`time point`)\n# or\ncolnames(botched)[2] &lt;- \"TIMEPOINT\"\ncolnames(botched)\n\n[1] \"SUBJ\"      \"TIMEPOINT\" \"AGE\"       \"SEX\"       \"PLACEBO\"   \"ARM\"      \n\n\nNow, fix the values in the TIMEPOINT column. We can use the toupper().\n\n# change to upper case\ntp &lt;- toupper(botched$TIMEPOINT)\n\n# replace day with \"D\"\ntp &lt;- str_replace_all(tp, \"DAY\", \"D\")\n\n# remove all spaces\ntp &lt;- str_replace_all(tp, \" *\", \"\")\n\ntable(tp, botched$TIMEPOINT)\n\n    \ntp   D 0 D 1 D0 D1 day 0 day 1 Day0 Day1\n  D0   5   0 52  0     3     0    1    0\n  D1   0   5  0 50     0     2    0    4\n\n# looks good!\nbotched$TIMEPOINT &lt;- tp\n\nThe ARM column should be easy, too. We only have to pay attention to the first letter. There are two exceptions, though ‚Äì sometimes control is used instead of placebo, and in some cases grippal has been used instead of agrippal.\n\narm &lt;- toupper(botched$ARM)\narm &lt;- str_replace_all(arm, \"^P.*\", \"PLACEBO\")\narm &lt;- str_replace_all(arm, \"^F.*\", \"FLUAD\")\narm &lt;- str_replace_all(arm, \"^A.*\", \"AGRIPPAL\")\narm &lt;- str_replace_all(arm, \"^C.*\", \"PLACEBO\")\narm &lt;- str_replace_all(arm, \"^G.*\", \"AGRIPPAL\")\nunique(arm)\n\n[1] \"PLACEBO\"  \"FLUAD\"    \"AGRIPPAL\"\n\nbotched$ARM &lt;- arm\n\nProceed with the columns SEX and PLACEBO in the same way.\nFinally, AGE. The simplest way would be to replace everything that is not a digit:\n\nage &lt;- botched$AGE\nage &lt;- str_replace_all(age, \"[^0-9]\", \"\")\nage &lt;- as.numeric(age)\n\n# check for NAs\nany(is.na(age))\n\n[1] FALSE\n\n# replace the original column\nbotched$AGE &lt;- age",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "solutions.html#day-4",
    "href": "solutions.html#day-4",
    "title": "Solutions to Exercises",
    "section": "Day 4",
    "text": "Day 4\n\nSelecting columns (Exercise¬†4.1)\nSolution to Exercise¬†4.1.\n\n# Load the data\nlibrary(tidyverse)\n\nresults &lt;- read_csv(\"Datasets/transcriptomics_results.csv\")\n\n# what columns are there?\ncolnames(results)\n\n [1] \"...1\"           \"ProbeName\"      \"GeneName\"       \"SystematicName\"\n [5] \"Description\"    \"logFC.F.D0\"     \"qval.F.D0\"      \"logFC.F.D1\"    \n [9] \"qval.F.D1\"      \"logFC.F.D2\"     \"qval.F.D2\"      \"logFC.F.D3\"    \n[13] \"qval.F.D3\"     \n\n# Select the columns that we need\nresults &lt;- select(results, GeneName, Description, logFC.F.D1, qval.F.D1)\ncolnames(results) &lt;- c(\"Gene\", \"Description\", \"logFC\", \"qval\")\n\nAlternatively, we could have specified the new column names directly in the select() function. This is useful if you want to rename only some of the columns:\n\nresults &lt;- select(results, Gene=GeneName, \n  Description, \n  LFC=logFC.F.D1, FDR=qval.F.D1)\ncolnames(results)\n\n[1] \"Gene\"        \"Description\" \"LFC\"         \"FDR\"        \n\n\n\n\nSorting by last name (Exercise¬†4.3)\n\npersons &lt;- c(\"Henry Fonda\", \"Bob Marley\", \"Robert F. Kennedy\", \n  \"Bob Dylan\", \"Alan Rickman\")\n\n# first, create a vector with last names only. \n# basically, remove everything before the last space\n# and the space itself\nlastnames &lt;- str_replace_all(persons, \".* \", \"\")\nlastnames\n\n[1] \"Fonda\"   \"Marley\"  \"Kennedy\" \"Dylan\"   \"Rickman\"\n\n# now get the order for the last names\nord &lt;- order(lastnames)\nord\n\n[1] 4 1 3 2 5\n\n# and use it to sort the original vector\npersons[ord]\n\n[1] \"Bob Dylan\"         \"Henry Fonda\"       \"Robert F. Kennedy\"\n[4] \"Bob Marley\"        \"Alan Rickman\"     \n\n\n\n\nLogical vectors (Exercise¬†4.6)\n\nsignificant &lt;- tr_res$FDR &lt; 0.05\ninterferons &lt;- str_detect(tr_res$Description, \"interferon\")\nboth &lt;- significant & interferons\n\n# how many are there?\nsum(both)\n\n[1] 23\n\n# how many are significant, but not interferons?\nsum(significant & !interferons)\n\n[1] 1455\n\ngbps &lt;- str_detect(tr_res$Gene, \"^GBP\")\nsum(gbps & interferons)\n\n[1] 4\n\n\n\n\nThe merging of two data frames (Exercise¬†4.9)\nSolution to Exercise¬†4.9.\nFirst, we load the data. Nothing fancy here. You should examine the resulting data frames with View (or by clicking on the data frame in the Environment pane in RStudio), but in the code below we simply list the available columns.\n\n# The libraries needed\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Load the data\nlabresults &lt;- read_csv(\"Datasets/labresults_full.csv\")\ntargets &lt;- read_excel(\"Datasets/expression_data_vaccination_example.xlsx\", \n  sheet=\"targets\")\n\n# what columns are there?\ncolnames(labresults)\n\n [1] \"USUBJID\"   \"SUBJ\"      \"LBTEST\"    \"LBTESTCD\"  \"LBCAT\"     \"LBORRES\"  \n [7] \"LBORRESU\"  \"LBSPEC\"    \"VISIT\"     \"Timepoint\"\n\ncolnames(targets)\n\n [1] \"USUBJID\"   \"SUBJ\"      \"Batch\"     \"ID\"        \"Timepoint\" \"ARMCD\"    \n [7] \"ARM\"       \"AGE\"       \"SEX\"       \"PLACEBO\"  \n\nintersect(colnames(labresults), colnames(targets))\n\n[1] \"USUBJID\"   \"SUBJ\"      \"Timepoint\"\n\n\nintersect\nThe intersect() function returns the common elements between two vectors. In this case, it returns the common column names between the two data frames. As you can see, there are USUBJID and SUBJ columns in both data frames, chances are that there are common elements between the data frame here. We will focus at the SUBJ column.\nOK, do we see any common subjects between the two data frames?\n\n# how many unique subjects are there in each data frame?\nlength(unique(labresults$SUBJ))\n\n[1] 123\n\nlength(unique(targets$SUBJ))\n\n[1] 61\n\n# how many are common?\nlength(intersect(labresults$SUBJ, targets$SUBJ))\n\n[1] 61\n\n\nRight, so there are more unique subjects in the labresults data frame than in the targets data frame.\nThe other column that is common between the two data frames is Timepoint. That already indicates that each row ‚Äì in both data frames ‚Äì corresponds to a sample rather than subject. That is, we need to identify the rows not only by subject, but also by timepoint.\nIf you use the unique() function as above, you will find that there are only 2 unique timepoints in the targets data frame, but 23 unique timepoints in the labresults data frame.\nNow first let us select only the column that we need, as mentioned in the exercise. For the targets data frame, we need the columns SUBJ and Timepoint, as well as ARM, Timepoint, AGE and SEX.\n\ntargets &lt;- select(targets, SUBJ, Timepoint, ARM, AGE, SEX)\n\nIf you inspect the labresults data frame, you will see that it has only one column that looks like a measurement ‚Äì LBORRES. Also, the column LBTEST ostensibly contains the name of the test that was performed, and LBTESTCD contains the code for the test. However, we also find the Timepoint column here as well.\n\nlabresults &lt;- select(labresults, SUBJ, Timepoint, LBTEST, LBTESTCD, LBORRES)\n\nDepending on what one wants to do, we can try to get one of the four types of join (inner, left, right, full) between the two data frames. However, assuming that the goal is correlate expression data with lab data, we will need an inner join, so either the tidyverse function inner_join() or the base R function merge() with default parameter values will do the job.\n\njoined &lt;- merge(targets, labresults, by=c(\"SUBJ\", \"Timepoint\"))\ndim(joined)\n\n[1] 3782    8\n\ncolnames(joined)\n\n[1] \"SUBJ\"      \"Timepoint\" \"ARM\"       \"AGE\"       \"SEX\"       \"LBTEST\"   \n[7] \"LBTESTCD\"  \"LBORRES\"  \n\n\nYep, seems to have worked well. The resulting data frame has 3782 rows ‚Äì since we have many different measurements for each person.",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "solutions.html#day-5",
    "href": "solutions.html#day-5",
    "title": "Solutions to Exercises",
    "section": "Day 5",
    "text": "Day 5",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann.\n2021. ‚ÄúGene Name Errors: Lessons Not Learned.‚Äù PLoS\nComputational Biology 17 (7): e1008984. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984.\n\n\n‚ÄúKick the Bar Chart Habit.‚Äù 2014. Nature Methods\n11: 113. https://doi.org/10.1038/nmeth.2837.\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022.\n‚ÄúVenn Diagrams May Indicate Erroneous Statistical Reasoning in\nTranscriptomics.‚Äù Frontiers in Genetics 13: 818683. https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.818683/full.\n\n\nWeiner, January, David JM Lewis, Jeroen Maertzdorf, Hans-Joachim\nMollenkopf, Caroline Bodinham, Kat Pizzoferro, Catherine Linley, et al.\n2019. ‚ÄúCharacterization of Potential Biomarkers of Reactogenicity\nof Licensed Antiviral Vaccines: Randomized Controlled Clinical Trials\nConducted by the BIOVACSAFE Consortium.‚Äù Scientific\nReports 9 (1): 20362. https://www.nature.com/articles/s41598-019-56994-8.",
    "crumbs": [
      "Introduction",
      "References"
    ]
  }
]