[
  {
    "objectID": "day3-reading-your-data.html",
    "href": "day3-reading-your-data.html",
    "title": "3¬† Reading and Writing Files",
    "section": "",
    "text": "3.1 Aims for today\nToday is a special day. If there is one thing that I would like you to learn from this course, it is how to read and write the data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#aims-for-today",
    "href": "day3-reading-your-data.html#aims-for-today",
    "title": "3¬† Reading and Writing Files",
    "section": "",
    "text": "Reading data\nCleaning data\nRegular expressions\nData management",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#reading-data",
    "href": "day3-reading-your-data.html#reading-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.2 Reading data",
    "text": "3.2 Reading data\n\n3.2.1 Data types\nIn your work you will encounter many different types of data. Most frequently, you will work with tabular data, either as Excel files or as comma or tab separated values (CSV and TSV files, respectively). I am sure you have worked with such files before.\n To read these files, we will use two packages: readr and readxl. The former, readr, is part of the tidyverse package, so when you load the tidyverse using library(tidyverse), readr is loaded as well. The latter, readxl, is a separate package that you need to install and load separately.readr and readxl packages\n There are also ‚Äúbase R‚Äù functions read.table, read.csv, read.tsv (there is no function for reading XLS[X] files in base R). These are always available when you start R, but don‚Äôt use them. The tidyverse functions are not only faster, but also much better behaving and, which is most important, they are safer ‚Äì it is less likely to mess up your data with them.read.table(), read.csv(), read.tsv()\n tidyverse functions return tibbles, which, as you remember from yesterday, are a special flavor of data frames. Just to refresh your memory, here are key facts about tibbles:tibbles\n\nin most of the cases, they behave exactly like data frames\nwhen you print them, they are nicer\ntibbles have no row names\nwhen you select columns using [ , sel ], you always get a tibble, even if you selected only one column\n\nread_table(), read_tsv(), read_csv(), read_delim(), read_xls(), read_xlsx(), read_excel()\n\n\n\n\n\n\n\n\n\nData type\nFunction\nPackage\nNotes\n\n\n\n\nColumns separated by spaces\nread_table()\nreadr/tidyverse\none or more spaces separate each column\n\n\nTSV / TAB separated values\nread_tsv()\nreadr/tidyverse\nDelimiter is tab (\\t).\n\n\nCSV / comma separated\nread_csv()\nreadr/tidyverse\nComma separated values\n\n\nAny delimiter\nread_delim()\nreadr/tidyverse\nCustomizable\n\n\nXLS (old Excel)\nread_xls() read_excel()\nreadxl\nAvoid using XLS files. From the readxl package.\n\n\nXLSX (new Excel)\nread_xlsx() read_excel()\nreadxl\nFrom the readxl package. You need to provide the sheet number you wish to read. Note: returns a tibble, not a data frame!\n\n\n\nAs you can see, the functions we recommend to use can be used by loading the packages tidyverse and readxl. If you haven‚Äôt done that yet, please install these packages now:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")\n\nHowever, before we start using these functions, we need to dive into a very important problem: where are your files?\n\n\n\n\n\n\nRemember!\n\n\n\n\nFor reading text files (csv, tsv etc.), use the readr package. This package is loaded automatically when you load the tidyverse package: library(tidyverse). Then, use the functions read_csv, read_tsv etc.\nFor reading Excel files, use the readxl package: library(readxl). Then, use the function read_excel.\n\n\n\n\n\n3.2.2 Where is my file? Relative and absolute paths\nWe are coming now to a crucial problem and a source of endless frustration for beginning R programmers: how to tell R where your file is. Fortunately, there are many ways to deal with that, both with R and RStudio. Still, this is a key problem and we would like you to spend some time on the following chapter.\n In order to access a file, a program (any program) needs what is known a path. Path is a character string that tells the program how to get to the file and looks, for example, like this on a Windows computer: C:/Users/johndoe/Documents/RProjects/MyFirstRProject/data.csv, and like this on a Mac: /Users/johndoe/Documents/RProjects/MyFirstRProject/data.csv.Path\n\n\n\n\n\n\nPath separators on different systems\n\n\n\nMost computer systems separate the directories and files in a path using a slash (/). However, Windows uses a backslash (\\). This is quite annoying for R users, because in R character vectors, the slash has a special meaning. To use backslashes, you need to ‚Äúescape‚Äù them by putting another backslash in front of each backslash. So instead of C:\\Users\\johndoe\\Documents, you need to write C:\\\\Users\\\\johndoe\\\\Documents. Alternatively, you can use the forward slash even on a Windows system, so type C:/Users/johndoe/Documents. We recommend the latter approach.\nThis is also why simply copying a path from the Windows Explorer to your R script will not work in R ‚Äì because the copied text contains single backslashes.\n\n\nIf R cannot find your file, it will return an error message. At first, you will be seeing this error a lot:\nError: file not found\n\nlibrary(tidyverse)\nmydata &lt;- read_csv(\"data.csv\")\n\nError: 'data.csv' not found in the current working directory\n('C:/Users/johndoe/Documents/RProjects/MyFirstRProject/').\n\n\n Before we proceed, you need to understand one important thing. When you start your RStudio and R session, the R session runs in a specific directory. This is called the working directory. You can check what is using the getwd() function1:getwd()1¬†You can change it using the setwd() function, but avoid doing that. This path leads to madness, trust me on that.\n\ngetwd()\n\n[1] \"C:/Users/johndoe/Documents/RProjects/MyFirstRProject\"\n\n\nOf course, the result will be different on your computer. By the way, the above is called an absolute path. That means that it works no matter where you are in your system, because a program can always find the file or directory using this path.\nThe easiest way to read the data is this: copy your data files to the directory returned by getwd().\n\n\n\n\n\n\n\nExercise 3.1 (Reading your first file) ¬†\n\nCheck your working directory using getwd()\nLoad the tidyverse package using library(tidyverse)\nGo to the URL https://github.com/bihealth/RCrashcourse-book/Datasets\nClick on ‚Äúiris.csv‚Äù\nClick on the ‚ÄúDownload raw file‚Äù button on the right side of the screen\nSave the file in the directory returned by getwd()\nRead the file using read_csv(\"iris.csv\")\n\n\n\n\n\nThe following code should now work without an error:\n\nlibrary(tidyverse)\niris_data &lt;- read_csv(\"iris.csv\")\n\nNow the contents of the file are stored in the iris_data object. There are many ways to have a look at it:\n\ntype iris_data or print(iris_data) in the console\ntype View(iris_data) in the console\nclick on the little spreadsheet icon next to the iris_data object in the Environment tab in RStudio (upper right panel)\n\nPlease make sure that the above works for you. If it does not, read the instructions again. In RStudio, there is a ‚ÄúFiles‚Äù tab in the lower right panel. You should see your working directory as well as the ‚Äúiris.csv‚Äù file there.\n\n\n3.2.3 Reading with relative paths\nOK, so far, so good. That was easy. Now comes a slightly harder part.\nSaving your data files in the working directory works well if you have one or two. However, the more files you read and write, the more cluttered your project directory becomes. You will soon find yourself in a situation where you have no idea which file is which.\nIt is generally a good idea to keep your data files in a separate directory, or even multiple directories.\n\n\n\n\n\n\n\nExercise 3.2 (Reading your first file from a data directory) ¬†\n\nIn the working directory, create a new directory called ‚ÄúDatasets‚Äù\nMove the ‚Äúiris.csv‚Äù file to the ‚ÄúDatasets‚Äù directory\nRead the file using read_csv(\"Datasets/iris.csv\")\n\n\n\n\n\n The path ‚ÄúDatasets/iris.csv‚Äù is called a relative path, because it is relative to the working directory and will not work from another location. So why should we use relative paths? Wouldn‚Äôt it be easier to use absolute paths all the time, for example read_csv(\"C:/Users/johndoe/Documents/RProjects/MyFirstRProject/Datasets/iris.csv\")?Relative path\nActually, no. The problem is that if you move your R project to another location, of if you share it with someone else, the absolute path will no longer work. In other words, the absolute path is not portable.\n\n\n\n\n\n\nRemember!\n\n\n\nDo not use absolute paths in your code. Always use relative paths.\n\n\n\n\n3.2.4 More on relative paths\nSome times the data files are not in your R project directory. For example, you are writing your PhD thesis. You have created a directory called ‚ÄúPhD‚Äù, which contains directories ‚Äúmanuscript‚Äù, ‚Äúdata‚Äù, ‚Äúimages‚Äù, ‚ÄúR_project‚Äù and so on. You use R to do a part of the calculations, but you want to keep the data files in the ‚Äúdata‚Äù directory. How to read them?\nWhen you type getwd(), you will get the path to the ‚ÄúR_project‚Äù directory, something like C:/Users/johndoe/Documents/PhD/R_project. The date files are in the directory C:/Users/johndoe/Documents/PhD/data. To get the relative path from the R project directory to the data directory, think about how you would navigate from one directory to another in the Windows Explorer or Finder. You need to go up one level, to get into ‚ÄúPhD‚Äù, and then down again to ‚Äúdata‚Äù.\n Getting ‚Äúup one level‚Äù in a path is done by using ... So the relative path to the file ‚Äúiris.csv‚Äù in your ‚Äúdata‚Äù directory is ../data/iris.csv.Up one level with ..\n\n\n\n\n\n\n\nExercise 3.3 (Reading a file from a data directory using relative paths) ¬†\n\nIn the directory that contains the working directory create a directory called ‚ÄúData‚Äù. That is, if your working directory is C:/Users/johndoe/Documents/PhD/R_project, create the directory C:/Users/johndoe/Documents/PhD/Data\nMove the ‚Äúiris.csv‚Äù file to the new ‚ÄúData‚Äù directory\nRead the file using read_csv(\"../Data/iris.csv\")\n\n\n\n\n\nBut what about the case when your data directory is at a completely different location? For example, on a different drive, or maybe on your desktop?\nFirst, I don‚Äôt recommend keeping your data files separately from the R project directory. In general, try to put everything in one place, as part of one structure. This structure can be complex, but it should be coherent. If necessary, copy the data files into your R project directory.\nHowever, sometimes it simply isn‚Äôt possible. Maybe the files are huge and you need to read them from a special drive. In this case, there are three options.\nUsing absolute paths. Yes, I told you not to use absolute paths, but sometimes you have no choice.\nCreate shortcuts. In all systems it is possible to create shortcuts to your data directories (on Unix-like systems like MacOS they are called ‚Äúsymbolic links‚Äù). You can put these shortcuts in your R project directory ‚Äì R will treat them as normal directories.\nCreate a complex relative path. Depending on how far ‚Äúaway‚Äù your data directory is, you can create a complex relative path. For example, if your R project directory is C:/Users/johndoe/Documents/PhD/R_project and your data directory is D:/Data, you can use the path ../../../../Data/iris.csv. Unfortunately, despite being a relative path, this is not very portable (and it is easy to lose count on the ..‚Äôs).\n\n\n3.2.5 Using RStudio to import files\nRStudio has a very nice feature that allows you to import files using a point-and-click interface. When you click on a data file in the ‚ÄúFiles‚Äù tab (lower right panel), you will see two options: ‚ÄúView File‚Äù and ‚ÄúImport Dataset‚Äù. Choosing the latter opens a dialog window which shows a preview of the file and allows you to construct your read_csv, read_tsv or another command using a point-and-click interface. You also see a preview of the expression that will be executed.\nThis feature is particularly useful when you are not sure about the format of the data to import, e.g.¬†what type of delimiter is used, how many lines should be skipped etc.\nThen you can click on ‚ÄúImport‚Äù to actually run the command, however I would recommend another approach. Clicking on ‚ÄúImport‚Äù runs the command directly in the console, bypassing your script ‚Äì and you should always enter the code in your script before executing it.\nRather than clicking on ‚ÄúImport‚Äù, click on the little Copy icon next to the ‚ÄúCode preview‚Äù field, and then cancel the dialog. Paste the copied code into your script, and then run it.\nThere is one more thing to modify. The code generated by RStudio often uses absolute paths. Try to modify it to use relative paths to ensure portability of your script.\n\n\n\n\n\n\n\nExercise 3.4 (Reading data) ¬†\n\nGo to the URL https://github.com/bihealth/RCrashcourse-book/Datasets\nDownload the following files:\n\nTB_ORD_Gambia_Sutherland_biochemicals.csv\niris.tsv\nmeta_data_botched.xlsx\n\nAlternatively, you can download the whole repository as a ZIP file and unpack it.\nSave the files in the ‚ÄúData‚Äù directory in your working directory ‚Äì or another location of your choice.\nRead the files using the appriopriate functions. Consult the table above for the correct function names, or use the RStudio data import feature. Make sure that you are using relative paths.\n\n\n\n\n\n\n\n3.2.6 Reading Excel files\nReading files sometimes requires diligence. This is especially true for Excel files ‚Äì they can contain multiple sheets, tables often do not start on the first row etc.\n The package readxl (which you hopefully successfully used to read the XLSX file in the previous exercise) contains several example files. They have been installed on your system when you installed the package. Manually finding these example files is annoying, and that is why the readxl package provides a convenience function, readxl_example(), that returns the absolute path to the file (yes, I know what I said about absolute paths; this is an exception).Reading excel files with readxl package\nreadxl_example(),read_excel()\n\nlibrary(readxl)\nfn &lt;- readxl_example(\"deaths.xls\")\nprint(fn)\n\n[1] \"/home/january/R/x86_64-pc-linux-gnu-library/4.4/readxl/extdata/deaths.xls\"\n\ndeaths &lt;- read_excel(fn)\n\nNew names:\n‚Ä¢ `` -&gt; `...2`\n‚Ä¢ `` -&gt; `...3`\n‚Ä¢ `` -&gt; `...4`\n‚Ä¢ `` -&gt; `...5`\n‚Ä¢ `` -&gt; `...6`\n\n\nIf you view this file (or if you use the RStudio data import feature), you will notice that the actual data in this file starts on line 5; the first 4 lines contain the text ‚ÄúLots of people simply cannot resist writing some notes at the top of their spreadsheets or merging cells‚Äù. To skip these lines, we need to use an argument to the read_excel function. If you look up the help file for the function (e.g.¬†using ?read_excel), you will find the following line:\nskipping lines with read_excel(skip=...)\nskip    Minimum number of rows to skip before reading anything,\n        be it column names or data. Leading empty rows are \n        automatically skipped, so this is a lower bound. Ignored\n        if range is given.\nTherefore, we can modify the code above to skip the first four lines:\n\ndeaths &lt;- read_excel(fn, skip=4)\n\n\n\n\n\n\n\n\nExercise 3.5 (Reading data with options) If you take a closer look at the file deaths.xls, you will notice that there is some extra text at the bottom of the file as well. How can you omit that part when reading? Hint: there are two ways to do that. If in doubt, look up the ‚Äúexamples‚Äù section of the readxl helpfile.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#diagnosing-and-cleaning-data",
    "href": "day3-reading-your-data.html#diagnosing-and-cleaning-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.3 Diagnosing and cleaning data",
    "text": "3.3 Diagnosing and cleaning data\n\n3.3.1 Diagnosing datasets\nCongratulations! You are now proficient in reading data into R. Believe me or not, this alone is an important step forward ‚Äì many who try their hand at R get stuck at this point.\nHowever, reading data is only the first step. In most of the cases, the data requires some treatment: cleaning, transformation, filtering etc. In many projects that I have worked on as a bioinformatician, data import, diagnosis and cleaning took up the majority of time spent on the project. Unfortunately, this is necessary before any real fun with the data can start.\n Let us examine the file iris.csv that you have just read. The dataset comes from a famous paper by Ronald Fisher, who used it to demonstrate his newly developed method called linear discriminant analysis ‚Äì an early machine learning algorithm, if you will. The dataset contains measurements (in cm) of 150 flowers of three species of irises: Iris setosa, Iris versicolor and Iris virginica. The measurements are the sepal and petal length and width, four measurements in total. Each row consists of these four measurements, plus a column which contains the species name.The iris dataset\nI have doctored the file to mimick typical problems with imported data, especially in clinical trial settings. Humans who enter data make errors, this is normal and expected, all of us do. Before we analyse them, we need to correct them. Before we correct them, we need to find them.\nFirst, note that when you read the data using read_csv, the function conveniently shows what types of data were assigned to each column:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\")\n\nRows: 150 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): Sepal Length, Petal?Length, Species\ndbl (2): Sepal Width, Petal.Width\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis should already ring some alarm bells. First, it looks like the columns are not consistently named: we have Sepal Length and Petal?Length, and also Petal.Width.\n Secondly, we would expect that the measurements are imported as numbers. However, both sepal and petal lengths are imported as characters. We can confirm this by using the class function:class()\n\nclass(iris_data[[\"Sepal Length\"]])\n\n[1] \"character\"\n\nclass(iris_data[[\"Sepal Width\"]])\n\n[1] \"numeric\"\n\nclass(iris_data[[\"Petal?Length\"]])\n\n[1] \"character\"\n\nclass(iris_data[[\"Petal.Width\"]])\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nclass and typeof\n\n\n\nThe class function returns the class of an object, which is a higher-level classification of the object. An object can have multiple classes. The typeof function returns the internal storage type of the object, which is a lower level classification. For example, both tibbles and data frames have the type list, but their classes are different. Another example:i if mtx is a matrix of numbers, typeof(mtx) is double, and class(mtx) is matrix.\n\n\nNote that instead of using iris_data$Sepal Length (which will not work, because there is a space in the column name), we use the double bracket notation. An alternative would be to use quotes: iris_data$'Sepal Length'. This a reason why want to avoid spaces and special characters in column names (in a moment we will show you how to standardize column names). If you use tab-completion with RStudio, the quotes will be inserted automatically.\n We can also use the summary function on the whole dataset:summary()\n\nsummary(iris_data)\n\n Sepal Length        Sepal Width     Petal?Length        Petal.Width   \n Length:150         Min.   : 2.000   Length:150         Min.   :0.100  \n Class :character   1st Qu.: 2.800   Class :character   1st Qu.:0.300  \n Mode  :character   Median : 3.000   Mode  :character   Median :1.300  \n                    Mean   : 3.411                      Mean   :1.199  \n                    3rd Qu.: 3.375                      3rd Qu.:1.800  \n                    Max.   :36.000                      Max.   :2.500  \n   Species         \n Length:150        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nOps, another problem. Did you notice the maximum value for sepal widths? It is 36 cm. This cannot be, way too large for the flowers. Also, both the mean and median are around 3 cm, so this must be either an outlier or a clerical error.\n\n\n\n\n\n\nThe summary() functions\n\n\n\nUnder the hood, there is no single summary() function. Instead, different classes can have different types of summaries. Whenever you produce a result, always try the summary function with it.\n\n\n Alternatively, we can use the tidyverse glimpse function. Rather then providing a summary, this function uses a terse format to show the data type for each column and first few values:glimpse()\n\nglimpse(iris_data)\n\nRows: 150\nColumns: 5\n$ `Sepal Length` &lt;chr&gt; \"5.1\", \"4.9\", \"4.7\", \"4.6\", \"5\", \"5.4\", \"4.6\", \"5\", \"4.‚Ä¶\n$ `Sepal Width`  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, ‚Ä¶\n$ `Petal?Length` &lt;chr&gt; \"1.4\", \"1.4\", \"1.3\", \"1.5\", \"1.4\", \"1.7\", \"1.4\", \"1.5\",‚Ä¶\n$ Petal.Width    &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, ‚Ä¶\n$ Species        &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"seto‚Ä¶\n\n\n Another way how we can diagnose the dataset is to use the str function. This provides a more detailed summary for each column:str()\n\nstr(iris_data)\n\nspc_tbl_ [150 √ó 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Sepal Length: chr [1:150] \"5.1\" \"4.9\" \"4.7\" \"4.6\" ...\n $ Sepal Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal?Length: chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Sepal Length` = col_character(),\n  ..   `Sepal Width` = col_double(),\n  ..   `Petal?Length` = col_character(),\n  ..   Petal.Width = col_double(),\n  ..   Species = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n3.3.2 Using colorDF to diagnose datasets\n I have written a package called colorDF that creates yet another flavor of data frames, which is displayed in the console in color (if you haven‚Äôt installed it yet, do so now with install.packages(\"colorDF\"). However, we will only use one convenient function from this data set, summary_colorDF. Because summary_colorDF is too much to type, we will use a shortcut, scdf.colorDF,summary_colorDF()\n\nscdf &lt;- colorDF::summary_colorDF\nscdf(iris_data)\n\n# Color data frame (class colorDF) 5 x 5:\n ‚îÇCol         ‚îÇClass‚îÇNAs  ‚îÇunique‚îÇSummary                                   \n1‚îÇSepal Length‚îÇ&lt;chr&gt;‚îÇ    0‚îÇ    37‚îÇ5: 10, 5.1: 9, 6.3: 9, 5.7: 8, 6.7: 8, 5.‚Ä¶\n2‚îÇSepal Width ‚îÇ&lt;dbl&gt;‚îÇ    0‚îÇ    25‚îÇ 2.00 [ 2.80 &lt; 3.00&gt;  3.38] 36.00         \n3‚îÇPetal?Length‚îÇ&lt;chr&gt;‚îÇ    0‚îÇ    47‚îÇ1.5: 13, 1.4: 12, 4.5: 8, 1.3: 7, 5.1: 7,‚Ä¶\n4‚îÇPetal.Width ‚îÇ&lt;dbl&gt;‚îÇ    0‚îÇ    22‚îÇ0.1 [0.3 &lt;1.3&gt; 1.8] 2.5                   \n5‚îÇSpecies     ‚îÇ&lt;chr&gt;‚îÇ    0‚îÇ     6‚îÇvirginica: 46, setosa: 45, versicolor: 42‚Ä¶\n\n\n Above we have created a copy of the summary_colorDF function in the variable scdf. This new variable behaves exactly like the original and you can use it just as you would use summary_colorDF. The colorDF::  prefix is there so that we don‚Äôt have to load the colorDF package. Instead we tell R to look for the function directly in the colorDF package.Copying functionspackage::function()\nThis summary function shows more than just the str function. In addition to column types, for character and factor columns it shows the unique values, ordered by their frequency. For numerical values, it shows their non-parametric summary statistics (range, median, quartiles).\n Hm, when you look at the output you might notice one more thing: for the last column, ‚ÄúSpecies‚Äù, summary_colorDF shows that there are six unique values. However, how can that be? We know that there are only three species in this dataset. We can check this using the unique function:unique()\n\nunique(iris_data[[\"Species\"]])\n\n[1] \"setosa\"     \"Setosa\"     \"versicolor\" \"Versicolor\" \"virginica\" \n[6] \"Virginica\" \n\n\nThere it is. Looks like whoever typed the data2, sometimes used a lower-case species designation, and sometimes upper-case. However, for R (and for computers in general), ‚Äúversicolor‚Äù and ‚ÄúVersicolor‚Äù are two different things.\n2¬†In this case, it was me. More information can be gained using the table() function:table()\n\ntable(iris_data[[\"Species\"]])\n\n\n    setosa     Setosa versicolor Versicolor  virginica  Virginica \n        45          5         42          8         46          4 \n\n\nHere we not only see the unique values, but also their frequency. The table() function is very useful for any categorical data, as you will see later.\n\n\n\n\n\n\nLower and upper case\n\n\n\nFor computers in general and R in particular, ‚Äúlowercase‚Äù and ‚ÄúUppercase‚Äù are two different things. Variables a and A are different, as are versicolor and Versicolor. This is called case sensitivity.\n\n\n\n\n3.3.3 Checking individual values\nWe have seen before that some measurement columns were imported as character vectors, while others were correctly imported as numbers. To understand why, we need to have a closer look at the individual columns.\nOne of the columns that has been imported as a character vector is the ‚ÄúSepal Length‚Äù. We will extract the column as a vector and then try to manually convert it to a number:\nas.numeric()\n\nsepal_length &lt;- iris_data[[\"Sepal Length\"]]\nas.numeric(sepal_length)\n\nWarning: NAs introduced by coercion\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0\n [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5\n [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2  NA 5.9 6.1\n [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5\n [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n[109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7  NA 6.0 6.9 5.6 7.7 6.3 6.7 7.2\n[127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\n\nTwo things to note: first, R issued a warning, ‚ÄúNAs introduced by coercion‚Äù. That means that some values could not be converted to numbers, and were replaced by the special value NA3. Second, there was no error. R happily allowed you to create a vector of numbers from a vector even though there were some problems.\n3¬†See Day 1 for more information on NA values.When you look at the output, you will easily spot the NA values. You could even figure out at which positions in the vector they occur. However, this is not the way to go ‚Äì what if you have not 150 values, but 150,000?  Instead, use the is.na function:is.na()\n\n# convert to numbers\nsepal_length_n &lt;- as.numeric(sepal_length)\n\nWarning: NAs introduced by coercion\n\n# how many are NA?\nsum(is.na(sepal_length_n))\n\n[1] 2\n\n# which are NA?\nwhich(is.na(sepal_length_n))\n\n[1]  70 119\n\n# show the values\nsepal_length[is.na(sepal_length_n)]\n\n[1] \"5,6\"   \"&gt; 7.7\"\n\n\nOne more problem remains: the value ‚Äú36‚Äù in the sepal width column. We can check which values are greater than 10 with a similar approach:\n\nsepal_width &lt;- iris_data[[\"Sepal Width\"]]\n\n# how many are greater than 10?\nsum(sepal_width &gt; 10)\n\n[1] 2\n\n# which are greater than 10?\nwhich(sepal_width &gt; 10)\n\n[1]  42 110\n\n# show the values\nsepal_width[sepal_width &gt; 10]\n\n[1] 23 36\n\n\n\n\n\n\n\n\n\nExercise 3.6 (Petal lengths) Repeat the steps above for the Petal length column.\n\n\n\n\n\n\n3.3.4 Diagnosing datasets: a checklist\nWhenever you read a data set, you should check the following things:\nColumn names. Are they consistent? Are they easy to type (no spaces, no special characters)? Are they in the correct order? Are they what you expect them to be? Do you understand what each column is?\nData types. Are the columns of the correct type (e.g. numerical columns imported as numbers)?\nCategorical variables. Do you see what you expect? Is the number of categories correct? Do you understand what each category is?\nNumerical variables. Do the summary statistics make sense? Are there any outliers? Missing values? Were the variables imported incorrectly as characters? Why?\nMissing values. Are there any missing values? Why? Are they important? How to deal with them?\n\n\n\n\n\n\nChecklist for importing data\n\n\n\n\nColumn names\nData types\nCategorical variables\nNumerical variables\nMissing values\n\n\n\n\n\n\n\n\n\n\nExercise 3.7 (Botched metadata) Load the file meta_data_botched.xlsx using the readxl package. Diagnose the problems.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#mending-the-data",
    "href": "day3-reading-your-data.html#mending-the-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.4 Mending the data",
    "text": "3.4 Mending the data\n\n3.4.1 Correcting column names\nColumn names should contain only letters, numbers and underscores. While all other characters are also allowed (so a column name can be Ë±öËÇâ \"temp.\"? (¬∞C) üòÄ), it is way easier to work with columns with simple, standardized names.\nYou already know one way of dealing with this ‚Äì simply assign new names to the columns:\n\ncolnames(iris_data) &lt;- c(\"sepal_length\", \"sepal_width\", \"petal_length\",\n                      \"petal_width\", \"species\")\ncolnames(iris_data)\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\n However, there is a quicker way. The janitor package contains a function called clean_names that does exactly that, automatically and for all  columns. Install it with install.packages(\"janitor\") if you haven‚Äôt installed it yet.janitor packageclean_names()\n\nlibrary(janitor)\niris_data &lt;- clean_names(iris_data)\ncolnames(iris_data)\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\nMy advice is to use clean_names on every dataset you import.\n\n\n3.4.2 Correcting outliers\nIn the column with sepal widths, which now has the name sepal_width, we found two values that probably lack the decimal point: 23 and 36. They are at positions 42 and 110, respectively.\nIt is tempting to simply substitute the data in the original data frame:\n\n# works, but don't do it\niris_data$sepal_width[42] &lt;- 2.3\n\nUnfortunately, this is not a good idea, because this solution is a bit like manual editing in a spreadsheet. There is a better way. First, generalize the issue. What seems to be the problem? The values that are larger than 10 are probably missing a decimal point. The simplest solution is to divide them by 10. Using logical vectors, we can do that for all the values that are larger than 10.\n\n# numbers that are missing a decimal point\ntoo_large &lt;- iris_data$sepal_width &gt; 10\n\niris_data$sepal_width[too_large] &lt;- iris_data$sepal_width[too_large] / 10\n\nThis a bit hard to read, so we make it more explicit:\n\n# numbers that are missing a decimal point\nsepal_width &lt;- iris_data$sepal_width\ntoo_large &lt;- sepal_width &gt; 10\n\nsepal_width[too_large] &lt;- sepal_width[too_large] / 10\nsepal_width[too_large]\n\nnumeric(0)\n\n\nLooks good. Finally, we assign the corrected values back to the data frame:\n\niris_data$sepal_width &lt;- sepal_width\n\nAs the last step, we need to check whether our approach worked for all  values in the column. For this, we will use the any() function, which returns TRUE if any of the values in a logical vector is TRUE:any()\n\nany(iris_data$sepal_width &gt; 10)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\nExercise 3.8 Can you spot what the potential problem with this approach is? Hint: what would happen if the original value was 1.10? Can you think of a better approach? Is it actually possible to make sure that our guess is correct?\n\n\n\n\n\n\n3.4.3 Correcting categorical data\nIn the column ‚Äúspecies‚Äù, we found that there are two different values for the same species. There are many ways we can handle it. Obviously, we could manually assign the correct labels, but that would be a lot of work.\nA better way would be to somehow automatically convert all the values. In our case, the problem is quite simple: it is sufficient to put all the values in uniform lower case, so that Versicolor becomes versicolor.\n We can do that in one line of code thanks to the tolower function:tolower()\n\niris_data$species &lt;- tolower(iris_data$species)\n\n There is also a corresponding function, toupper, that converts all the letters to upper case.toupper()\nRemember to check that your changes were successful:\n\ntable(iris_data$species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nThis was a simple case, but what if we had more complex typos? For example, ‚Äúi.versicolor‚Äù, ‚Äúvers.‚Äù, ‚ÄúV‚Äù etc. In such cases, you would need to use a more sophisticated approach. This approach involves regular expressions, and will also help us to correct the sepal and petal lengths.\n\n\n3.4.4 Incorrectly imported numeric data\nWe have two columns which were imported as characters, but should be numbers. Their brand-new names are sepal_length and petal_length. All in all, there are 2 problems in the sepal length column and 5 in the petal length column, as shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\nColumn\nPosition\nOriginal\nCorrect\n\n\n\n\nsepal_length\n70\n5,6\n5.6\n\n\nsepal_length\n119\n&gt; 7.7\n7.7\n\n\npetal_length\n12\n1.6!\n1.6\n\n\npetal_length\n13\n1 .4\n1.4\n\n\npetal_length\n14\n1.1?\n1.1\n\n\npetal_length\n115\n5k.1\n5.1\n\n\npetal_length\n149\n5. 4\n5.4\n\n\n\n\n\nThe last column in the table above is our target ‚Äì this is how we would like to correct the data. How can we do that?\n\n\n\n\n\n\nCorrecting values\n\n\n\nOf course, without additional information we can only guess that 5,6 should be 5.6. Maybe there were two measurements, 5 and 6? Maybe 5. 4 should be 5.04, and not 5.4? In a real-world scenario, you would need to consult the author(s) of the data, or check your lab book.\n\n\nIt is tempting to simply substitute the data in the original data frame:\n\n# works, but don't do it\niris_data$sepal_length[70] &lt;- 5.6\n\nAs before, that is not a good idea. If you receive another version of the file, with added lines before the 70th, the position will change and you will have to correct the version manually again. Also, if there are new problems, like yet another typo in the data, you will have to spot it and manually add a line to correct it. Again, we need to generalize the issue.\nWhat is the problem on line 70? A comma instead of a decimal dot ‚Äì maybe someone who typed it was used to entering numbers in a German version of Excel, where a comma is used instead of a dot. So why not replace all the commas by a dot? This should do the trick not only for line 70, but also for possible future problems.\n We will use for that the function str_replace_all from tidyverse4. Before we do that, however, we will make a copy of the column. That will allow us to easily see the changes will make, check that everything is OK and only then replace the column in the data frame. Later on, you will learn more efficient ways of doing this.str_replace_all()4¬†In base R, you can use the gsub function. However, it has a slightly different syntax, which makes it less convenient when you start using pipes tomorrow.\n\n# make a copy\nsepal_length &lt;- iris_data$sepal_length\n\n# record the problematic places\nproblems &lt;- is.na(as.numeric(sepal_length))\n\nWarning: NAs introduced by coercion\n\nsepal_length[problems]\n\n[1] \"5,6\"   \"&gt; 7.7\"\n\n# replace the comma \nsepal_length &lt;- str_replace_all(sepal_length, \",\", \".\")\n\n# what was the result?\nsepal_length[problems]\n\n[1] \"5.6\"   \"&gt; 7.7\"\n\n\nIt worked! The str_replace_all is a general search-and-replace function. You can always use it with character vectors to replace stuff.\nHowever, we still have the problem with &gt; 7.7. You will find such values quite often in clinical settings, along with the counterpart &lt; 7.7. These may indicate that the measurement was out of range of an instrument. However, we cannot usually treat them as missing values, because they contain some information. Depending on whether or not we want to keep this information, we could either replace them by NAs, or decide to keep the value as is (in this case, change &gt; 7.7 to 7.7).\nIn the latter case, we can use the str_replace_all function again:\n\nsepal_length &lt;- str_replace_all(sepal_length, \"&gt; \", \"\")\n\nIn any case, the last point that remains is to convert the vector to numbers and assign it to the column holding sepal lengths. We do it in one go and also check if everything is OK:\n\n# finalize\niris_data$sepal_length &lt;- as.numeric(sepal_length)\n\n# check whether the column is numeric\nis.numeric(iris_data$sepal_length)\n\n[1] TRUE\n\n# check whether our problems are gone\niris_data$sepal_length[problems]\n\n[1] 5.6 7.7\n\n# check whether there are any NA's\nany(is.na(iris_data$sepal_length))\n\n[1] FALSE\n\n\n The new function, is.numeric(), checks whether the column sepal_length is indeed numeric. Finally, we make sure that no NA‚Äôs were produced in the conversion.as.numeric()\n\n\n\n\n\n\n\nExercise 3.9 Find the problems in the following vector and correct them:\nvec &lt;- c(\" 5\", \"5,6\", \"5.7\", \"5. 4\", \"&gt; 5.0\", \"6.O\")",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#regular-expressions",
    "href": "day3-reading-your-data.html#regular-expressions",
    "title": "3¬† Reading and Writing Files",
    "section": "3.5 Regular expressions",
    "text": "3.5 Regular expressions\n\n3.5.1 Introduction to regular expressions\n Regular expressions, regexps for short, are for finding certain patterns in the data and handling them automatically. They are quite old (going back at least to 1960‚Äôs) and have remained virtually the same for many decades. You will find them in almost all programming languages, and while details may differ, the principles are usually the same. The regexps in R are very similar to regular expressions in other programming languages such as Python. As you may guess, they have a different syntax then R ‚Äì they are something else altogether.Regular expressions\nWe will not go in depth in regular expressions in R here. Here, just a short primer.\n Before we start, let us introduce a new function called str_detect()5. The function is available as soon as you load the tidyverse package. This function is used to search for a pattern in a character vector. For every element that matches the pattern, it will return a TRUE, otherwise a FALSE. Say, we have a vector (or data frame column) containing sample names, and we want to find all the controls.str_detect()5¬†In base R, there is the grep() function. It is very similar, but has a different syntax. We will stick to str_detect() for now.\n\nsamples &lt;- c(\"ko_1_ctrl\", \"ko_2_ctrl\", \"ko_1_treat\", \"ko_2_treat\",\n            \"wt_1_ctrl\", \"wt_2_ctrl\", \"wt_1_treat\", \"wt_2_treat\")\nstr_detect(samples, \"ctrl\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\nsamples[str_detect(samples, \"ctrl\")]\n\n[1] \"ko_1_ctrl\" \"ko_2_ctrl\" \"wt_1_ctrl\" \"wt_2_ctrl\"\n\n\nIn case that someone did not pay attention to lower or upper case, we can tell str_detect to ignore the case:\n\nsamples &lt;- c(\"ko_1_ctrl\", \"ko_2_CTRL\", \"ko_1_treat\", \"ko_2_treat\",\n            \"wt_1_CTRL\", \"wt_2_ctrl\", \"wt_1_treat\", \"wt_2_treat\")\n# this does not work\nstr_detect(samples, \"ctrl\")\n\n[1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n# but this does\nstr_detect(samples, regex(\"ctrl\", ignore_case=TRUE))\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\nHere we are looking for a literal string ‚Äúctrl‚Äù. However, we can use regular expressions to search for more complex patterns. What if the sample names are more hap-hazard, like ko_1_control, ko_2_kontrol, ko_1_ctrl?\n\nsamples &lt;- c(\"ko_1_control\", \"ko_2_ctrl\", \"ko_1_trt\", \"ko_2_treatment\",\n            \"wt_1_Control\", \"wt_2_kontrol\", \"wt_1_Trtmt\", \"wt_2_treated\")\n\nOK, so we got control, ctrl, Control and kontrol. A pattern emerges:\n\nfirst letter is either k or c or C\nthen we might have an on (or not)\nnext always comes tr\nthen we might have an o\nthen we always have an l\nthen the word ends.\n\nAll this ‚Äúmights‚Äù and ‚Äúors‚Äù and ‚Äúeithers‚Äù and ‚Äúalways‚Äù can be encoded in a regular expression:\n\n\n\nPattern\nExplanation\n\n\n\n\n[kcC]\nmust have one of the letters k, c or C\n\n\no?\nzero or one o (i.e., ‚Äúthere might be an o‚Äù)\n\n\nn?\nzero or one n (i.e., ‚Äúthere might be an n‚Äù)\n\n\ntr\nmust have literally the string tr\n\n\no?\nzero or one o (i.e., ‚Äúthere might be an o‚Äù)\n\n\nl\nmust have literally the string l\n\n\n$\nhere must be the end of the string\n\n\n\nTaken together, this gives us the regular expression [kcC]o?n?tro?l$. This looks weird, but has the magic power of matching all the control-like strings in our vector. Let us try it:\n\nstr_detect(samples, \"[kcC]o?n?tro?l$\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\nsamples[str_detect(samples, \"[kcC]o?n?tro?l$\")]\n\n[1] \"ko_1_control\" \"ko_2_ctrl\"    \"wt_1_Control\" \"wt_2_kontrol\"\n\n\nThis works, but‚Ä¶ what does that all mean?\n\n\n\n\n\n\n\nExercise 3.10 (Quick question) Look at the table above. Can you think of other patterns that would match this regular expression? Would kotrl work? What about cotro?\n\n\n\n\n\n\n3.5.2 How regexp works\n A regular expression is a sequence of characters ‚Äì a string. However, some of these strings have a special meaning. Here is a short table, we will discuss these in more detail in a moment:regexp rules\n\n\n\nCharacter\nMeaning\n\n\n\n\na\na literal character a\n\n\n.\nany character\n\n\n[abc]\none of the characters a, b or c\n\n\n[^abc]\nany character except a, b or c\n\n\n[a-z]\nany character from a to z\n\n\n[0-9]\nany digit\n\n\n\nThese are the so-called ‚Äúatoms‚Äù. They stand for something that we want to match. In the simplest case (like the a above), they stand for themselves. Also, did you notice the square brackets? They have nothing to do with how we use square brackets in R.\nIn addition we can specify how many times we want to match the atom:\n\n\n\nQuantifier\nMeaning\n\n\n\n\n?\nzero or one\n\n\n*\nzero or more\n\n\n+\nat least one\n\n\n\nThen, we can ‚Äúanchor‚Äù the pattern in our string:\n\n\n\nAnchor\nMeaning\n\n\n\n\n^\nstart of the string\n\n\n$\nend of the string\n\n\n\nLet us put that together. Say we have the following strings:\n\nstrings &lt;- c(\"a\", \"ab\", \"ac\", \"ad\", \"bc\", \"abc\", \"bac\", \"cab\")\n\nTo match these which contain an a, we can use the regular expression a:\n\nstrings[str_detect(strings, \"a\")]\n\n[1] \"a\"   \"ab\"  \"ac\"  \"ad\"  \"abc\" \"bac\" \"cab\"\n\n\nTo match these which start with an a, we can use the regular expression ^a:\n\nstrings[str_detect(strings, \"^a\")]\n\n[1] \"a\"   \"ab\"  \"ac\"  \"ad\"  \"abc\"\n\n\nTo find these which start with an a followed by b or c, but not d, we can use the square brackets:\n\nstrings[str_detect(strings, \"^a[bc]\")]\n\n[1] \"ab\"  \"ac\"  \"abc\"\n\n\nOK, that was a lot. Take some time to digest it. Regulare expressions are what you call in German gew√∂hnungsbed√ºrftig ‚Äì they require some getting used to, but they are worth it.\n\n\n3.5.3 Usage of regular expressions in R\nMost commonly you will use regular expressions in R for two things:\n\nSearching for something in a character vector (like we did above)\nReplacing something in a character vector, for example using str_replace_all\n\nWhat we did not tell you when we introduced str_replace_all is that it actually, its second argument is a regular expression. Therefore, we can use it to find a pattern and replace it with something else. For example, we can unify the following messed up vector denoting the gender of patients:\n\ngender &lt;- c(\"m\", \"f\", \"m\", \"w\", \"frau\",\n            \"female\", \"other\", \"male\", \n            \"m√§nnlich\", \"x\", \"weiblich\")\n\nGerman and English are mixed here, but we can see a pattern: if the strings starts with m, it is male, if it starts with either f or w, it is female. And in remaining cases it is ‚Äúother‚Äù. We can clean up this mess with just three lines of code:\n\ngender &lt;- str_replace(gender, \"^m.*\", \"male\")\ngender &lt;- str_replace(gender, \"^[fw].*\", \"female\")\ngender &lt;- str_replace(gender, \"^[^mfw].*\", \"other\")\n\nNote that the ^ character has two different meanings on line 3 above. As the first character of a regular expression, it anchors it to the beginning of the string. However, inside the square brackets, it negates the selection. So, [^mfw] means ‚Äúany character except m, f or w‚Äù.\nThe .* idiom is a common one in regular expressions. It means ‚Äúzero or more of any character‚Äù. So, ^m.* matches both the string ‚Äúm‚Äù and the string ‚Äúmale‚Äù. And because we used ^, it will only match the strings where m is the first letter (so it does not match ‚Äúfemale‚Äù).\n\n\n\n\n\n\n\nExercise 3.11 (Quick question) What would happen if we ommited the ^ at the beginning of the strings above? For example, if we used [^mfw].* instead of ^[^mfw].*? Think first, then try it out.\n\n\n\n\nBut wait, if some characters have a special meaning, how can we replace them? For example, how can we replace a dot in a string? The following will not work as intended:\n\nvec &lt;- c(\"5.6\", \"5.7\", \"5.8\")\nstr_replace_all(vec, \".\", \",\")\n\n[1] \",,,\" \",,,\" \",,,\"\n\n\nSince the character . means ‚Äúany character‚Äù, every character will be replaced by a comma in the example above. In order to search or replace  special characters, we must escape them ‚Äì which in R means putting two backslashes6 in front of them in the regular expression.escaping characters in a regexp6¬†In R, it is two backslashes. In other programming languages, it is usually a single backslash.\n\nvec &lt;- c(\"5.6\", \"5.7\", \"5.8\")\nstr_replace_all(vec, \"\\\\.\", \",\")\n\n[1] \"5,6\" \"5,7\" \"5,8\"\n\n\n\n\n\n\n\n\n\nExercise 3.12 ¬†\n\nUse str_replace_all() to make the following uniform: c(\"male\", \"Male \", \"M\", \"F\", \"female\", \" Female\")\nUsing str_replace_all() and toupper(), clean up the gene names such that they conform to the HGNC (all capital letters, no spaces, no dashes): c(\"ankrd22\", \"ifng\", \"Nf-kb\", \" Cxcl 5\", \"CCL 6.\", \"ANK.r.d. 12\")\nWhat regular expression matches all of the ankyrin repeat genes (but not other genes) in the following vector: c(\"ANKRD22\", \"ank.rep.d. 12\", \"ANKRD-33\", \"ankrd23\", \"ANKEN\", \"MAPK\", \"ifng-1\", \"ANKA-REP-6\")? Ankyrin repeat domain genes are the first 4 in the vector.\n\n\n\n\n\n\n\n3.5.4 Correcting columns in iris_data with regular expressions\nLet us now turn to another column in the data frame, the petal lengths. Using the approach we have just learned, we can find the problematic values:\n\npetal_length &lt;- iris_data$petal_length\nproblems &lt;- is.na(as.numeric(petal_length))\n\nWarning: NAs introduced by coercion\n\nwhich(problems)\n\n[1]  12  13  14 115 149\n\npetal_length[problems]\n\n[1] \"1.6!\" \"1 .4\" \"1.1?\" \"5k.1\" \"5. 4\"\n\n\nWe could use for example str_replace_all(sepal_length, \"k\", \"\"), to change 5k.1 to 5.1, but that would not be a general solution. What if it is j in the new data? We should actually remove everything that is not a number and not a decimal dot. To this end, we will use the regular expressions.\nWe need to remove anything but numbers and decimal dots. We can use the square brackets for that:\n\npetal_length &lt;- str_replace_all(petal_length, \"[^0-9.]\", \"\")\n\nThe 0-9 means anything between 0 and 9, and the . is a literal dot. As you can see, you don‚Äôt have to escape the dot if it is already in the  square brackets. The ^ inside the square brackets negates the selection, so [^0-9.] means exactly what we wanted.[^0-9]\n\n# check the problems\npetal_length[problems]\n\n[1] \"1.6\" \"1.4\" \"1.1\" \"5.1\" \"5.4\"\n\n# convert to numbers\npetal_length &lt;- as.numeric(petal_length)\n\n# check for remaining NA's\nany(is.na(petal_length))\n\n[1] FALSE\n\n# assign\niris_data$petal_length &lt;- as.numeric(petal_length)\n\nDone! The iris data set is now clean.\n\nscdf(iris_data)\n\n# Color data frame (class colorDF) 5 x 5:\n ‚îÇCol         ‚îÇClass‚îÇNAs  ‚îÇunique‚îÇSummary                                  \n1‚îÇsepal_length‚îÇ&lt;dbl&gt;‚îÇ    0‚îÇ    35‚îÇ4.3 [5.1 &lt;5.8&gt; 6.4] 7.9                  \n2‚îÇsepal_width ‚îÇ&lt;dbl&gt;‚îÇ    0‚îÇ    23‚îÇ2.0 [2.8 &lt;3.0&gt; 3.3] 4.4                  \n3‚îÇpetal_length‚îÇ&lt;dbl&gt;‚îÇ    0‚îÇ    43‚îÇ1.00 [1.60 &lt;4.35&gt; 5.10] 6.90             \n4‚îÇpetal_width ‚îÇ&lt;dbl&gt;‚îÇ    0‚îÇ    22‚îÇ0.1 [0.3 &lt;1.3&gt; 1.8] 2.5                  \n5‚îÇspecies     ‚îÇ&lt;chr&gt;‚îÇ    0‚îÇ     3‚îÇsetosa: 50, versicolor: 50, virginica: 50\n\n\n\n\n\n\n\n\n\nExercise 3.13 (Correcting metadata) In Exercise¬†3.7, you have diagnosed the file meta_data_botched.xlsx. Now go ahead and correct the problems you have found.\n\n\n\n\n\n\n3.5.5 Just a quick look\nThe newly cleaned up data set begs for a quick look. We already created a simple plot before using the plot() function, and today I will show you how to use the ggplot2 package to create somewhat nicer plots. In this case, it is rather trivial. ggplot2\n\nlibrary(ggplot2)\nggplot(iris_data, aes(x=sepal_length, y=sepal_width, color=species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nAs you can see, the ggplot function takes two arguments: the data frame and something which is calles aesthetics and which is produced by the aes() function. The aes() function serves as a kind of link between the data and the plot, showing how what should be plotted. In this case, we tell aes() that the x-axis should be the sepal length, the y-axis the sepal width, and the color should be determined by the species.\nThen, a magical thing happens. Whatever is produced by ggplot() is not a number, but nonetheless can be added with a + sign to other stuff. In this case, we add a geom_point() function, which tells ggplot to plot points at the x, y coordinates (there are many others, and you will learn about them the day after tomorrow).\nThere, all done. See how I. setosa is different from the other two?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#writing-data",
    "href": "day3-reading-your-data.html#writing-data",
    "title": "3¬† Reading and Writing Files",
    "section": "3.6 Writing data",
    "text": "3.6 Writing data\n\n3.6.1 Keep your data organized\nWriting data is way easier than reading it. However, you always have to come up with a name for the file, and I would like you to follow a few points.\n\nNever overwrite the original file (we will discuss it in more detail shortly).\nCreate an output directory and save your files only there, do not mix the original and the output files.\nUse a meaningful name, so not output, but iris_cleaned. No spaces or special characters (use an underscore _ instead).\nThirdly, always version your files: add a date, your initials, a version number and a date, e.g.¬†iris_cleaned_2021-09-01_JW_v1.csv.\nDo not use the old Excel XLS format (file extension .xls). Use the newer XLSX format (file extension .xlsx).\n\nThis latter point warrants an explanation. There are two main problems with that format. Firstly, it is less portable then XLSX (the new one) ‚Äì so many programs can read XLSX, but not XLS (or, which is worse, they can, but read it incorrectly). Secondly, the number of rows and columns in an XLS file is severly limited (65536 rows and 256 columns). These limits are easily reached in modern bioinformatic datasets.\n\n\n3.6.2 Functions to use for writing data\nwrite_tsv(), write_csv(),write_xlsx()\n\n\n\n\n\n\n\n\n\nData type\nFunction\nPackage\nNotes\n\n\n\n\nTSV / TAB separated values\nwrite_tsv()\nreadr\nNo rownames!\n\n\nCSV / comma separated\nwrite_csv()\nreadr\nNo rownames!\n\n\nXLS (old Excel)\n\n\nJust don‚Äôt use it. No, seriously, don‚Äôt.\n\n\nXLSX (new Excel)\nwrite_xlsx()\nwritexl\nNo rownames!\n\n\n\nJust as in the case of reading data, there are several functions to write data. There are also functions in the base R that can be used for writing (e.g.¬†write.csv), but they are not recommended. We advise you to only use the readr package for writing TSV and CSV files, and the writexl package for writing XLSX files.\nHowever, keep in mind that row names are not exported with these packages. That is why we do not use row names in this course.\n\n\n\n\n\n\n\nExercise 3.14 (Writing data) ¬†\n\nIn your project directory, create the directory ‚ÄúData_clean‚Äù (if you want, you can use the R function dir.create() for that).\nWrite the cleaned iris_data to a new file in the ‚ÄúData_clean‚Äù directory. Use the write_csv function from the readr package. Use a meaningful name, version it and use a date.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#principles-of-data-management",
    "href": "day3-reading-your-data.html#principles-of-data-management",
    "title": "3¬† Reading and Writing Files",
    "section": "3.7 Principles of data management",
    "text": "3.7 Principles of data management\n\n3.7.1 Keeping originals\nAs every day, we end the day with some food for thought. Today, we would like to introduce a few important principles that you should follow in your everyday work with data ‚Äì not only in R, but in general.\nThe first rule is: always keep the originals. Whether you have received the data from someone, extracted them from a database such as RedCap or created it yourself, once you start working with R you should freeze your data, version it and not touch it anymore.\nLet me reiterate: do not touch the original data. If you absolutely have to edit it by hand, make a copy and add a suffix like _edited or your initials to the file. However, it is better to perform all editing operations in R.\nVersioning. Always include a date, and possibly your initials and version in the file name. If you ever submitted a manuscript of a scientific paper, you know how quickly versions can accumulate. It is absolutely crucial to know which version of the data was used for which analysis.\nData editing. Every edit operation on your data should be somehow recorded. This is not different from a lab notebook, where you write down every step of your experiment. When working with a spreadsheet this is hard to do, therefore‚Ä¶\n\n\n3.7.2 Use R for data editing\nOptimally, your mode of operation should never involve editing data directly in a spreadsheet. You should read your data in R and perform all operations here, recording them in a script. This has two huge advantages:\n\nReproducibility. You and others will be able to trace back all your operations. There will be no doubt whether and which data were edited and how.\nAutomation. When you get updated data (and you will), for example when new samples arrived, you will be able to re-run your script (maybe with a few changes) and get the updated results with minimal effort. Otherwise, you will have to repeat every. Single. Edit. Again.\n\nConsider the example of the iris.csv file. We have seen that one of the values of sepal width seems to be missing a decimal dot. You could edit that in the Excel or CSV file directly. You might even make a note of that somewhere. However, chances are that you were wrong ‚Äì maybe that was a flower wiht a 36 sepal width after all. This change influences your data and your analysis results and must be documented if you were to do science. If you did it in R, you probably have entered a comment in your script, something like this:\n\n# There seems to be a missing decimal dot in some of the sepal widths\n# Changing it to 1/10th of the value\n\nsepal_width &lt;- iris_data[[\"Sepal Width\"]]\nsel &lt;- which(sepal_width &gt; 10)\nsepal_width[sel] &lt;- sepal_width[sel] / 10\n\niris_data[[\"Sepal Width\"]] &lt;- sepal_width\n\nThis concisely informs the reader that a change was made, how and why.\nAnd what happens when Ronald Fisher raises from his grave and collects  another 150 flowers? You will get an updated data set with 300 flowers and import it again. In the best case scenario, you will find the problem again (or remember it). Then you will have to go into Excel and make the same edit again. And all the other edits as well. If you did it in R, you simply use your script again.undead Ronald Fisher\n\n\n3.7.3 Working with Excel and other spreadsheets\n We all use Excel, LibreOffice Calc or Google Sheets. They are great tools, no doubt about that. However, by now you can see that certain operations make working with R harder, and some even that can mess up your data.How to work with Excel\nIf possible, you should actually avoid working with Excel when working with scientific data. Excel does certain operations automatically (depending on the settings), so it can change your data without leaving a trace of the changes. This is the precise opposite of what you want in science.\n\n\n\n\n\n\nAvoid working with Excel\n\n\n\nExcel can automatically replace strings with dates, like changing MARCH9 into 9th of March. However, MARCH9 is a valid gene name. In fact, it turns out that a substantial proportion (about a third!) of all Excel files containing gene names and published as supplementary materials online contain gene names transformed to dates. Not only that, but even though that this has been discovered many years ago and even some genes were officialy renamed because of Excel, this is still a problem. And Excel is now able to recognize dates in many languages, exacerbating the problem (Abeysooriya et al. 2021).\n\n\nHowever, sometimes it is unavoidable to work with Excel.\nDon‚Äôt‚Äôs:\n\nUsing color and font to encode information, for example marking the different treatments with different background color or formatting controls in bold. While it is possible to read this information in R, it is not trivial. In addition, even outside of R formatting of cells can be lost when the data is passed around, and this would mean that the information is lost.\nComments in the same cells as values, for example 50 (measured twice). This prevents the columns to be interpreted as numbers and has to be dealt with in R. Create a separate column for comments, even it is just a single comment in a row.\nMeta-data information in the header. Very often, column names contain additional information, for example units, dates, encoding (like ‚Äú0 for male, 1 for female‚Äù). This makes it hard to both, import the data and actually use that information. Create a separate sheet in your spreadsheet for meta data, with one row per column name.\nAdding header and tail information. Just like in the example with deaths.xlsx file, additional lines in spreadsheets require you to carefully select what you wish to import. Avoid it if you can, use another sheet.\nMore than one table on a spreadsheet. A spreadsheet should contain a single table. Otherwise, just like in case of header and tail information, you will have to spend some time cautiously selecting the areas that you want to import.\nMerging cells. Merged cells are a nightmare to work with in R. If you have to, unmerge them before importing the data.\n\nDo‚Äôs:\n\nUse a single table per sheet. If you have more than one table, use more than one sheet.\nUse a single header row. If you have more than one header row, use create another sheet with meta-data on the column names.\nSwitch off automatic conversion. Excel can automatically change your data in many ways. For example, it can change gene names to dates. It can also change the decimal separator, which can mess up your data. Newer versions of Excel allow to switch off this behaviour, so do it (here are the instructions).\nControl your import. When importing data from CSV files to TSV files, Excel allows you to control which fields are imported as text, which as dates etc. Use this feature to make sure that your data is correctly imported.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#review",
    "href": "day3-reading-your-data.html#review",
    "title": "3¬† Reading and Writing Files",
    "section": "3.8 Review",
    "text": "3.8 Review\nThings that you learned today:\n\nReading and writing\n\nread_csv, read_tsv, read_delim, read_xls, read_xlsx\nreading a file from working directory\nreading a file from a different directory using relative paths\n\nPaths\n\nGet the current working directory with getwd()\nrelative and absolute paths\n\nDiagnosing datasets\n\ndata types: str, class, typeof\nsummaries: summary, glimpse\ncategorical data: unique, table\nsummary_colorDF\ndiagnosis checklist\n\nCorrecting datasets\n\nconverting character vectors to numbers with as.numeric()\nchecking a vector with is.numeric()\nchecking a logical vector with any()\nreplacing values with str_replace_all()\n\nPrinciples of data management\n\nkeeping originals\nversioning\ndata editing\nworking with Excel\n\nOther\n\nusing help pages to find out about functions and their parameters\naccessing functions in a package without loading it (colorDF::summary_colorDF)\n\n\n\n\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann. 2021. ‚ÄúGene Name Errors: Lessons Not Learned.‚Äù PLoS Computational Biology 17 (7): e1008984. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "appendix-wide-long.html",
    "href": "appendix-wide-long.html",
    "title": "Appendix: Wide vs long data",
    "section": "",
    "text": "Wide and Long format\nWide advantages:",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#wide-and-long-format",
    "href": "appendix-wide-long.html#wide-and-long-format",
    "title": "Appendix: Wide vs long data",
    "section": "",
    "text": "Long advantages:Wide and long format\n\neasier to filter, process, visualize, do statistics with\nfocused on measurement (‚Äúpatient ID‚Äù or equivalent is a covariate, and so is measurement type)\n\n\n\ngroups data by a covariate (‚Äúpatient ID‚Äù)\ncan be easier to manage (each column one measurement type)",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-wide-to-long",
    "href": "appendix-wide-long.html#converting-from-wide-to-long",
    "title": "Appendix: Wide vs long data",
    "section": "Converting from wide to long:",
    "text": "Converting from wide to long:\nFirst, let‚Äôs create a wide data set. We use a neat trick here that you have not seen before: you can use the base R function read.table to read data directly from a character string, rather than a file!\nread.table(text=...)\n\nwide &lt;- read.table(header=TRUE, text='\n subject sex control cond1 cond2\n       1   M     7.9  12.3  10.7\n       2   F     6.3  10.6  11.1\n       3   F     9.5  13.1  13.8\n       4   M    11.5  13.4  12.9\n')\nwide\n\n  subject sex control cond1 cond2\n1       1   M     7.9  12.3  10.7\n2       2   F     6.3  10.6  11.1\n3       3   F     9.5  13.1  13.8\n4       4   M    11.5  13.4  12.9\n\n\nThis is clearly a wide format ‚Äì each row corresponds not to one observation (one measurement), but one subject. We want to convert this to long format, where each row corresponds to one observation (and therefore, for each subject, there are three rows).\npivot_longer()\n\nlibrary(tidyverse)\n\npivot_longer(wide, cols=control:cond2,\n  names_to=\"condition\", values_to=\"measurement\")\n\n# A tibble: 12 √ó 4\n   subject sex   condition measurement\n     &lt;int&gt; &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1       1 M     control           7.9\n 2       1 M     cond1            12.3\n 3       1 M     cond2            10.7\n 4       2 F     control           6.3\n 5       2 F     cond1            10.6\n 6       2 F     cond2            11.1\n 7       3 F     control           9.5\n 8       3 F     cond1            13.1\n 9       3 F     cond2            13.8\n10       4 M     control          11.5\n11       4 M     cond1            13.4\n12       4 M     cond2            12.9\n\n\nNote that we must put quotes around condition and measurement in the code above. These are column names, but the columns don‚Äôt exist yet.",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-long-to-wide",
    "href": "appendix-wide-long.html#converting-from-long-to-wide",
    "title": "Appendix: Wide vs long data",
    "section": "Converting from long to wide",
    "text": "Converting from long to wide\nHere is another example data set, this time in long format:\n\nlong &lt;- read.table(header=TRUE, text='\n subject  sampleID sex condition measurement\n       1  ID000001 M   control         7.9\n       1  ID000002 M     cond1        12.3\n       1  ID000003 M     cond2        10.7\n       2  ID000004 F   control         6.3\n       2  ID000005 F     cond1        10.6\n       2  ID000006 F     cond2        11.1\n       3  ID000007 F   control         9.5\n       3  ID000008 F     cond1        13.1\n       3  ID000009 F     cond2        13.8\n')\nlong\n\n  subject sampleID sex condition measurement\n1       1 ID000001   M   control         7.9\n2       1 ID000002   M     cond1        12.3\n3       1 ID000003   M     cond2        10.7\n4       2 ID000004   F   control         6.3\n5       2 ID000005   F     cond1        10.6\n6       2 ID000006   F     cond2        11.1\n7       3 ID000007   F   control         9.5\n8       3 ID000008   F     cond1        13.1\n9       3 ID000009   F     cond2        13.8\n\n\nAs you see, there are three subject, each with three measurements (one control, one for condition 1, and one for condition 2). We want to convert it to wide format, so we expect one row per subject, in total three rows.\nHowever, watch out. The first thing we might want to try does not give the expected result:\npivot_wider()\n\n## not what we wanted!!! Why?\npivot_wider(long, names_from=condition, values_from=measurement)\n\n# A tibble: 9 √ó 6\n  subject sampleID sex   control cond1 cond2\n    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 ID000001 M         7.9  NA    NA  \n2       1 ID000002 M        NA    12.3  NA  \n3       1 ID000003 M        NA    NA    10.7\n4       2 ID000004 F         6.3  NA    NA  \n5       2 ID000005 F        NA    10.6  NA  \n6       2 ID000006 F        NA    NA    11.1\n7       3 ID000007 F         9.5  NA    NA  \n8       3 ID000008 F        NA    13.1  NA  \n9       3 ID000009 F        NA    NA    13.8\n\n\nThe problem is in the sampleID column. Given that names of the variables are in the condition column, and measurement is in the measurement column, R considers all the remaining columns to be the identifier columns. But the column sampleID contains only unique values, so they must be put in separate rows, as above.\nTo fix this, we need to tell R not to use sampleID as an identifier column; instead, only the subject column should be used as an identifier. We can also throw in the sex column if we want to keep it, since it has only 1 value per subject.\npivot_wider(id_cols=...)\n\n## Instead: \npivot_wider(long, id_cols=c(subject, sex),\n                  names_from=condition, values_from=measurement)\n\n# A tibble: 3 √ó 5\n  subject sex   control cond1 cond2\n    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 M         7.9  12.3  10.7\n2       2 F         6.3  10.6  11.1\n3       3 F         9.5  13.1  13.8\n\n\n\n\n\n\n\n\n\nExercise 1 (Converting to long format) Convert the following files to long format:\n\nlabresults_wide.csv\nThe built-in iris data set (data(iris))\ncars.xlsx (tricky! hint: how do you tell which value in the long format belongs to which row in the wide format?)\n\nClean up and convert to long format (what seems to be the problem? How do we deal with that?):\n\nmtcars_wide.csv",
    "crumbs": [
      "Introduction",
      "Appendix: Wide vs long data"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html",
    "href": "day4-manipulating-your-data.html",
    "title": "4¬† Manipulating data frames",
    "section": "",
    "text": "4.1 Aims for today",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#aims-for-today",
    "href": "day4-manipulating-your-data.html#aims-for-today",
    "title": "4¬† Manipulating data frames",
    "section": "",
    "text": "Searching, sorting and selecting\nMatching and merging data\nPipes - writing readable code\nWide and long format",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#selecting-columns",
    "href": "day4-manipulating-your-data.html#selecting-columns",
    "title": "4¬† Manipulating data frames",
    "section": "4.2 Selecting columns",
    "text": "4.2 Selecting columns\n\n4.2.1 Selecting columns using square brackets\nAs you know, if we want the actual column as vector, we use the $ operator:\n\ndf &lt;- data.frame(ara=1:5, bera=6:10, cora=11:15, dora=16:20)\ndf$ara # same as df[[\"ara\"]]\n\n[1] 1 2 3 4 5\n\n\nJust a reminder: you don‚Äôt want to try to select a single column using square brackets (df[, 1]), because the behavior is different for data frames and tibbles. Better use df$a or df[[\"a\"]].\nWe will now discuss selecting and searching for more than one column. Quite often, you want to select only some columns from a data set, or maybe you want to change their order. As you learned on Day 2, you can select columns from a data frame using vectors. This is very similar to how you select elements from a matrix, or even a vector. You can use integers, negative integers, or column names.\n\n# select columns 1 to 2\ndf2 &lt;- df[ , 1:2]\n\n# select anything but column 2\ndf2 &lt;- df[ , -2]\n\n# select all columns in reverse order\ndf2 &lt;- df[ , ncol(df):1]\n\n# select columns ara and cora\ndf2 &lt;- df[ , c(\"ara\", \"cora\")]\n\n# select all columns ara and cora, but in reverse order\ndf2 &lt;- df[ , c(\"cora\", \"ara\")]\n\nThis is very similar to what we did when dealing with matrices, and actually similar to how we select elements from a vector. Note, however, that rather than using 4:1 in the line 8 above, we use ncol(df):1. This ensures that if data frame grows for some reason, we still get all the columns.\n\n\n4.2.2 Selecting columns using tidyverse\n Tidyverse has the select() function, which is a bit more explicit and readable. Most importantly, it also has extra features that make it easier to work with.select()\n\nlibrary(tidyverse)\n# select columns ara and cora\ndf2 &lt;- select(df, ara, cora)\n\n# select columns ara to cora\ndf2 &lt;- select(df, ara:cora)\n\n# select anything but column bera\ndf2 &lt;- select(df, -bera)\n\nCan you spot what is weird about the code above? Exactly! There are no quotes around the column names. One would expect that R should throw an error ‚Äì after all, there is no variable ‚Äúara‚Äù defined yet. However, this is an extra feature of tidyverse. It can be confusing at first, but you will soon get to like it.\n What‚Äôs more, you see the constructs like ara:cora or -bera. In the base R, ara:cora means ‚Äúlook up variables ara and cora and return all integers between these two values‚Äù. In Tidyverse, that means ‚Äúselect all columns from ara to cora‚Äù. Similarly, -bera means ‚Äúselect all columns except bera‚Äù. This is called tidy evaluation and it works only with some tidyverse functions.Tidy evaluation\n\n\n\n\n\n\nRemember!\n\n\n\nTidy evaluation only works with tidyverse functions!\n\n\n Another nice feature of select() is that you can use the a few specialized helper functions, saving you tedious regular expression and column searches. For example, how would you select for columns that end with a particular suffix? This is a common enough task in data science ‚Äì many clinical data sets have hundreds of columns and a systematic way of naming them, thus allowing to search for example all columns related to laboratory diagnostics. This can easily be done with the ends_with() function:ends_with()\n\ndf2 &lt;- select(df, ends_with(\"ora\"))\ncolnames(df2)\n\n[1] \"cora\" \"dora\"\n\n\n There are many more such functions, like starts_with(), contains(), matches(), one_of(), everything() and even last_col() (for selecting the last column). You can find them all in the help page for select().Other tidy selection functions\n\n\n\n\n\n\nTo quote or not to quote?\n\n\n\nThe fact that you don‚Äôt use quotes around column names in tidyverse is confusing for many. As a rule of thumb, at this stage you should use quotes around column names, unless:\n\nyou are using $ operator (e.g.¬†df$ara)\nyou are using select(), mutate() or filter() from tidyverse\nyou are using another tidyverse function that uses tidy evaluation (look up the help page if unsure)\n\nOnce you start to program your own functions, the tidy evaluation will be an additional source of confusion, but burn that bridge when you get to it.\n\n\n\n\n4.2.3 Renaming columns\nThe select function has one more useful feature: you can directly rename the variables in the same call.\n\ndf2 &lt;- select(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"Gamma\"\n\n\n However, there is another function which can be used for renaming specific columns, aptly named rename()1. The way its syntax works it is well suitable for working with pipes (see below), and is a great deal easier than renaming the columns using colnames().rename()1¬†Unfortunately, there are many functions in R that are named rename(). Someone should really rename them (badum-tss).\n\ndf2 &lt;- rename(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"bera\"  \"Gamma\" \"dora\" \n\n\nAs you can see, no selection was made here, only renaming. Again, no quotes are needed around the column names.\n\n\n\n\n\n\n\nExercise 4.1 ¬†\n\nRead the file ‚ÄòDatasets/transcriptomics_results.csv‚Äô\nWhat columns are in the file?\nSelect only the columns ‚ÄòGeneName‚Äô, ‚ÄòDescription‚Äô, ‚ÄòlogFC.F.D1‚Äô and ‚Äòqval.F.D1‚Äô\nRename the columns to ‚ÄòGene‚Äô, ‚ÄòDescription‚Äô, ‚ÄòLFC‚Äô and ‚ÄòFDR‚Äô",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#sorting-and-ordering",
    "href": "day4-manipulating-your-data.html#sorting-and-ordering",
    "title": "4¬† Manipulating data frames",
    "section": "4.3 Sorting and ordering",
    "text": "4.3 Sorting and ordering\n\n4.3.1 Sorting and ordering vectors\nBefore we jump to sorting, filtering and ordering data frames, we should first discuss these operations on vectors. This is important, because although data frames can easily be sorted with functions such as arrange() from tidyverse, you will have situations in which you need to sort a vector, a list or another data type. Secondly, understanding the concepts of ordering will aid you also with your work with data frames.\n The basic R function for sorting is, appropriately, sort(). It is as simple as it gets:sort()\n\nvec &lt;- round(rnorm(10), 2)\nvec\n\n [1] -0.18 -0.61 -1.21  0.21  0.89 -0.23  1.09  0.93 -0.30  1.51\n\nsort(vec)\n\n [1] -1.21 -0.61 -0.30 -0.23 -0.18  0.21  0.89  0.93  1.09  1.51\n\nsort(vec, decreasing=TRUE)\n\n [1]  1.51  1.09  0.93  0.89  0.21 -0.18 -0.23 -0.30 -0.61 -1.21\n\n\nThe rnorm() function generates a vector of random numbers from a normal distribution (you met it on Day 2). The round() function with parameter 2 rounds the numbers to two decimal places. The sort() with the decreasing=TRUE argument sorts the vector in descending order.\n Amazingly, it is hardly ever used. Mostly we need the order() function. Rather than returning the sorted vector, it returns the indices of the sorted vector:order()\n\nord &lt;- order(vec)\nord\n\n [1]  3  2  9  6  1  4  5  8  7 10\n\n\nSo, what does that mean? Take the first element of ord. It is 3. This means that the first value to appear in the sorted vector should be the element number 3 of the original vector, which is -1.21 and the smallest number in vec (check it!). Likewise, the second element of ord is 2, which points to -0.61, the second smallest element of vec.\nSo what happens if we use the vector ord to select element from the vector vec? We get the sorted vector, that‚Äôs what:\n\nvec[ord]\n\n [1] -1.21 -0.61 -0.30 -0.23 -0.18  0.21  0.89  0.93  1.09  1.51\n\n\n\n\n\n\n\n\n\nExercise 4.2 (Reverse sorting) How can you get the reverse order vector for vec?\n\n\n\n\n\n\n4.3.2 Sorting character vectors\n We can also sort character vectors, but the behavior might not be exactly what you expect. Take a look:Sorting character vectors\n\nchvec &lt;- c(\"b\", \"a\", \"Zorro\", \"10\", \"Anton\", \"A\", \"2\", \"100\", \"zxy\")\nsort(chvec)\n\n[1] \"10\"    \"100\"   \"2\"     \"a\"     \"A\"     \"Anton\" \"b\"     \"Zorro\" \"zxy\"  \n\n\nOK, so what happened here? The sort() function sorts the vector in lexicographical order ‚Äì just like you sort words in a dictionary (or, in the olden days, in a phone book). For the most part that seems quite intuitive, however take a look at the numbers: 100 comes before 2.\nLexicographical order means that the words are sorted first by first letter, then by second etc. So a word starting with a 1 always comes before a word starting with a 2, even if it means a number that is larger than 2.\nThis has one important implication for all of us who work with data. Identifiers, especially sample and patient identifiers are quite often a combination of letters and numbers, e.g.¬†ID1 or Patient10. This can cause problems when sorting, as ID10 will come before ID2:\n\nchvec &lt;- c(\"ID1\", \"ID2\", \"ID3\", \"ID4\", \"ID10\", \"ID20\", \"ID100\")\nsort(chvec)\n\n[1] \"ID1\"   \"ID10\"  \"ID100\" \"ID2\"   \"ID20\"  \"ID3\"   \"ID4\"  \n\n\nThere are several solutions for that, but here is one with the tools that you already know. First, we will extract the numbers from the strings, then we will sort the vector based on these numbers:\n\n# extract numbers\nchvec_n &lt;- str_replace_all(chvec, \"ID\", \"\")\n\n# convert to numeric\nchvec_n &lt;- as.numeric(chvec_n)\n\n# get the order\nord &lt;- order(chvec_n)\n\n# sort the original vector\nchvec[ord]\n\n[1] \"ID1\"   \"ID2\"   \"ID3\"   \"ID4\"   \"ID10\"  \"ID20\"  \"ID100\"\n\n\n\n\n\n\n\n\n\nExercise 4.3 (Sorting by last name) Here is a vector of names. Can you think of how to sort it by last name?\npersons &lt;- c(\"Henry Fonda\", \"Bob Marley\", \"Robert F. Kennedy\", \n  \"Bob Dylan\", \"Alan Rickman\")\n\n\n\n\n\n\n4.3.3 Sorting data frames with order()\nBy now it should be clear that you can use this method to order data from data frames (or matrices, or other data types). We will show it on example of the transcriptomics_results.csv file that you have read in the Exercise¬†4.1. I assume that you have read the file and selected the columns with something like this:\n\nlibrary(tidyverse)\ntr_res &lt;- read_csv(\"Datasets/transcriptomics_results.csv\")\ntr_res &lt;- select(tr_res, \n                 Gene=GeneName, Description,\n                 logFC=logFC.F.D1, FDR=qval.F.D1)\n\nThe transcriptomics data set comes from a vaccine study (Weiner et al. 2019) and show the changes in gene expression after vaccination with a particular adjuvanted fluad vaccine. logFC stands for log fold change, and FDR is the false discovery rate (resulting from raw p-values being corrected using the Benjamini-Hochberg procedure). We will sort the data frame by decreasing logFC:\n\nord &lt;- order(tr_res$logFC, decreasing=TRUE)\nhead(tr_res[ord, ])\n\n# A tibble: 6 √ó 4\n  Gene     Description                                            logFC      FDR\n  &lt;chr&gt;    &lt;chr&gt;                                                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Q5D1D6   tc|Q5D1D6_CERAE (Q5D1D6) Guanylate binding protein 1,‚Ä¶  4.85 1.80e-14\n2 ANKRD22  ref|Homo sapiens ankyrin repeat domain 22 (ANKRD22), ‚Ä¶  4.81 5.55e-15\n3 ETV7     ref|Homo sapiens ets variant 7 (ETV7), transcript var‚Ä¶  4.65 2.53e-15\n4 SERPING1 ref|Homo sapiens serpin peptidase inhibitor, clade G ‚Ä¶  4.52 5.95e-16\n5 CXCL10   ref|Homo sapiens chemokine (C-X-C motif) ligand 10 (C‚Ä¶  4.38 1.51e-10\n6 GBP1P1   ref|Homo sapiens guanylate binding protein 1, interfe‚Ä¶  4.11 2.33e-17\n\n\n\n\n\n\n\n\n\nExercise 4.4 (Sorting data frames with order()) ¬†\n\nSort the data frame tr_res by increasing FDR\nSort the data frame alphabetically by Gene\n\n\n\n\n\n\n\n4.3.4 Sorting data frames with tidyverse\n Tidyverse makes it simple to sort the data frames with the arrange() function:arrange()\n\ntr_res &lt;- arrange(tr_res, desc(logFC))\nhead(tr_res)\n\n# A tibble: 6 √ó 4\n  Gene     Description                                            logFC      FDR\n  &lt;chr&gt;    &lt;chr&gt;                                                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Q5D1D6   tc|Q5D1D6_CERAE (Q5D1D6) Guanylate binding protein 1,‚Ä¶  4.85 1.80e-14\n2 ANKRD22  ref|Homo sapiens ankyrin repeat domain 22 (ANKRD22), ‚Ä¶  4.81 5.55e-15\n3 ETV7     ref|Homo sapiens ets variant 7 (ETV7), transcript var‚Ä¶  4.65 2.53e-15\n4 SERPING1 ref|Homo sapiens serpin peptidase inhibitor, clade G ‚Ä¶  4.52 5.95e-16\n5 CXCL10   ref|Homo sapiens chemokine (C-X-C motif) ligand 10 (C‚Ä¶  4.38 1.51e-10\n6 GBP1P1   ref|Homo sapiens guanylate binding protein 1, interfe‚Ä¶  4.11 2.33e-17\n\n\n Note the use of desc() function ‚Äì rather than specifying an argument like decreasing=TRUE.desc()\nUsing arrange(), it is easy to sort by multiple columns:\n\ntr_res &lt;- arrange(tr_res, logFC, FDR)\nhead(tr_res)\n\n# A tibble: 6 √ó 4\n  Gene            Description                                      logFC     FDR\n  &lt;chr&gt;           &lt;chr&gt;                                            &lt;dbl&gt;   &lt;dbl&gt;\n1 DSC1            ref|Homo sapiens desmocollin 1 (DSC1), transcri‚Ä¶ -1.97 3.33e-4\n2 XLOC_010084     tc|Q6FUE3_CANGA (Q6FUE3) Similarity, partial (1‚Ä¶ -1.76 7.59e-3\n3 LRRC69          ref|Homo sapiens leucine rich repeat containing‚Ä¶ -1.68 5.06e-2\n4 ZMAT4           ref|Homo sapiens zinc finger, matrin-type 4 (ZM‚Ä¶ -1.67 4.61e-2\n5 PCSK6           ref|Homo sapiens proprotein convertase subtilis‚Ä¶ -1.63 3.13e-2\n6 ENST00000456460 Unknown                                          -1.59 2.53e-2\n\n\nIt is also allowed to use the expressions like logFC * FDR to sort by the sum of the two columns (that doesn‚Äôt make much sense in this context, but you get the idea):\n\ntr_res &lt;- arrange(tr_res, logFC * FDR)\n\nabs()\n\n\n\n\n\n\n\nExercise 4.5 (Sorting data frames with arrange()) The abs() function returns the absolute value of a number, for example abs(c(-3, 7)) returns 3, 7. Use the arrange() function to sort the data frame tr_res by the decreasing absolute value of logFC.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#modifying-and-filtering-data-frames-in-tidyverse",
    "href": "day4-manipulating-your-data.html#modifying-and-filtering-data-frames-in-tidyverse",
    "title": "4¬† Manipulating data frames",
    "section": "4.4 Modifying and filtering data frames in tidyverse",
    "text": "4.4 Modifying and filtering data frames in tidyverse\n\n4.4.1 Modifying with mutate()\n Yesteday we have been busy cleaning up data sets. To modify the columns of the data frame, we have been using simple assignment. This works, but there is another method in tidyverse. Just like with regular assignments, the mutate() function allows both, creating new columns and modifying existing ones.mutate()\nBelow we create a new column logFC_abs which contains the absolute value of the logFC column:\n\ntr_res &lt;- mutate(tr_res, logFC_abs = abs(logFC))\n\nArguably, this is not much simpler than using simple assignment:\n\ntr_res$logFC_abs &lt;- abs(tr_res$logFC)\n\nThe main difference, as you see, is that mutate() operates on data frames ‚Äì it takes a data frame as the first argument and returns a data frame. The advantages will be obvious later, when you start to use pipes (see below).\n\n\n4.4.2 Using ifelse() with mutate()\n Say, you would like to create a new column that contains ‚Äúup‚Äù for genes that are upregulated (i.e.¬†logFC &gt;= 0) and ‚Äúdown‚Äù for genes that are downregulated (i.e.¬†logFC &lt; 0). We could do it with simple assignment and logical vectors:ifelse()\n\n# default value\ntr_res$direction &lt;- \"up\"\n\n# create logical vector\nnegative &lt;- tr_res$logFC &lt; 0\n\n# change \"up\" to \"down\" where logFC &lt; 0\ntr_res$direction[negative] &lt;- \"down\"\n\nHowever, this is a frequent operation and a much more convenient way exists. It is often used in combination with mutate, although it also can work in a normal assignment:\n\ntr_res &lt;- mutate(tr_res, \n                 direction = ifelse(logFC &lt; 0, \"down\", \"up\"))\n\nThe ifelse() function takes three arguments: a logical vector, a value to return if the logical vector is TRUE, and a value to return if the logical vector is FALSE. In other words, it ‚Äúconverts‚Äù a logical vector replacing TRUE with the second argument and FALSE with the third.\n\n\n4.4.3 Filtering with logical vectors and filter()\nAs you have learned, you can filter data frames using logical vectors. This is relatively straightforward. For example, we might want to create a table with only genes that are significant, that is have an FDR &lt; 0.05:\n\nsmall_fdr &lt;- tr_res$FDR &lt; 0.05\nsignificant &lt;- tr_res[small_fdr, ]\n\nIf you use nrow(significant), you will find that there are 1478 significant genes in the data set.\n The same can be achieved using the filter() function from tidyverse:filter()\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05)\n\nSimlarly, we can use the str_detect() function (which you learned about yesterday) to select only genes that contain the word ‚Äúinterferon‚Äù in the description:\n\ninterferon &lt;- filter(tr_res, \n                     str_detect(Description, \"interferon\"))\n\nThere are 80 such genes.\nBut what if we want to set two conditions? In statistics it is common to set a threshold not only for the p-value or FDR, but also for an effect size. In our case, the measure of the effect size is the log2 fold change. Thus, we might want to select only genes that are at leas 2-fold upregulated or down-regulated. Twofold upregulation corresponds to log2-fold change greater than 1, and 2-fold downregulation corresponds to log2-fold change less than -1.\nOne way is to use filter() with two parameters:\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05, abs(logFC) &gt; 1)\n\n However, there is a more elegant and versatile way. We can achieve the same effect by combining two logical vectors with the & operator:& (logical operator)\n\nlarge_fc &lt;- abs(tr_res$logFC) &gt; 1\nsignificant &lt;- tr_res[small_fdr & large_fc, ]\n\nOr, in tidyverse,\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05 & abs(logFC) &gt; 1)\n\nThe & operator is the logical AND operator. It combines two logical vectors, returning (at the given position) TRUE only if both vectors are TRUE. Take a look:\n\nvec1 &lt;- c(TRUE, TRUE, FALSE, FALSE)\nvec2 &lt;- c(TRUE, FALSE, TRUE, FALSE)\nvec1 & vec2\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nAt the first position both vectors are TRUE, so the result is TRUE. At position 2, the first vector is TRUE and the second is FALSE, so the result is FALSE. And so on. Try it.\n\n\n\n\n\n\n\nExercise 4.6 (Logical vectors) ¬†\n\nCreate a logical vector that selects genes that are both significant (FDR &lt; 0.05) and contain the string ‚Äúinterferon‚Äù in the Description. How many are there? How many genes are significant and do not contain ‚Äúinterferon‚Äù in the Description column?\nHow many of the genes containing ‚Äúinterferon‚Äù in the Description are are guanylate binding proteins? You can recognize the GBPs by their Gene symbol ‚Äì it starts with ‚ÄúGBP‚Äù.\n\n\n\n\n\n\n\n4.4.4 Using the %in% operator\n Another useful utility in searching data frames is the %in% operator. It compares two vectors; for each element of the first vector, it returns TRUE if that element is in the second vector, and FALSE otherwise.%in%\n\nvec1 &lt;- c(\"a\", \"b\", \"c\")\nvec2 &lt;- c(\"a\", \"b\", \"x\", \"y\", \"z\")\nvec1 %in% vec2\n\n[1]  TRUE  TRUE FALSE\n\n\nThe elements 1, of the result are TRUE, because both a and b are in vec2. The last element, however, is FALSE, because c is not in vec2.\nWith %in% we can for example select a specified subset of genes that we are interested in.\n\ngenes &lt;- c(\"GBP1\", \"GBP2\", \"GBP3\", \"GBP4\", \"GBP5\", \"ANKRD22\")\nfilter(tr_res, Gene %in% genes)\n\n# A tibble: 7 √ó 6\n  Gene    Description                         logFC      FDR logFC_abs direction\n  &lt;chr&gt;   &lt;chr&gt;                               &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 GBP4    ref|Homo sapiens guanylate binding‚Ä¶  2.25 2.59e-21      2.25 up       \n2 GBP5    ref|Homo sapiens guanylate binding‚Ä¶  2.92 1.39e-20      2.92 up       \n3 GBP3    ref|Homo sapiens guanylate binding‚Ä¶  2.63 1.62e-15      2.63 up       \n4 ANKRD22 ref|Homo sapiens ankyrin repeat do‚Ä¶  4.81 5.55e-15      4.81 up       \n5 GBP2    ref|Homo sapiens guanylate binding‚Ä¶  1.8  1.17e-13      1.8  up       \n6 GBP1    ref|Homo sapiens guanylate binding‚Ä¶  2.76 7.38e-12      2.76 up       \n7 GBP3    ref|Homo sapiens guanylate binding‚Ä¶  1.27 3.40e- 3      1.27 up       \n\n\n\n\n\n\n\n\n\nExercise 4.7 Which of these genes are significant? Use the & operator to combine the result from %in% with the result of FDR &lt; 0.05.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#combining-data-sets",
    "href": "day4-manipulating-your-data.html#combining-data-sets",
    "title": "4¬† Manipulating data frames",
    "section": "4.5 Combining data sets",
    "text": "4.5 Combining data sets\n\n4.5.1 Binding data frames with rbind() and removing duplicates\nThe simplest way to combine two data frames is to bind them. This can be done horizontally (when you have the same number of observations ‚Äì rows) or vertically, by stacking them on top of each other, when you have the same columns. The two functions are, respectively, cbind() and rbind(), and they work for both, matrices and data frames.\n Let‚Äôs start with stacking them on top of each other. For example, we might have generated two data frames with interesting results: one with interferones and the other with guanylate binding proteins, and at some point we decide to have them in one data frame. Stacking them can be done with the rbind() (‚Äúrow bind‚Äù) function:rbind()\n\ninterferon &lt;- filter(tr_res, \n                     str_detect(Description, \"interferon\"))\ngbp &lt;- filter(tr_res,\n              str_detect(Gene, \"GBP\"))\nresults &lt;- rbind(interferon, gbp)\n\nNote that this works only if the data frames have the same columns. If you were to try to bind two data frames with different columns, you would get an error:\n\na &lt;- data.frame(a=1:5, b=6:10)\nb &lt;- data.frame(a=11:15, c=16:20)\nrbind(a, b)\n\nError in match.names(clabs, names(xi)): names do not match previous names\n\n\nOK, but if you have checked the interferon and gbp data frames, you would have noticed that there are some genes that are both interferones and guanylate binding proteins. You did not check, because I haven‚Äôt told you yet how to do it. Here is how:\nintersect()\n\nintersect(interferon$Gene, gbp$Gene)\n\n[1] \"GBP1P1\" \"GBP2\"   \"GBP1\"  \n\n\n The intersect() function returns the elements that are common to two vectors. As you can see, there are three: GBP1P1, GBP2 and GBP1. We can get rid of them from the results table thanks to the function duplicated(). For every element of a vector, duplicated() returns TRUE if the element occured previously in the vector. So for the vector c(1, 2, 1) the result will be FALSE, FALSE, TRUE - because the second 1 is duplicated.duplicated()\nTo remove the elements which are duplicated, however, we need to use the negation ‚Äì ! to get TRUE for every element which is not duplicated. Here it is in action:\n\n# count the number of the duplicated results\nsum(duplicated(results$Gene))\n\n[1] 19\n\n# number of rows before removing dups\nnrow(results)\n\n[1] 96\n\n# remove duplicates\nresults &lt;- filter(results, !duplicated(Gene))\n\n# how many are left?\nnrow(results)\n\n[1] 77\n\n\n\n\n\n\n\n\n\nExercise 4.8 You might have been expecting that there are only three duplicates; after all, only three genes were in common between interferon and gbp- So where do these other duplicates come from? What are they?\n\n\n\n\n\n\n4.5.2 Binding data frames with cbind()\nCombining data frames (or matrices) horizontally is much easier, because you do not have to worry about column names. You just need to make sure that the number of rows is identical2.\n2¬†Strictly speaking, the recycling rules apply. See Day 1. This can be sometimes useful, but usually it is better to be more explicit.cbind()\n\ndf1 &lt;- data.frame(a = 1:4, b=11:14)\ndf2 &lt;- data.frame(c=21:24)\ncbind(df1, df2)\n\n  a  b  c\n1 1 11 21\n2 2 12 22\n3 3 13 23\n4 4 14 24\n\n\nWe will use cbind() tomorrow when running a principal component analysis (PCA).\n\n\n4.5.3 Merging data sets\nQuite often you will have to merge two data sets. For example, you might have one Excel file containing the covariates for the patients (like BMI, age, gender etc.) and another file containing the gene expression data. Both data frames must have some identifiers present that allow us to match the two data frames.\nThis is not a simple binding operation, because you do not have the guarantee that (a) the identifiers between these two data sets are in the same order or even that (b) both data frames have exactly the same sets of identifiers. Actually, the reverse is more common that not.\nIn order to merge two data frames we need first to identify by what to merge them. In the code below we will concoct two data frames that share some of the identifiers.\n\nids_x &lt;- paste0(\"ID\", sample(1:8, 6))\ndf_x &lt;- data.frame(ID=ids_x, val_x=rnorm(6))\ndf_x\n\n   ID     val_x\n1 ID3 0.1115434\n2 ID7 0.4967613\n3 ID8 0.8739055\n4 ID6 1.2220256\n5 ID4 1.0963176\n6 ID5 0.4095521\n\nids_y &lt;- paste0(\"ID\", sample(1:8, 6))\ndf_y &lt;- data.frame(ID=ids_y, val_y=rnorm(6, 10, 1))\ndf_y\n\n   ID     val_y\n1 ID6  9.580403\n2 ID7 11.244461\n3 ID5 10.939076\n4 ID1  8.932028\n5 ID8 11.344077\n6 ID3  9.623245\n\nintersect(ids_x, ids_y)\n\n[1] \"ID3\" \"ID7\" \"ID8\" \"ID6\" \"ID5\"\n\n\nsample()\nThe sample() function is very useful ‚Äì it generates a random sample by choosing elements from a vector. This is what is called sampling and comes in handy in many situations. Here we use it to create two sets of IDs that are similar, but not identical. As you can see from the output of intersect(), only 5 elements out of the total 6 are common between the two data frames.\nMerging can be done either with the base R function merge(), or with corresponding *_join functions from the tidyverse. Whether you prefer one or the other is a matter of preference; I like merge() and we will be using it in the examples below.\nInner join, full join\nFirst, however, let us think how we want to merge, or, more specifically, what to do with the identifiers that are present in one data frame, but not in the other? That depends only on what we want to achieve. Possibly we are not interested in any elements that are not present in both data frames. This is what is called in database lingo an inner join Alternatively, we might want to keep all possible elements, whether they are found in df_x or df_y only. This is called an full join.\nmerge()\n\n# inner join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\")\njoined\n\n   ID     val_x     val_y\n1 ID3 0.1115434  9.623245\n2 ID5 0.4095521 10.939076\n3 ID6 1.2220256  9.580403\n4 ID7 0.4967613 11.244461\n5 ID8 0.8739055 11.344077\n\n# full join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all=TRUE)\njoined\n\n   ID     val_x     val_y\n1 ID1        NA  8.932028\n2 ID3 0.1115434  9.623245\n3 ID4 1.0963176        NA\n4 ID5 0.4095521 10.939076\n5 ID6 1.2220256  9.580403\n6 ID7 0.4967613 11.244461\n7 ID8 0.8739055 11.344077\n\n\nAs you can see, in the first case there are just 5 rows ‚Äì as many as there are common elements between the ID columns of the two data frames. In the second case, we get all 7 possible IDs. Where the ID was present in one, but not the other data frame, the values were filled up with NA‚Äôs. To get the full join we used the parameter all=TRUE, standing for ‚Äúuse all IDs‚Äù.\nNote: it is possible to omit the by= parameter specifying by what column to join the data frames. However, do yourself a favor and always specify the column to join on explicitely.\nLeft join, right join\nThere are two more situations, allthough less common in practice. We might want to keep all elements of df_x, but discard any elements of df_y which are not present in df_x. This is called a left join. Or, vice versa, we will discard only these elements which are present in df_x, but not in df_y. This is called, you guessed it, a right join.\n\n# left join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all.x = TRUE)\njoined\n\n   ID     val_x     val_y\n1 ID3 0.1115434  9.623245\n2 ID4 1.0963176        NA\n3 ID5 0.4095521 10.939076\n4 ID6 1.2220256  9.580403\n5 ID7 0.4967613 11.244461\n6 ID8 0.8739055 11.344077\n\n# right join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all.y = TRUE)\njoined\n\n   ID     val_x     val_y\n1 ID1        NA  8.932028\n2 ID3 0.1115434  9.623245\n3 ID5 0.4095521 10.939076\n4 ID6 1.2220256  9.580403\n5 ID7 0.4967613 11.244461\n6 ID8 0.8739055 11.344077\n\n\nAs you can see, in case of the left join, there are missing values in the right column only, and in case of the right join ‚Äì only in the left column.\nLet us summarize it in a table:\n\n\n\n\n\n\n\n\n\n\nType of join\nall x\nall y\nmerge options\ntidyverse alternative\n\n\n\n\ninner join\nFALSE\nFALSE\nmerge(x, y)\ninner_join(x, y)\n\n\nleft join\nTRUE\nFALSE\nmerge(x, y, all.x=TRUE)\nleft_join(x, y)\n\n\nright join\nFALSE\nTRUE\nmerge(x, y, all.y=TRUE)\nright_join(x, y)\n\n\nfull join\nTRUE\nTRUE\nmerge(x, y, all=TRUE)\nfull_join(x, y)\n\n\n\nYou don‚Äôt have to remember the names (inner, left, right, full) ‚Äì just remember the principle: sometimes you need the common elements, sometimes you need all elements, and sometimes you need all elements from x or y only.\nDifferent identifier column names. There is one other thing that you should know, because it happens quite often. The two data frames might have identifiers in columns that have different names, for example ID in one and PatientID in the other. You can, of course, rename the columns, but you can also specify the columns to join on explicitly:\nmerge() with different column names\n\ndf_x &lt;- data.frame(ID=ids_x, val_x=rnorm(6))\ndf_y &lt;- data.frame(PatientID=ids_y, val_y=rnorm(6, 10, 1))\n\n# inner join by ID\njoined &lt;- merge(df_x, df_y, by.x=\"ID\", by.y=\"PatientID\")\n\nThe resulting data frame has the identifiers in the column name specified in the ‚Äúx‚Äù data frame, that is ‚Äì in this case ‚Äì in the ID column.\n\n\n4.5.4 Complex merges\nThe situation above is quite trivial. In real world, unfortunately, the situations can be quite complex.\nFirst, there can be duplicates. For example, one data frame contains the meta data on a group of lab animals, while the second contains the results of the experiments at two time points. This is not a problematic situation, but you should understand what happens here:\n\nmeta_data &lt;- data.frame(ID=paste0(\"ID\", 1:4),\n                        age=sample(1:3, 4, replace=TRUE),\n                        group=rep(c(\"control\", \"treated\"), each=2))\nmeta_data\n\n   ID age   group\n1 ID1   3 control\n2 ID2   1 control\n3 ID3   2 treated\n4 ID4   1 treated\n\ncrp &lt;- data.frame(ID=rep(paste0(\"ID\", 1:4), each=2),\n                           time=rep(c(\"day1\", \"day2\"), 4),\n                           CRP=rnorm(8))\ncrp\n\n   ID time        CRP\n1 ID1 day1 -0.9638103\n2 ID1 day2  0.5258168\n3 ID2 day1 -0.9405811\n4 ID2 day2 -0.7939182\n5 ID3 day1  0.8564467\n6 ID3 day2 -0.1906111\n7 ID4 day1 -0.9409314\n8 ID4 day2  0.7253967\n\nmerged_data &lt;- merge(meta_data, crp, by=\"ID\")\nmerged_data\n\n   ID age   group time        CRP\n1 ID1   3 control day1 -0.9638103\n2 ID1   3 control day2  0.5258168\n3 ID2   1 control day1 -0.9405811\n4 ID2   1 control day2 -0.7939182\n5 ID3   2 treated day1  0.8564467\n6 ID3   2 treated day2 -0.1906111\n7 ID4   1 treated day1 -0.9409314\n8 ID4   1 treated day2  0.7253967\n\n\nAs you can see, each line in the meta_data data frame is duplicated to fill up the information matching the ID in the crp data frame.\nHowever, chances are the situation is more complex. Imagine that apart from the above measurements, you have another set of measurements, say, of the albumin (ALB) levels. You would like to merge the two data frames. Naturally, you expect that you will get a data frame with 8 rows. The following will not work as expected:\n\nalbumin &lt;- data.frame(ID=rep(paste0(\"ID\", 1:4), each=2),\n                      time=rep(c(\"day1\", \"day2\"), 4),\n                      ALB=rnorm(8))\n\n# incorrect code!!!\ncrp_alb &lt;- merge(crp, albumin, by=\"ID\")\ncrp_alb\n\n    ID time.x        CRP time.y        ALB\n1  ID1   day1 -0.9638103   day1 -0.9049786\n2  ID1   day1 -0.9638103   day2  0.9992066\n3  ID1   day2  0.5258168   day1 -0.9049786\n4  ID1   day2  0.5258168   day2  0.9992066\n5  ID2   day1 -0.9405811   day1  0.1732707\n6  ID2   day1 -0.9405811   day2  0.4903695\n7  ID2   day2 -0.7939182   day1  0.1732707\n8  ID2   day2 -0.7939182   day2  0.4903695\n9  ID3   day1  0.8564467   day1  0.5506633\n10 ID3   day1  0.8564467   day2 -0.4586490\n11 ID3   day2 -0.1906111   day1  0.5506633\n12 ID3   day2 -0.1906111   day2 -0.4586490\n13 ID4   day1 -0.9409314   day1  0.7629459\n14 ID4   day1 -0.9409314   day2  1.6650973\n15 ID4   day2  0.7253967   day1  0.7629459\n16 ID4   day2  0.7253967   day2  1.6650973\n\n\nUh-oh, what happened here?\nYou see, the problem is that the merge() function merges the data frames by the column ID, but it does not take into account the time column. Since it observes that ID‚Äôs are duplicated in both data frames, it creates all possible pairs of the rows: CRP from day1 with ALB day2; but also CRP from day1 and ALB from day1, CRP from day2 and ALB from day1 and so on. For each identifier. That is why we are getting 4 (and not 2) rows for each identifier.\n This is a common problem, and the solution is to merge by more than one identifier. The important question is: what combination of identifiers uniquely identifies a row in the data frame? In our case, it is the combination of ID and time. We can specify that in the by= parameter:Merging by more than one ID\n\ncrp_alb &lt;- merge(crp, albumin, by=c(\"ID\", \"time\"))\ncrp_alb\n\n   ID time        CRP        ALB\n1 ID1 day1 -0.9638103 -0.9049786\n2 ID1 day2  0.5258168  0.9992066\n3 ID2 day1 -0.9405811  0.1732707\n4 ID2 day2 -0.7939182  0.4903695\n5 ID3 day1  0.8564467  0.5506633\n6 ID3 day2 -0.1906111 -0.4586490\n7 ID4 day1 -0.9409314  0.7629459\n8 ID4 day2  0.7253967  1.6650973\n\n\n\n\n4.5.5 Bringing it all together\nWe are now coming to the final exercise in this section. This exercise is based on data from the vaccination study (Weiner et al. 2019). One file contains meta-data for the samples used in transcriptomic analysis ‚Äì it only includes the subjects and time points for which the gene expression data is present. The other file is quite large and contains the results of several laboratory tests taken for many patients at many time points.\nNote that the data is not real, just based on real data.\n\n\n\n\n\n\n\nExercise 4.9 (Merging large data sets) The files expression_data_vaccination_example.xlsx and labresults_full.csv contain data from the same study.\n\nRead CSV file and the first sheet from the XLSX file.\nWhich columns contain the ID the subjects? Are there any subjects in common? How do you match the subjects?\nWe are interested only in the following information: Subject ID, ARM (group), time point, sex, age, test name and the actual measurement. Are the measurements numeric? Remember, you can use expressions like [ , c(\"ARM\", \"sex\") ] or select(df, ARM, sex) to select the desired columns from a data set.\nUse the subjects and / or other information to merge the two data frames however you see fit. Note that there are multiple time points per subject and multiple measurements per subject and time point.\n\nSolution: Section 1",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#pipes-in-r",
    "href": "day4-manipulating-your-data.html#pipes-in-r",
    "title": "4¬† Manipulating data frames",
    "section": "4.6 Pipes in R",
    "text": "4.6 Pipes in R\n\n4.6.1 Too many parentheses\nRemember how functions work? Each function takes some arguments and returns a value. And we can use that value to plug in back into another function. This allows us to write terse code, for example:\n\nvec &lt;- c(1, NA, 3, 10)\nwhich(is.na(as.numeric(vec)))\n\n[1] 2\n\n\nThis is quite easy to understand. Unfortunately, this can quickly become unreadable. Let us consider cleaning up the data from yesterday. We can do it one by one, like this:\n\nlibrary(tidyverse)\nlibrary(janitor)\niris_data &lt;- read_csv(\"Datasets/iris.csv\")\niris_data &lt;- clean_names(iris_data)\niris_data$species &lt;- tolower(iris_data$species)\niris_data$petal_length &lt;- str_replace_all(iris_data$petal_length ,\"[^0-9.]\", \"\") \niris_data$petal_length &lt;- as.numeric(iris_data$petal_length)\niris_data$sepal_length &lt;- str_replace_all(iris_data$sepal_length, \",\", \".\")\niris_data$sepal_length &lt;- str_replace_all(iris_data$sepal_length ,\"&gt; \", \"\") \niris_data$sepal_length &lt;- as.numeric(iris_data$sepal_length)\n\nThat is a lot of code. However, we could do it all on one line:\n\niris_data &lt;- \n  mutate(mutate(mutate(clean_names(read_csv(\"Datasets/iris.csv\")),\n  petal_length = as.numeric(\n  str_replace_all(petal_length, \"[^0-9.]\", \"\"))),\n  sepal_length = as.numeric(str_replace_all(\n  str_replace_all(sepal_length, \",\", \".\"), \"&gt; \", \"\"))),\n  species = tolower(species))\n\nIf you spend some time deciphering this, you will see that it does precisely the same operations as the previous code fragment. However, it is completely unreadable and unmaintainable3.\n3¬†I got a headache just trying to figure out the correct number of parentheses in this code.\n\n4.6.2 Pipes\n Fortunately, there is a dirty trick that results in clean and readable code: the pipe operator |&gt; (pronounced ‚Äúpipe‚Äù; in older code you might see ‚Äú%&gt;%‚Äù instead4).Pipe operator |&gt;4¬†The idea of the pipe operator was popularized by the magrittr package, which is a part of Tidyverse, as %&gt;%. R users found it so cool that with R version 4.1.0, the pipe operator |&gt; was included in the base R. There are some differences between |&gt; and %&gt;%, but for our purposes, they are equivalent.\nPipe operator allows for writing very clean, very convenient and very readable code. The basic idea behind the pipe operator is as follows: a |&gt; f(b) is the same as f(a, b). We can say it pipes the value of a to the function f. And if a is already the result of a function, say g(a), then g(a) |&gt; f(b) is the same as f(g(a), b). We build a pipe from the output of g() to the input of f().\nSo, for example, the following two lines are equivalent:\n\nvec &lt;- c(1, NA, 3, 10)\nas.numeric(vec)\n\n[1]  1 NA  3 10\n\nvec |&gt; as.numeric()\n\n[1]  1 NA  3 10\n\n\nPipe takes whatever is on its left side and inserts it in the parentheses of the function on its right side. Now think for a moment what it does to constructs like f1(f2(f3(f4(f5(x))))). Now compare this:\n\nwhich(is.na(as.numeric(vec)))\n\n[1] 2\n\nvec |&gt; as.numeric() |&gt; is.na() |&gt; which()\n\n[1] 2\n\n\nThe two lines are equivalent. However, the latter is much more readable: it shows how the value goes into as.numeric(), then the output of as.numeric() goes into is.na(), and the resulting logical vector goes into which().\nSee how that works for our iris dataset:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\") |&gt;\n        clean_names() |&gt;\n        mutate(species = tolower(species)) |&gt;\n        mutate(petal_length = str_replace_all(petal_length ,\"[^0-9.]\", \"\")) |&gt;\n        mutate(petal_length = as.numeric(petal_length)) |&gt;\n        mutate(sepal_length = str_replace_all(sepal_length, \",\", \".\")) |&gt;\n        mutate(sepal_length = str_replace_all(sepal_length ,\"&gt; \", \"\")) |&gt;\n        mutate(sepal_length = as.numeric(sepal_length))\n\nIt is just as clear and readable as the first example, but with some additional benefits.\nFirst, think what happens if you want to change the name of the variable, say, from iris_data to flowers. In the first example, you would have to change every single occurence of iris_data to flowers. Sure, it would not be all too bad if you can do it automatically with a search-and-replace, but what if the variable name was a?\nSecond, the pipe facilitates development. You build a pipe line by line, and each time you execute the code, all operations are repeated. This is a good thing ‚Äì you avoid the situation where you forget to execute one line and the code does not work.\nThere is an alternative to the above code which uses pipes, but not all on the same line:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\") |&gt;\n        clean_names() |&gt;\n        mutate(species = tolower(species))\n\niris_data$petal_length &lt;- iris_data$petal_length |&gt; \n        str_replace_all(\"[^0-9.]\", \"\") |&gt;\n        as.numeric()\n\niris_data$sepal_length &lt;- iris_data$sepal_length |&gt;\n        str_replace_all(\",\", \".\") |&gt;\n        str_replace_all(\"&gt; \", \"\") |&gt;\n        as.numeric()\n\nWhether you prefer this one or the previous is a matter of both, taste and context. The second is more appropriate if for each column you have to do a lot of manipulations.\nOne limitation is that in the form that is used here5, the value passed on from function to function is always the first argument. This is why we are doing the substitutions with the Tidyverse function str_replace_all() (which takes the character vector as the first argument) rather than the base R function gsub() (which takes the pattern as the first argument).\n5¬†This limitation can be circumvented either by using only named arguments, or by using a placeholder, _. See here for an explanation.\n\n\n\n\n\n\nExercise 4.10 (Botched meta data) Go back to the code that you have used yesterday for cleaning up the dataset from file meta_data_botched.xlsx. Use pipes to make it more readable.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#review",
    "href": "day4-manipulating-your-data.html#review",
    "title": "4¬† Manipulating data frames",
    "section": "4.7 Review",
    "text": "4.7 Review\nThings that you learned today:\n\nSelecting columns in data frames:\n\nusing square brackets\nusing select() from tidyverse\ntidy evaluation\nrenaming columns with rename()\n\nSorting, ordering, filtering\n\nsort() to sort a vector\norder() to get the order of a vector\norder() to sort a data frame\narrange() from tidyverse to sort a data frame\nfiltering with logical vectors\nfiltering with filter() from tidyverse\nhandling duplicates with duplicated()\ncombining logical vectors with &\nusing %in% operator\n\nMerging data frames\n\nrbind() and cbind()\nmerge() inner, left, right and full joins\nmerging by more than one column\n\nPipes in R\n\nusing |&gt; operator\nbuilding pipelines\n\nOther\n\nusing rnorm() to generate random numbers\nusing round() to round numbers\nsample() for generating random samples\n\n\n\n\n\n\nWeiner, January, David JM Lewis, Jeroen Maertzdorf, Hans-Joachim Mollenkopf, Caroline Bodinham, Kat Pizzoferro, Catherine Linley, et al. 2019. ‚ÄúCharacterization of Potential Biomarkers of Reactogenicity of Licensed Antiviral Vaccines: Randomized Controlled Clinical Trials Conducted by the BIOVACSAFE Consortium.‚Äù Scientific Reports 9 (1): 20362. https://www.nature.com/articles/s41598-019-56994-8.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html",
    "href": "day5-visualization-and-statistics.html",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "",
    "text": "5.1 R markdown",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#r-markdown",
    "href": "day5-visualization-and-statistics.html#r-markdown",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "",
    "text": "5.1.1 What is R markdown?\nR markdown is a remarkably simple to use and yet powerful tool. The basic idea is as follows: you write a document in a simple text format ‚Äì for example, a manuscript, a report, or even a thesis. In that document, you include R ‚Äúcode chunks‚Äù. These code chunks are executed by R, and the output is included in the document ‚Äì including figures or tables. And then, from this single simple text document you can create a PDF, a Word document, an HTML page (or a whole website!), a book, a slide presentation and much, much more.\n\n\n\n\n\nflowchart LR\n    A(R markdown / Quarto\\ndocument\\nwith R code) --&gt;|knitr| B(Markdown\\nwith R code output)\n    B --&gt; C[LaTeX]\n    C --&gt; CC[PDF]\n    B --&gt; D[Word]\n    B --&gt; E[HTML]\n    B --&gt; F[Presentation]\n    B --&gt; G[Book]\n\n\n\n\n\n\nIn fact, this book you are reading now has been entirely written in R markdown1. You can go to the github repository for this book and see the R markdown/Quarto files that generated it ‚Äì for example, here is the page you are reading right now. It is also possible to create a scientific paper completely in R markdown (here is an example of such a paper: Weiner 3rd, Obermayer, and Beule (2022)).\n1¬†Actually, in its successor called ‚ÄúQuarto‚Äù.Why is this so great?\n\nReproducibility: It allows you to keep the methods, analysis itself and the analysis results in one place.\nAvoiding errors: Whenever you re-run your analysis, R automatically updates all your figures, tables, results, etc. in your document. This prevents you from forgetting to update a figure or a number in your manuscript.\nFlexibility: You can easily change the output format of your document; you can update your figures or your tables easily without having to resort to manual editing.\nBibliography. As you have seen in this book, it is not hard to include bibliography in your document, in any style you desire. You can use a free package manager such as Zotero or Mendeley to manage your bibliography, and produce the bibliography in the format that R markdown uses.\n\nWhat are the disadvantages?\n\nSteeper learning curve. You have to learn all the stuff first. Luckily, when it comes to R markdown, it is quite easy and there are plenty of resources, and after today you will know all the most important stuff.\nNo fine control over the layout. While you can easily use simple formatting commands, you will have to resort to more complex tools to control things like font size of the chapter headers2. Some of us consider it to be a good thing3 ‚Äì this is the boring part that the computers should take care of, but sometimes it is annoying. However, you can always generate a Word file with R markdown that uses a Word template, and then fine tune it in Word.\nCollaboration. Collaborating with markdown is not as easy as with a Word document, because most people can‚Äôt use markdown. Often that means that you need to communicate with your co-authors using Word, and then patiently type in their changes into your markdown document.\n\n2¬†Actually, you can control the format for PDF and HTML output very precisely, but then you have to learn LaTeX and CSS, respectively. For Word, the only option is to use a Word file with pre-defined styles as a template.3¬†Me.\n\n\n\n\n\nR markdown vs Quarto\n\n\n\n\nR markdown is older and more widely supported\nQuarto is newer, slightly more complex, with additional features and generally better looking\n\nDocuments created in Quarto can largely be processed with R markdown and vice versa, only some visual bells & whistles might be lacking.\nBoth Quarto and R markdown are available if you have installed RStudio. Standalone R installations without RStudio may require additional packages (e.g.¬†rmarkdown for R markdown) or programs (quarto for Quarto).\n\n\n\n\n\n\n\n\n\nExercise 5.1 (Create an R markdown document) Go to the ‚ÄúFile‚Äù menu in Rstudio and choose ‚ÄúNew File‚Äù -&gt; ‚ÄúR markdown‚Äù. Enter a title and click on ‚ÄúOK‚Äù. Save the file in your project directory and take a look at it. Rstudio has created a simple R markdown file for you so that you might get an idea of how it works.\nClick on the ‚Äúknit‚Äù icon in the toolbar to render the document. What do you see? How does it relate to the content of the document? Try changing a few words in the document and click on ‚Äúknit‚Äù again. What happens?\n\n\n\n\n\n\n5.1.2 Markdown basics\nThe ‚ÄúR‚Äù in R markdown stands for ‚ÄúR‚Äù, the programming language, combined with markdown. But what is markdown?\nMarkdown is a very lightweight formatting system, much like what many of us are using in emails or messengers, stressing words by surrounding them with stars etc., but with a few extra features. The idea is that you can write the text in a very simple way and it remains readable even with the formatting marks (take a look!). Basic markdown formatting\n\n\n\nCode\nOutput\n\n\n\n\n**bold**\nbold\n\n\n*italic*\nitalic\n\n\n3^2^\n32\n\n\nlog~2~\nlog2\n\n\n`code`\ncode\n\n\n[URL](https://cubi.bihealth.org)\nlink\n\n\n\n\nHere is another feature: lists. Lists in markdown\n\n\nCode:\n- item 1\n- item 2\n  - subitem 1\n  - subitem 2\n- item 3\n\nResult:\n\nitem 1\nitem 2\n\nsubitem 1\nsubitem 2\n\nitem 3\n\n\n\n\nThere is much more to it (look up for example markdown guide from Rstudio or the Quarto markdown documentation), but you don‚Äôt need it right now. Just keep in mind that you can always take a look at the markdown source of this document to see how things can be done.\n\n\n\n\n\n\nMathematical formulas\n\n\n\nUsing a special syntax, it is possible to include virtually any mathematical formula in R, both inline variant (like \\(\\sigma=\\sqrt{\\frac{(x_i-\\bar{x})^2}{n}}\\)) or as a stand-alone block:\n\\[\\sigma=\\sqrt{\\frac{(x_i-\\bar{x})^2}{n}}\\]\nWhen you convert the R markdown document to Word, you will even be able to edit the formulas natively in Word.\nThere are several formulas in this book, if you are interested, look up the book quarto sources on github or check this guide.\n\n\n\n\n\n\n\n\n\nExercise 5.2 (Formatting markdown) Try some formatting commands in markdown. For example, try to make a word bold or italic. Try to create a list.\n\n\n\n\n\n\n5.1.3 R markdown header\nYou might have noticed that at the top of the first R markdown file you created there is a block of text that might look something like this:\n---\ntitle: \"Untitled\"\nauthor: \"JW\"\ndate: \"`‚Äãr Sys.Date()`\"\noutput: html_document\n---\nThis is a block with meta-information about your document. For example, it specifies the title, author and date. Note that date is updated automatically every time the document is rendered by executing the r command Sys.Date() (you will learn about the inline chunks in a moment). Naturally, you can run the command in your console or script. `Sys.Date()¬¥\n\nSys.Date()\n\n[1] \"2024-10-07\"\n\n\nThe output: specifies the type of output you would like to have.\n\n\n\n\n\n\n\nExercise 5.3 (Choosing output format) When editing your R markdown document, click on the little ‚ñº arrow next to the ‚ÄúKnit‚Äù icon in RStudio. Choose the PDF format. Observe what happens to the header of your document.\n\n\n\n\n\n\n5.1.4 R code chunks\nIn between the markdown text, you can include R code chunks. These are executed consicutively by R and the output is included in the document. Every single fragment of code in this book is a code chunk.\nEach chunk starts with a ```{r} and ends with a ``` (these should  be placed on separate lines). You can also add the chunks by clicking on the little green plus sign in the Rstudio toolbar and choosing ‚ÄúR chunk‚Äù. Here is an example of a code chunk:R chunks\n\n\nR markdown:\n```{r}\nx &lt;- 1:10\nprint(x)\nplot(x)\n```\n\n\n\nOutput:\n\nx &lt;- 1:10\nprint(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nplot(x)\n\n\n\n\n\n\n\n\n\n\nBy default, the code itself is also shown in the document. You can configure this by clicking on the little cogwheel ‚öô icon to the right of the chunk start. You will notice that configuring the options means essentially adding stuff like echo=FALSE to the chunk start (echo=FALSE  means that the code itself is not shown in the document).echo=FALSE\n\n\nR markdown:\n```{r echo=FALSE}\nx &lt;- 1:10\nprint(x)\nplot(x)\n```\n\n\n\nOutput:\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\n\n\n\nThe code chunks can also be inline, that is, you can put directly a code chunk in your sentence. For example, when I write that the \\(\\pi\\) constant is equal to 3.1415927, I am using an inline code chunk:\nFor example, when I write that the $\\pi$ constant is \nequal to `‚Äãr pi`, I am using an inline code chunk:\nThis point is not to save typing. The point is to update your document whenever you update your analysis. For example, if you add samples to your data and re-run the analysis, the number of samples that you have written in your method section must reflect the change. If you use an inline code chunk, it will do so automatically, without you needing to painstakingly update each number, each figure, each result in your manuscript.\n\n\n\n\n\n\n\nExercise 5.4 (Put your code in your markdown) ¬†\n\nGo back to Day 1, the water lillies example in Section 1.6.1. Copy the code to the R markdown, including the plot and Exercise¬†1.8.\nWrite a brief summary including inline code chunks that show the number of days of the simulation used.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#visualizations-with-r",
    "href": "day5-visualization-and-statistics.html#visualizations-with-r",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.2 Visualizations with R",
    "text": "5.2 Visualizations with R\n\n5.2.1 Base R vs ggplot2\nWe already had simple examples of plots in R (for example on day one, in the Water Lily example - Section 1.6.1). We even had a look at the ggplot2 package on Day 3 (Section 3.5.5). Why are there so many plotting systems in R?\nActully, the situation is even more complex than you might imagine. Partly that is because there are so many different types of plots that it is hard to make a single system that would be good for all of them. Partly because when R was first created, many modern graphical formats and features did not exist yet. But the main problem, as usual, is that anybody can write their own. And so they did.\nFortunately for us, the two graphic systems ‚Äì base R and ggplot2 ‚Äì are sufficient even for the most sophisticated plots. It is useful to know them both, however. Base R is simple and allows very quick plotting of simple tasks. Moreover, many base R statistical functions have built-in plotting capabilities (e.g.¬†you can simply call plot(model) for a linear regression model to get all relevant plots). ggplot2, on the other hand, is more complex, but it is also working on a much higher level and takes care of many things automatically.\nHere is a table comparing the basic features of the two systems.\n\n\n\n\n\n\n\nBase R\nggplot2\n\n\n\n\nSimple to use for simple task, gets very complex for complex tasks\nMore complex, but more powerful and makes complex tasks easier\n\n\nMany packages\nMany packages\n\n\nPlots need many lines to be customized\nPlots are customized in one line\n\n\nLow-level, with absolute feature control\nHigh-level, with automatic feature control\n\n\n\nMost of what can be done with ggplot2 can be done with base R, but it often requires many more lines of code. On the other hand, it is easier to develop a de novo approach with base R, because programming new features in ggplot is not for the faint of heart.\nLet us plot a simple scatter plot with both systems and show how customization will look like in both. We will use the iris data set4.\n4¬†Not the doctored version we have been loading in the previous days, but the built-in original data set. If you have created an iris variable in your environment, type rm(iris) to remove it.Base R:\n\ncolors_map &lt;- c(setosa=\"red\", versicolor=\"green\", virginica=\"blue\")\ncolors &lt;- colors_map[iris$Species]\nplot(iris$Sepal.Length, iris$Sepal.Width, \n     col=colors, pch=19, xlab=\"Sepal length\", \n     cex=1.5,\n     ylab=\"Sepal width\", main=\"The iris data set\")\nabline(-2.8, 1.1, col=\"red\")\nlegend(\"topright\", legend=unique(iris$Species), \n       col=colors_map[unique(iris$Species)], pch=19)\n\n\n\n\n\n\n\n\nNote that in the code above the col= parameter simply takes a vector describing colors. Color management must be done by the user ‚Äì you cannot simply say ‚Äúchoose the viridis palette‚Äù (or similar). You also need to remember some weird things, like that the pch=19 means ‚Äúfilled circles‚Äù (in contrast, pch=1 means ‚Äúempty circles‚Äù, and if you need triangles, you have to use pch=2)5. And cex is the size of the points.\n5¬†For many years I had a piece of paper hanging over my desk with all the pch values written on it and symbols scrawled with pencil next to them.Legend drawing is a completely separate function that slaps the legend on top of the existing plot, whatever it is. The abline() function draws a line on the plot, also not really caring what was drawn before.\nGgplot2:\n\nlibrary(ggplot2)\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size=3) +\n  geom_abline(intercept = -2.8, slope = 1.1, color = \"red\") +\n  scale_color_viridis_d() +\n  labs(x = \"Sepal length\", y = \"Sepal width\", \n       title = \"The iris data set\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this example you see why ggplot2 is so popular. The code is much cleaner and does not require you calling separate functions for each task. The parameters have explicit names, easy to remember. Legend and abline are directly connected to the plot, and the color scale is chosen with a single command, scale_color_viridis_d(). There is no need to separately change colors for the legend and the plot. The theme_minimal() function changes a whole range of parameters such as the font face used or the background color to achieve a ‚Äúminimal look‚Äù. scale_color_viridis_d(), theme_minimal()\nNoteworthy is also the labs() function, which allows you to change the labes of the axes and the title of the plot in one go (you can also use individual functions like xlab(), ylab() and ggtitle()). labs(), xlab(), ylab(), ggtitle()\nHowever, some operations can be challenging. If you want your legend placed directly on top of the drawing in the right upper corner, you have to specify the coordinates manually. And if you want triangles‚Ä¶ well, you still have to use the numbers, for example using geom_point(shape=2) for triangles6.\n6¬†And here the punchline: I still have this piece of paper with symbols scrawled on it.7¬†You basically have to calculate the LOESS lines manually and then manually plot them with lines() and / or polygon().Then again, some complex operations are easy. For example, to add per-group LOESS lines to the plot with confidence intervals, you would simply add geom_smooth(method=\"loess\"). In base R, the same operation requires about a dozen lines of code7.\n\n\n\n\n\n\n\nExercise 5.5 (Ggplot2) Take the code for the ggplot2 plot above and make the following modifications:\n\nTry adding a geom_smooth() layer\nRemove scale_color_viridis_d() layer. What happens?\nAdd the following layer: scale_color_manual(values=colors_map). What happens?\nChange the color of all points to black (use the color parameter in geom_point())\nIf you wanted to show different shapes for different species rather than different colors, where would you change the plot? Hint: where is the species mentioned in the code?\n\n\n\n\n\n\n\n5.2.2 Esthetics and information channels\nThe last point in the exercise was sneaky. The answer is: you would need to change the aes() function call. Rather than use color=Species, you would have to use shape=Species.\nThe aes() function ‚Äì and the concept of plot esthetics ‚Äì is at the core of ggplot2. The idea is that a plot has different information channels, such as x position, y position, color, shape, size, etc. Each of these can be mapped or linked to a column in the input data frame. This is the job of aes(). The geom functions such as geom_point() then use these mapping to actually draw the plot.\nThis is why the layers (geom_point(), scale_color_viridis_d(), etc.) are being added to the plot using the + operator. Each layer adds another piece of information on how to display the plot. Then, the plot is displayed all in one go.\nThe assumption is also that this information is mapped automatically. You do not need (or should not need) to specify which precisely color which groups get, or which symbols are used for plotting. Rather, you chose a particular information channel (‚Äúuse different shapes depending on species‚Äù) and ggplot2 takes care of the rest.\nOf course, it is still possible to manually specify the colors, shapes etc. For example, and as you have seen in the previous exercise, you can use the same manual mapping of colors as above by using scale_color_manual(values=colors_map).\n\n\n5.2.3 Boxplots and violin plots\nFor continuous data, violin plots combined with boxplots or boxplots alone are the method of choice. In the following, we will use the sepal lengths of the iris dataset8. First, a boxplot. We have created a boxplot a moment ago using the R base function boxplot(). Now we will do the same with ggplot2. geom_boxplot()\n8¬†Not the doctored version we have been loading in the previous days, but the built-in original data set. If you have created an iris variable in your environment, type rm(iris) to remove it.\nlibrary(ggplot2)\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe thick line in the middle of the box is the median, the box itself goes from the lower quartile to the upper quartile (so its vertical size is the interquartile range), and the whiskers show the range of the data (excepting the outliers). The points shown are outliers. All in all, it is a non-parametric representation ‚Äì that is, we do not assume that the data is normally distributed and we can use it for any kind of continuous data.\nHowever, boxplots are still not perfect, as they do not show the actual distribution of the data. Here is a better method ‚Äì we create a so-called violin plot which extrapolates a distribution from the data points.  In addition, we overlay that with a boxplot to show the quartiles and outliers.geom_violin()\n\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_violin() + \n  geom_boxplot(width=0.1)\n\n\n\n\n\n\n\n\nFinally, in cases where we do not have too many data points we might want to show them directly on the plot (we can combine it with a boxplot or a violin plot). A particularly fine way of showing the actual points is implemented by the ggbeeswarm package and its geom_beeswarm() function. geom_beeswarm(), ggbeeswarm\n\nlibrary(ggbeeswarm)\nggplot(iris, aes(x=Species, y=Sepal.Length)) +\n  geom_boxplot(outlier.shape=NA) +\n  geom_beeswarm(alpha=.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow actual data\n\n\n\nIf possible, always strive to show the actual data points (with geom_beeswarm() or geom_jitter()) in addition to the summary statistics. If not, at least show the distribution (with geom_violin()).\n\n\n\n\n5.2.4 Boxplots and violin plots vs bar plots\nBar plots are quite common in scientific literature, despite the fact that they actually should be avoided in most scenarios (‚ÄúKick the Bar Chart Habit‚Äù 2014). Bar plots should actually only be used when showing count or proportion data, and the \\(y\\) axis in this case should always start at zero. In all the other applications, box plots or, better, violin plots are preferred.\nThe advantages of the box plots and violin plots over bar plots are evident when the data is not normally distributed. Let us construct a small example.\n\nn &lt;- 250\nx &lt;- rnorm(n, mean=20, sd=1)\ny &lt;- rbeta(n, 2, 22) * 20  + 18.6\n\nThe vectors x and y are normally distributed and beta distributed, respectively, and they have been on purpose manipulated such that the standard deviations calculated with sd() are the same, but the means differ a bit. This is why, on a bar chart, they would show a clear difference. Here is a ggplot2 code that produces a bar chart. Don‚Äôt worry too much about the syntax: it is here for demonstration purposes only, and you will rarely bar plots in practice. The function geom_bar() is the one responsible for the bar plots; geom_errorbar() adds the error bars; coord_cartesian() is here to limit the y-axis to a certain range. geom_bar(), geom_errorbar(), coord_cartesian()\n\ndf &lt;- data.frame(value=c(mean(x), mean(y)), \n                 group=c(\"x\", \"y\"),\n                 sd=c(sd(x)/sqrt(n), sd(y)/sqrt(n)))\nggplot(df, aes(x=group, y=value)) + \n  geom_bar(stat=\"identity\", width=0.5, fill = \"skyblue\") +\n  geom_errorbar(aes(ymin = value - sd, ymax = value + sd), width = 0.2) +\n  labs(x=\"Group\", y=\"Value\") +\n  coord_cartesian(ylim = c(19.5, 20.5))\n\n\n\n\n\n\n\n\nWow, that looks really different! Unfortunately, the above figure is a lie. It suggests something that it is not true.\nFirstly, instead of standard deviations, which show the true spread of the data, I used the standard error of the mean (SEM) equal to \\(\\frac{\\sigma}{\\sqrt{n}}\\), which shows the precision of the mean estimate and gets very small for large data sets (but it looks good on a figure). Secondly, the y-axis does not start at zero, which is not OK.\nAnd last but not least, one should not use a bar plot for continuous data, because it does not show their real distribution. Let us now produce a box plot and a violin plot.\nBefore, however, let me introduce yet another useful package, cowplot9. With this package, we can create composite plots that consist of several individual plots. Basically, first you create the ggplot plots and save them to a variable; then you use cowplot function plot_grid() to put the plots . cowplot\n9¬†There is also the newer patchwork package, which is more elegant and flexible, but the syntax requires a bit of getting used to.plot_grid()\n\nlibrary(cowplot)\ndf &lt;- data.frame(values = c(x, y), \n                 group = c(rep(\"x\", n), rep(\"y\", n)))\np1 &lt;- ggplot(df, aes(x=group, y=values)) +\n  geom_boxplot() +\n  labs(x=\"Group\", y=\"Value\")\n\n# violin plot\np2 &lt;- ggplot(df, aes(x=group, y=values)) +\n  geom_violin() +\n  geom_boxplot(width=0.1) +\n  labs(x=\"Group\", y=\"Value\")\n\n# plot_grid puts the different plots together \n# ncol=2 -&gt; two columns\n# labels=... -&gt; labels for the plots\nplot_grid(p1, p2, ncol=2, labels=c(\"A\", \"B\"))\n\n\n\n\n\n\n\n\nAs you can see, the violin plots show a completely different story. The group y only looks larger, because the mean is driven by the long upper tail of the distribution. The medians are practically identical (median of x is 19.96, median of y is 19.98), and the distributions largely overlap.\nSimilarly, if you were to run a t-test, which assumes normal distribution, the p-value would have been 0.0044; while in a Wilcoxon test, which does not assume normal distribution, the p-value would have been 0.15.\n\n\n5.2.5 Heatmaps\nA very common type of figure in high throughput data setting and one which is hard to achieve with other tools is the heatmap. There are numerous packages and functions (including the base R function heatmap()), however we will use the pheatmap() function from the pheatmap package. pheatmap()\nFirst, however, we need some data. For starters, we take our beloved iris data set; however we will chose only 10 flowers from each species. You haven‚Äôt learned the following idiom yet, but here is how to do it efficiently in tidyverse:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.3     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter()    masks stats::filter()\n‚úñ dplyr::lag()       masks stats::lag()\n‚úñ lubridate::stamp() masks cowplot::stamp()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\niris_sel &lt;- iris |&gt;\n            group_by(Species) |&gt;\n            slice_sample(n=10) |&gt;\n            ungroup()\ntable(iris_sel$Species)\n\n\n    setosa versicolor  virginica \n        10         10         10 \n\n\nThe group_by() function groups the data by the Species column, which means that the subsequent functions will affect each group separately. The slice_sample() is a specialized function only for this purpose ‚Äì it randomly selects rows from a data frame. Finally we remove the grouping with the ungroup() function. group_by(), slice_sample(), ungroup()\n\nlibrary(pheatmap)\niris_mtx &lt;- t(as.matrix(iris_sel[, 1:4]))\npheatmap(iris_mtx)\n\n\n\n\n\n\n\n\nAs you can see, we have first converted the first four columns of the matrix into a data frame, then transposed it (‚Äúflipped‚Äù so the rows become columns and vice versa) and finally plotted it with pheatmap(). pheatmap(),t()\nHowever, we do not see the species information on the plot. We can add it using a special data frame that contains the species information. We will also add a few more parameters to the pheatmap() function to make the plot more readable.\n\niris_species &lt;- data.frame(species=iris_sel$Species)\nfoo &lt;- pheatmap(iris_mtx, labels_col=iris_sel$Species,\n  color = colorRampPalette(c(\"blue\", \"white\", \"red\"))(100)\n)\n\n\n\n\n\n\n\n\n\n\n5.2.6 Output formats\nWe will not be spending time here on the details of how to create different output formats for plots; however, there is one thing that we want to mention.\nIn general, there are two types of graphical files: raster (bitmap) and vector graphics. Raster graphics are made of pixels, like photos, and are good for complex images. Vector graphics are made of lines and shapes, like drawings, and are good for plots.\nThe two graphics below look identical, however, in the HTML version of this book, the left one is a raster image, and the right one is a vector graphics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry magnifying these images ‚Äì right click, select ‚Äúopen image in a new tab‚Äù (or similar) and then zoom in. You will see that the left image gets pixelated, while the right-hand one remains sharp:\n\n\n\nRaster vs vector graphics\n\n\nThe advantage of vector graphics is not only that you can zoom in as much as you want and the image will not get pixelated. First and foremost, you can edit them using an appropriate tool. Many people use the commercial Adobe Illustrator for this task, but there are also free and open source tools like Inkscape. Both these latter tools will do the job in a scientific setting ‚Äì they will allow you to change the colors, the fonts, the line thickness, the text shown etc. of your plots.\nTherefore, if possible, you should always save your plots in a vector format. You can always convert them to a raster format later, but once a plot is in a raster format, converting it to a vector format is practically impossible. The only exception is when you have an image with millions of data points ‚Äì sometimes this may challenge your computer‚Äôs memory (vector graphics is more computationally intensive than raster graphics).\n\n\n\n\n\n\nVector graphics\n\n\n\nIf possible, use a vector graphic format for your plots. You can always convert them to a raster format later, while the opposite is not true.\n\n\nR can produce vector graphics in the PDF and in the SVG format. Both formats can be edited in Inkscape or Illustrator, but the SVG format is also suitable for HTML pages, because SVG is a standard related to HTML ‚Äì in fact, all the plots in this book are in SVG format.\nTo choose SVG or PDF format, you have the following options:\nUse Rstudio. In the right hand ‚ÄúPlots‚Äù tab on the lower right panel, you can click on ‚ÄúExport‚Äù and choose one of the available formats, including PDF or SVG.\nUsing R markdown or Quarto. You can either use global document options or per-chunk options.\nFor global options, include the following code at the beginning of your R markdown file, just after the header:\n```{r include=FALSE}\nknitr::opts_chunk$set(dev=\"svg\")\n```\nAlternatively, insert this into the header:\nknitr:\n  opts_chunk:\n    dev: svg\nIf you want to make sure that one specific chunk produces SVG, you can always set the option in the given chunk:\n```{r dev=\"svg\"}\nplot(1:10)\n```\nThe only problem with this method is that if you create a Word document, the output will invariably be a raster image in your document.\n\n\n\n\n\n\nWord and R plots\n\n\n\nWhen you create a Word document with Rmarkdown, never copy your plots from that file! Either create a standalone file (see below for instructions or use the ‚ÄúExport‚Äù button in Rstudio), or copy the SVG graphics from the HTML file.\n\n\nDirectly creating graphics in your script. If you want your script to produce a file with the plot, you can do that in one of the many ways. Two of them are shown below, one using svg(), and the other one using pdf() or cairo_pdf()10. Both commands need to be finalized with dev.off():\n10¬†The cairo_pdf() supports a wider range of characters, including Unicode characters, and allows font embedding. Long story short, use it if your fonts are garbled.\n# producing an SVG image file\nsvg(\"test.svg\", width=14, height=7)\nplot(1:10)\ndev.off()\n\nThis produces an SVG file ‚Äútest.svg‚Äù with the nominal size 14 x 7 inches.\n\n# produce a PDF file\npdf(\"test.pdf\", width=14, height=7)\nplot(1:10)\ndev.off()\n\nThis produces a PDF file ‚Äútest.pdf‚Äù with the nominal size 14 x 7 inches.\n\n\n\n\n\n\nRemember!\n\n\n\nUse SVG format for your plots whenever possible.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#basic-statistics-with-r",
    "href": "day5-visualization-and-statistics.html#basic-statistics-with-r",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.3 Basic statistics with R",
    "text": "5.3 Basic statistics with R\n\n5.3.1 Statistics with R\nR is a powerful tool for statistics. It has a staggering number of packages that can be used for statistical analysis. I would venture the guess that if someone somewhere came up with a statistical method, then there is an R package for it11.\n11¬†Almost the same goes for bioinformatics and visualization.One interesting fact about statistics is that is is way harder than programming in R. In fact, after some initial learning curve, you will find that it is much harder to understand, say, which statistical test you should use than how to actually run it in R. You will also see that it is harder to understand the plots that you have produced then to actually produce them.\nNonetheless, it takes a while to get into the ‚ÄúR mindset‚Äù of doing statistics. It is done very much differently than in UI-based software like Graphpad Prism or Excel.\n\n\n\n\n\n\n\nExercise 5.6 (P values) P-values are one of the most basic concepts in statistics, and part of the language of science ‚Äì it is hard to find a scientific paper without any p-values.\n\nTake a piece of paper or open a text editor and write down a one- or two-sentence explanation of what a p-value is.\nOnly when you have done this, read the section ‚ÄúClarifications about p-values‚Äù in this Wikipedia article. Did you get it right? If yes, you are in the minority of scientists.\n\n\n\n\n\n\n\n5.3.2 Descriptive statistics\nYou have already seen some basic statiscs in R (see Day 1, for example  Tip¬†1.1). Here is a quick reminder of how to get some basic statistics for a vector of numbers:Descriptive statistics\n\nx &lt;- rnorm(1000)\n\n# mean\nmean(x)\n\n[1] 0.04245759\n\n# median\nmedian(x)\n\n[1] 0.06697673\n\n# standard deviation\nsd(x)\n\n[1] 1.033466\n\n# Some basic statistics in one go\nsummary(x)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.96087 -0.69269  0.06698  0.04246  0.74825  3.19717 \n\n\nFor a vector of numbers, the summary() function returns a vector with six numbers. Four of them should be self-explanatory: the minimum, median, mean, and maximum. The other two are the first and the third quartile.\nThe first number is the 1st quartile, also called ‚Äúthe lower quartile‚Äù or  25th percentile, which means that 25% of the data are below that number. The second is the 75th percentile (or the upper quartile), which means that 75% of the data are below the number. Together, between the first and the second number, there are 50% of the data. You can check it for yourself:Quartiles\n\ns &lt;- summary(x)\nsum(x &lt; s[2]) / length(x)\n\n[1] 0.25\n\nsum(x &gt; s[5]) / length(x)\n\n[1] 0.25\n\nsum(x &gt; s[2] & x &lt; s[5]) / length(x)\n\n[1] 0.5\n\n\nOf particular interest is the so called interquartile range (IQR), which is the difference between the upper and lower quartile.  Just like the median is a robust, non-parametric measure of the ‚Äúcenter‚Äù of the data, the IQR is a robust, non-parametric measure of the ‚Äúspread‚Äù of the data. While median corresponds to the parametric mean, IQR corresponds to the standard deviation.Interquartile range\nIn non-normally distributed data, for example count data (when we count things) median and IQR are often more informative than mean and standard deviation. We will see that later when we come to visualizing data with bar plots.\nYou can get the IQR using the IQR() function: IQR()\n\nIQR(x)\n\n[1] 1.440937\n\n\nFor data frames, you can either use the summary() function or use the summary_colorDF function from the colorDF package (see Section 3.3.1 and Section 3.3.2).\n\n\n5.3.3 Simple tests\nMost everyday statistical tests are available in base R without the need for loading any additional packages. Here we will show you some of them.\nt-test. The Student‚Äôs test is one of the most common tests in all statistics. It compares two groups of data. In its simplest form, the function t.test() takes two vectors of numbers. t.test()\n\n# simulate two vectors with different means\na &lt;- rnorm(15, mean=1, sd=1)\nb &lt;- rnorm(15, mean=3, sd=1)\n\n# perform the t-test\nt.test(a, b)\n\n\n    Welch Two Sample t-test\n\ndata:  a and b\nt = -6.4119, df = 24.521, p-value = 1.129e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.491792 -1.279291\nsample estimates:\nmean of x mean of y \n0.8976566 2.7831983 \n\n\nThe actual output of the t.test() function is a list with a lot of information, but when printed to the console it shows a human-readable summary.\nIt is often useful to extract and format the p-value from the output of the t.test() function. You can save the t.test output to a variable and then access the element p.value:\n\nres &lt;- t.test(a, b)\nres$p.value\n\n[1] 1.128802e-06\n\n\nTo show the p-value in a more readable format, you can use the format.pval  function which converts the number to a string with the correct number of significant digits:format.pval(..., digits=2)\n\nformat.pval(res$p.value, digits=2)\n\n[1] \"1.1e-06\"\n\n\nThis allows us to include the p-value in a sentence, for example like this:\n\"The p-value of the t-test \nwas `‚Äãr format.pval(res$p.value, digits=2)`.\"\nThis will be rendered as: ‚ÄúThe p-value of the t-test was 1.1e-06.‚Äù\nIf we doo many tests or want to save the test results in a spreadsheet, it might be useful to use the tidy() function from the broom package. tidy()\n\nlibrary(broom)\ntidy(res)\n\n# A tibble: 1 √ó 10\n  estimate estimate1 estimate2 statistic    p.value parameter conf.low conf.high method                  alternative\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;      \n1    -1.89     0.898      2.78     -6.41 0.00000113      24.5    -2.49     -1.28 Welch Two Sample t-test two.sided  \n\n\nThe tidy() function returns a data frame with the test results. It understands many different statistical tests, not only the t-test.\n\n\n\n\n\n\n\nExercise 5.7 (T-test) Use either the builtin iris dataset or the cleaned-up version of the doctored data set from Day 3. Perform a t-test to compare the sepal length between Iris setosa and Iris versicolor, as well as between Iris versicolor and Iris virginica.\n\n\n\n\nWilcoxon test. The Wilcoxon test is a non-parametric version of the t-test, and almost as powerful. Use it via wilcox.test(): wilcox.test()\n\nwilcox.test(a, b)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  a and b\nW = 12, p-value = 3.507e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nAs you can see, the p-value is quite similar to the t-test, but we did not have to worry whether the data is normally distributed or not12.\n12¬†The t-test assumes normality, while the Wilcoxon test does not. However, despite that, the t-test is quite robust to non-normality, which means that it will work quite well even if the data is not quite normally distributed.Paired tests. Both the t-test and the Wilcoxon test can be used in a paired version. The paired variant is a special case of the regular t-test or Wilcoxon test where  the two groups are not independent. For example, you might have measured the same individuals before and after a treatment. You are not allowed to use a regular test if the data is paired, because one of the most fundamental and important assumptions is not met ‚Äì the assumption of independence. Also, in many cases, using a paired test will give you more statistical power, as we will see shortly.Paired t-test with paired=TRUE\nConsider this example. We first start with randomized vector a, then build vector b by adding a fixed effect and an error term:\nboxplot()\n\na &lt;- rnorm(15, mean=1, sd=1)\nb &lt;- a + 0.5 + rnorm(15, mean=0, sd=.5)\nboxplot(a, b)\n\n\n\n\n\n\n\n\nWe see on the boxplot that the groups, if treated independently, are not really different. However, we can visualize it much better using a paired plot, in which a line is connecting the two values for each individual. We will use ggplot2 for that purpose:\n\ndf &lt;- data.frame(value=c(a, b),\n                 group=rep(c(\"A\", \"B\"), each=15),\n                 id=1:15)\nggplot(df, aes(x=group, y=value, group=id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nThe new parameter here is the group aesthetic, group, which tells ggplot2 that the points which have identical id belong together. The geoms, such as geom_line() which connects data points with lines, work group-by-group, and therefore lines connect only points within one group. geom_line(), group aesthetic\nOn this plot we can clearly see that for almost every individual, the value in the second measurement (group B) is higher than in the first measurement (group A). We can confirm this with a paired t-test:\n\nt.test(a, b, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  a and b\nt = -3.8378, df = 14, p-value = 0.001811\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.6702263 -0.1896676\nsample estimates:\nmean difference \n      -0.429947 \n\n\nThe paired=TRUE argument tells R that the two vectors are paired. We get a p-value of 0.0018, whereas with regular t-test we would have gotten a p-value of 0.3. Similarly, if you were to run the Wilcoxon paired test, the p value would be 0.0034, whereas in the regular Wilcoxon test it would be 0.27.\n\\(\\chi^2\\) (Chi-square) test. If we have two categorical vectors, we can  use the \\(\\chi^2\\) test. Consider the results of the gene expression analysis we have looked at yesterday. We can define two vectors: ‚Äúsignificant/non-significant‚Äù and ‚Äúinterferon/noninterferon‚Äù. These can be logical or character vectors, it doesn‚Äôt matter:\\(\\chi^2\\) test\n\nlibrary(tidyverse)\ntr_res &lt;- read_csv(\"Datasets/transcriptomics_results.csv\")\ntr_res &lt;- select(tr_res, \n                 Gene=GeneName, Description,\n                 logFC=logFC.F.D1, FDR=qval.F.D1)\ninterferon &lt;- grepl(\"interferon\", tr_res$Description)\nsignificant &lt;- tr_res$FDR &lt; 0.01\ntable(significant, interferon)\n\n           interferon\nsignificant FALSE  TRUE\n      FALSE 40725    59\n      TRUE    834    21\n\n\nAbove we constructed a contingency table. Rows show the two  values of significant vector, columns ‚Äì same for the interferon vector. In other words, most (40725) genes are neither significant nor interferons. However, out of 855 significant genes, as much as 21 have ‚Äúinterferon‚Äù in their description. And vice verse, out of the 80 interferon genes, more than a third are significant. Is this significant? To answer this, we can use the chisq.test(). Contingency table with table(x, y)chisq.test()\n\nchisq.test(significant, interferon)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  significant and interferon\nX-squared = 221.44, df = 1, p-value &lt; 2.2e-16\n\n\nYep, looks like it. In fact, the above was a simple case of gene set enrichment analysis.\nWith this, we conclude this simple statistics part. If you are interested in more, take a look at the Appendix: more stats and visualizations ‚Äì I have included a few quite common statistical applications there.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#conclusions",
    "href": "day5-visualization-and-statistics.html#conclusions",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.5 Conclusions",
    "text": "5.5 Conclusions\n\n5.5.1 Where to go from here\nThere are numerous resources for R on the web, allow me to recommend a few.\n\nStackOverflow ‚Äì this is a general Q & A forum for programmers and other like them. Most of the questions you have someone has previously asked here, including many by yours truly.\nOnline R books\n\nHands-On programming with R A very different approach to teachin R to beginners than the one that I prefer, but maybe you will find it better. Worth checking in any case.\nR for data science (tidyverse) The tidyverse book, written by the main tidyverse developer.\nR graphics cookbook The cookbook format splits the information in small ‚Äúrecipies‚Äù for common problems.\nR markdown: the definitive guide A comprehensive guide to R markdown written by the people who created it.\n\nOther books:\n\nThe R Book by Michael J. Crawley ‚Äì this is a magnificent book on statistics. It uses very conservative R language (no tidyverse at all), but it discusses at length even the more complex statistical issues. I recommend this book to every person willing to learn statistics with R.\n\nLarge Language Models (LLMs): I have good experiences with LLMs (ChatGPT, Perplexity AI) for learning programming languages. While complex tasks may be out of reach for them, they are very good at explaining basics. ‚ÄúHow do I do X in R?‚Äù or ‚ÄúWhy doesn‚Äôt the following code work?‚Äù seem to work quite well. RStudio allows you also to use Copilot which is an LLM model that watches the code you write and tries to guess what you are trying to do. Just remember, that the one thing that LLMs don‚Äôt know how to say is ‚ÄúI don‚Äôt know‚Äù. If something doesn‚Äôt make sense, or if it is beyond the scope of their learning, they will ‚Äúhallucinate‚Äù ‚Äì give good sounding advice which is totally bonkers.\n\n\n\n5.5.2 Famous last words\nWe are now at the end of Five Days of R. You are basically on the beginning of your journey. Here is my last advice to you.\n\n\n\n\n\n\nGet on with R\n\n\n\nStart working with R, right now, for all your projects; for statistics, data management and even preparing reports and documents.\nAt first, doing the same task in R will take much more time then the same task in programs you used before. You will feel that you are wasting your time.\nYou are not. Sooner then you think, it will pay off.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#review",
    "href": "day5-visualization-and-statistics.html#review",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.6 Review",
    "text": "5.6 Review\n\nR markdown / Quarto\n\ncreating R markdown documents\nbasic markdown formatting (bold, italic etc.)\nincluding R chunks\ncreating output in different formats\nchoosing an output format\n\nVisualisations\n\nbasic scatterplots with ggplot()\nunderstanding esthetics\nbox plots with geom_boxplot() and violin plots with geom_violin()\n\n\n\n\n\n\n‚ÄúKick the Bar Chart Habit.‚Äù 2014. Nature Methods 11: 113. https://doi.org/10.1038/nmeth.2837.\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022. ‚ÄúVenn Diagrams May Indicate Erroneous Statistical Reasoning in Transcriptomics.‚Äù Frontiers in Genetics 13: 818683. https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.818683/full.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions to Exercises",
    "section": "",
    "text": "The merging of two data frames\nSolution to Exercise¬†4.9.",
    "crumbs": [
      "Introduction",
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#pca-and-scatter-plots",
    "href": "day5-visualization-and-statistics.html#pca-and-scatter-plots",
    "title": "5¬† R markdown, basic statistics and visualizations",
    "section": "5.4 PCA and scatter plots",
    "text": "5.4 PCA and scatter plots\n\n5.4.1 Principal component analysis\nIn this last section we will combine a lot of things (and introduce a few new concepts, despite it being Friday afternoon13).\n13¬†Assuming, of course, that you started this course on a Monday‚Ä¶One of the basic plots in statistics is the scatter plot, with two continuous variables. They are easy enough to generate in R, but we will do it with a twist, introducing you to a new statistical concept: principal component analysis (PCA). PCA, Principal component analysis\nRember how we plotted the iris data set two days ago (Section 3.5.5)? It was easy to see the differences between the groups, and if we wanted, we could have plotted all four variables on two plots. But what if we had thousands of variables? For example, expression of thousands of genes for hundreds of samples?\nOne of the possible approaches is to use a technique like PCA. PCA converts the data replacing the original variables with a new, smaller set, that however covers all the variance in the data. While the old variables are clearly defined, the new ones, called ‚Äúprincipal components‚Äù, result from combining them, so they do not correspond to something specific14. However, they have a few nice properties:\n14¬†Components are the linear combinations of the original variables. So for a given sample, we basically add up expression of all genes, but each gene has a different weight.\nMost of the variance of the samples sits in the first components, then the next biggest share sits in the second etc. That means that by looking at the first few we are likely to get a very good idea of the overall differences in the data.\nThe components are orthogonal, that is, they are not correlated with each other. This means that if, say, two groups correspond to one component, then it is quite likely that they will only correspond to that component. And thus we are able to say: ‚Äúthis component explains differences between treatments, and this explains gender‚Äù.\nThe analysis is unsupervised, that is, we do not need to tell the algorithm which samples belong to which group. It will find the differences on its own (if there are any).\n\n\n\n5.4.2 The data\nIn the following, we will be using a set of measurements of laboratory values (such as white blood cell count, hemoglobin, etc.) from a clinical vaccination study. There were three treatments - placebo and two vaccines, one with an adjuvant (Fluad) and one without (Agrippal), and samples were taken on different time points, starting with a screening time point and base line, then on to D0, D1 etc. The data is in the wide format, that is, each row corresponds to a single sample ‚Äì that is, a single patient and a single time point, and each column corresponds to a single measurement.\n\nlibrary(tidyverse)\nlabdata &lt;- read_csv(\"Datasets/labresults_wide.csv\")\ndim(labdata)\n\n[1] 1732   31\n\nhead(labdata[,1:10])\n\n# A tibble: 6 √ó 10\n  SUBJ.TP       ACA   ALB   ALP   ALT  BILI    CA CREAT   CRP   GGT\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 4485.SCREEN  2.25  45.9  79.5  25.4 14.1   2.37  81.4 0.195  28.8\n2 4485.BL      2.28  44.1  77.7  24.4  9.81  2.35  77.8 0.439  25.6\n3 4485.D0      2.28  44.0  78.2  20.7 15.2   2.36  81.4 0.372  27.5\n4 4485.D1      2.26  41.7  75.2  22.6 23.9   2.30  76.3 0.224  25.1\n5 4485.D2      2.18  39.9  66.4  22.9 20.6   2.18  75.1 0.202  20.8\n6 4485.D3      2.17  38.0  62.2  21.3 18.1   2.11  70.6 0.263  22.3\n\n\nThe last line above uses head() to show the first few lines of the data frame, and with [,1:10] we limit the output to the first ten columns15.\n15¬†If you want to know more on what the abbreviations mean, take a look at the labresults_full.csv file. This is the down-side of the wide format: no place to store extra information about the columns!As you can see, the first column identifies the sample by giving both the subject and the time point. In the following, we only want one time point (D1), and we could filter the data set by combining filter() with a suitable grepl() call, for example, filter(labdata, grepl(\"D1$\", TP)). Instead, we will use this as an opportunity to show you the separate() function. separate()\n\nlabdata &lt;- labdata |&gt;\n  separate(SUBJ.TP, into=c(\"SUBJ\", \"TP\"), sep=\"\\\\.\") |&gt;\n  filter(TP == \"D1\")\n\nThe separate() function takes a column name, the names of the new columns and a separator. The separator is a regular expression, and since the dot normally matches any character, we need to escape it with two backslashes. The resulting data frame has two columns named SUBJ and TP instead of one called SUBJ.TP, and we could directly filter the data frame for the desired time point.\nThere is one more thing that we need to take care of, unfortunately. There are some NA values in the data set, and we need to remove them. Rather than figure out how to handle them, we will simply remove all samples that have a missing value anywhere in the data set with one function: drop_na()\n\nlabdata &lt;- labdata |&gt; drop_na()\n\nThe problem we are facing now is that the data set does not contain any interesting meta-data, like any information about the actual treatment group! Lucky for us, we have that information in another file. The expression_data_vaccination_example.xlsx file contains the meta-data (as well as matching RNA expression data, but we will not use that here for now).\nWe will combine both data sets using an inner join with merge().\n\n# read the meta-data\nlibrary(readxl)\nmeta &lt;- read_excel(\"Datasets/expression_data_vaccination_example.xlsx\",\n                   sheet=\"targets\") |&gt;\n                   filter(Timepoint == \"D1\")\n                   \ncombined &lt;- merge(meta, labdata, by=\"SUBJ\")\nhead(combined[,1:10])\n\n  SUBJ          USUBJID Batch       ID Timepoint ARMCD      ARM AGE SEX PLACEBO\n1 4012 CRC987X-4012-439     A P4012.D1        D1     P  PLACEBO  30   M    TRUE\n2 4055  CRC987X-4055-89     B P4055.D1        D1     P  PLACEBO  30   F    TRUE\n3 4061   CRC987X-4061-3     B P4061.D1        D1     P  PLACEBO  19   M    TRUE\n4 4125 CRC987X-4125-392     C F4125.D1        D1     F    FLUAD  27   F   FALSE\n5 4176  CRC987X-4176-50     C A4176.D1        D1     A AGRIPPAL  18   M   FALSE\n6 4182 CRC987X-4182-194     C A4182.D1        D1     A AGRIPPAL  23   M   FALSE\n\ndim(combined)\n\n[1] 60 41\n\n\n\n\n\n\n\n\nCheck your results!\n\n\n\nCheck your results frequently. Do you get the number of rows you expect? How does the data look like? Use dim(), head(), tail(), summary(), View() all the time.\n\n\nTo run the PCA, we need, however, only the numeric columns. If you take a look at the combined data frame with, for example, View, you will see that we need the columns from ACA (adjusted calcium) to WBC (white blood cell count). We will select these columns and convert the resulting data frame to a matrix.\n\ncombined_mtx &lt;- select(combined, ACA:WBC) |&gt; \n  as.matrix()\nhead(combined_mtx[,1:5])\n\n          ACA      ALB      ALP      ALT      BILI\n[1,] 2.157274 40.12904 43.91842 32.11683 11.789734\n[2,] 2.152719 39.66053 57.46538 11.88092  8.990701\n[3,] 2.120058 39.25109 39.78216 18.20158 12.219731\n[4,] 2.099532 37.99078 68.88891 35.78890 10.166375\n[5,] 2.219020 42.96763 52.52566 21.36904 31.142545\n[6,] 2.102563 40.88200 63.15989 19.91463 15.447636\n\n\n\n\n5.4.3 Running the PCA\nThe PCA is a built-in function in R, and it is called prcomp(). We will only use a single parameter, scale. (mind the dot!) to tell PCA to scale the gene expression before working its magic: prcomp()\n\n# the actual PCA is just one line!\npca &lt;- prcomp(combined_mtx, scale.=TRUE)\nis.list(pca)\n\n[1] TRUE\n\nnames(pca)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nThe object returned by prcomp() is a list, and the element that we are particularly interested in is called x. This is a matrix of the principal components, PC1, PC2 etc. There are quite a few principal components, and each one has as many elements as there are samples in the data set:\n\ncolnames(pca$x)[1:10]\n\n [1] \"PC1\"  \"PC2\"  \"PC3\"  \"PC4\"  \"PC5\"  \"PC6\"  \"PC7\"  \"PC8\"  \"PC9\"  \"PC10\"\n\ndim(pca$x)\n\n[1] 60 30\n\ndim(combined)\n\n[1] 60 41\n\n\nAs you can see, the number of rows of pca$x is the same as the number of samples in the combined data frame. In other words, for each sample, we have a bunch of new values that are the principal components.\nWe won‚Äôt be interested in more than the first few components, but we want to see them in connection with the covariates. Therefore, we will use cbind() to combine the combined data frame with the first few columns of the pca$x matrix. We will use the ggplot2 package to plot the data. cbind()\n\nlibrary(ggplot2)\npca_df &lt;- cbind(combined, pca$x[,1:10])\nggplot(pca_df, aes(x=PC1, y=PC2, color=SEX)) +\n  geom_point(cex=3)\n\n\n\n\n\n\n\n\nOn the plot above, we have plotted the first two principal components, PC1 and PC2. You can clearly see that the first component separates the samples by sex: males have a different value of PC1 then females. We can see it clearly on a boxplot: geom_boxplot()\n\nggplot(pca_df, aes(x=SEX, y=PC1)) +\n  geom_boxplot(outlier.shape=NA) +\n  geom_jitter()\n\n\n\n\n\n\n\n\nThe geom_jitter() above allows us to see all the individual samples ‚Äì you will see a better way later today. However, because geom_boxplot() also puts a dot where outliers are, we use outlier.shape=NA to suppress them. geom_jitter()\n\n\n\n\n\n\n\nExercise 5.8 (Vaccines) Repeat the plot above, but instead of SEX, use the ARM column which corresponds to the study arm, i.e.¬†which vaccine (or placebo) was administered. Can you tell the groups apart? Try it with other components: PC3 vs PC4, PC5 vs PC6 etc. Any luck?\n\n\n\n\n\n\n5.4.4 Interpreting the PCA\nOK, but what does that mean? How does the PCA ‚Äúknow‚Äù who is male, and who is female? Where does this information come from?\nAs mentioned before, every principal component is derived from the actual values of the different variables. The values are summed up for each component, but not every variable contributes equally. We can wee how each value contributes to the component by directly looking at another member of the pca object, rotation. Again, it is a matrix, but now each row corresponds to a variable, and each column to a principal component:\n\nhead(pca$rotation[,1:5])\n\n             PC1          PC2         PC3         PC4         PC5\nACA  -0.17183825  0.323292082 -0.12523650  0.09240258 -0.03555072\nALB  -0.26540870  0.037798385 -0.21227924 -0.18413045 -0.07107573\nALP  -0.08486295  0.142972159  0.03721116 -0.19312135  0.02374674\nALT  -0.12616405  0.004200436  0.16325618  0.08350328  0.43320837\nBILI -0.16455543 -0.239674253 -0.03222498 -0.25938783 -0.09745293\nCA   -0.25968381  0.278220386 -0.19243053 -0.01613003 -0.07507028\n\n\nThese numbers are loadings, that is, the weights of each variable in the calculation of the given principal component. If the loading is close to 0, then the given variable does not contribute much to the component. If, however, it is very large, or very small (negative), then the variable contributes a lot.\nTo understand why the male samples are separated from the female samples on the plot above, we will focus on PC1 and sort it by the decreasing absolute loadings.\n\npc1 &lt;- pca$rotation[,1]\nord &lt;- order(abs(pc1), decreasing=TRUE)\nhead(pc1[ord])\n\n       HGB        HCT        RBC        ALB         CA      CREAT \n-0.3647368 -0.3386385 -0.3005406 -0.2654087 -0.2596838 -0.2590510 \n\n\nFirst value that we find is HGB with a value of -0.36. The fact that it is negative means that when the actual value of the variable is high, the resulting PC1 will be low and vice versa. That is, samples with a high value of this variable will be likely to be shown on the left side of the plot, and samples with a low value on the right side.\nNow, male samples are on the left side of the plot, and female samples on the right side. What is HGB? It turns out that it corresponds to hemoglobin. Since the loading is low, it means that males should have a low value of hemoglobin compared to females. We can check it with a boxplot:\n\n\n\n\n\n\n\n\n\nIndeed, that seems to be the case! And, of course, it makes perfect sense biologically.\n\n\n\n\n\n\n\nExercise 5.9 (Other top loadings) In the table below, you will find three more variables. Check their loadings. Make corresponding box plots and check if the values correspond to what you know about human biology.\n\n\n\nVariable\nDescription\n\n\n\n\nESR\nErythrocyte sedimentation rate\n\n\nCREAT\nCreatinine\n\n\nCA\nCalcium\n\n\n\n\n\n\n\nThe purpose of this exercise was not only to show you the mechanics of PCA. This is a useful method, and quite often it is worthwile to run it before you do anything else with the data. PCA shows you the possible groupings even before you even start to think about the actual analysis.\nHowever, another point of this whole story was to demonstrate an important aspect of bioinformatic analyses. Think how much work it was to pummel the data into a shape which was suitable for the PCA, and how much work to actually figure out what it means ‚Äì compared with the actual analysis, which was just a single line of code. This is a common situation in bioinformatics: the actual analysis is easy, but the data preparation and interpretation are hard and time consuming.\n\n\n\n\n\n\n\nExercise 5.10 (Top loadings for vaccines) Which PC is the most important for separating the vaccine groups? Which variables contribute the most to this component? Can you explain why?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>R markdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "appendix-more-stats.html",
    "href": "appendix-more-stats.html",
    "title": "Appendix: More statistics and visualizations",
    "section": "",
    "text": "Correlations\nCorrelation is a measure of how two variables change together. There are many different variants, the most popular being the Pearson correlation and the Spearman correlation. Both can be calculated using the cor() function in base R and tested for significance using the cor.test(). Here they are in action on the iris dataset:\n\ncor(iris$Sepal.Length, iris$Petal.Length)\n\n[1] 0.8717538\n\ncor.test(iris$Sepal.Length, iris$Petal.Length, method=\"spearman\")\n\nWarning in cor.test.default(iris$Sepal.Length, iris$Petal.Length, method = \"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nS = 66429, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.8818981 \n\n\nThe warning message above indicates that there are duplicated values in the data, which makes the Spearman correlation test less reliable.\nThe cor() function can also calculate the correlation matrix ‚Äì that means, correlate each variable with each. This is useful for visualizing the relationships between variables. Here is how you can do it:\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\nThe diagonal of the matrix is always 1, because a variable is always perfectly correlated with itself. The matrix is symmetric, because the correlation between \\(x\\) and \\(y\\) is the same as the correlation between \\(y\\) and \\(x\\). We can visualize this matrix using pheatmap:\n\nlibrary(pheatmap)\nM &lt;- cor(iris[,1:4])\npheatmap(M, scale=\"none\")\n\n\n\n\n\n\n\n\nThe scale=\"none\" parameter is used to avoid scaling the data by rows or by columns ‚Äì it does not make sense for a symmetrical matrix.\nAs you can see, the one oddball in the iris dataset is the Sepal.Width variable, which is not very correlated with the other three.\nThere is another package to visualize correlation matrices, called corrplot. It is more flexible and can be used to visualize the correlation matrix in many different ways. Here is an example:\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(M, method=\"color\")\n\n\n\n\n\n\n\n\nThere are many cool ways how corrplot can visualize the correlation matrix. You can check them out in the documentation of the package.\n\ncorrplot(M, method=\"ellipse\", \n         type=\"upper\", tl.pos=\"d\")\ncorrplot(M, add = TRUE, type = 'lower', \n         method = 'number',\n         col=\"black\", diag = FALSE, \n         tl.pos = 'n', cl.pos = 'n')\n\n\n\n\n\n\n\n\n\n\nCorrecting for multiple testing\nWhen you run multiple tests, you increase the chance of finding a false positive. If two data sets do not differ, and you run a test a 100 times, on average 5 of those tests will show a significant difference at the 0.05 level. This is called the multiple testing problem.\nTherefore, in order to trust the results of your tests, you need to correct for multiple testing. There are basically two main approaches to this:\n\nFamily wise error rate (FWER) correction, which controls the chance of making at least one false positive. Basically, we want the corrected p-values to mean just what the regular once do ‚Äì the probability of making a false positive. The most popular method for this is the Bonferroni correction, which divides the significance threshold by the number of tests.\nFalse discovery rate (FDR) correction, which controls the proportion of false positives among all significant results. The most popular method for this is the Benjamini-Hochberg (BH) correction.\n\nThe former is very conservative, which means that while indeed you make sure that the corrected p-values are what you think they are, you are also introducing a huge number of type II errors - false negatives.\nThe BH correction is more relaxed, and is often used in high-throughput experiments in biology. Since it is not really a p-value it is good to refer to it as a q-value or FDR value. An FDR of, say, 0.05 means that among the results which have an FDR of 0.05 or less, at most 5% are expected to be false positives.\nBoth of these corrections can be done with the p.adjust() function in R. Say, we make a number of comparisons using the iris dataset:\n\nlibrary(tidyverse)\nsv &lt;- iris |&gt; filter(Species != \"setosa\") |&gt;\n  mutate(Species=factor(Species))\npvals &lt;- sapply(1:4, function(i) {\n  t.test(sv[,i] ~ sv$Species)$p.value\n})\nnames(pvals) &lt;- colnames(sv)[1:4]\npvals\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n1.866144e-07 1.819483e-03 4.900288e-22 2.111534e-25 \n\n\nOK, so a lot has happened above that you have not seen before. First, why do we convert Species to a factor? The reason is that the Species is already a factor in the original data frame, but it has three levels: ‚Äúsetosa‚Äù, ‚Äúversicolor‚Äù, and ‚Äúvirginica‚Äù. When we filter out the ‚Äúsetosa‚Äù species, the levels remain unchanged, and the t.test function will complain that we have too many groups. Therefore, we need to convert the Species to a factor with only two levels.\nThe sapply function is a way to apply a function to each value of a vector or list. Here, we apply an anonymous function, that is, defined without giving it a name, to every value in the vector 1:4. The anonymous function takes as parameter a single value from the vector, and returns the p-value of the t-test between the corresponding column of the sv data frame and the Species variable.\nAnother new thing that you have not seen previously is the ~ sign. Rather than running a t-test on two vectors, we run it on a formula. We will cover formulas in a moment, but basically here it means for the t-test that the Species vector defines the groups, while column sv[,i] defines the variable to be tested.\nWe run 4 comparisons, and assuming that there were no differences between the species, we would expect 5% of the tests to be significant at the 0.05 level ‚Äì which means that the probability of having at least one false positive in 4 tests is \\(1 - (1 - 0.05)^4 = 0.185\\). Let‚Äôs see if we can do something about it:\n\np.adjust(pvals, method=\"bonferroni\")\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n7.464578e-07 7.277934e-03 1.960115e-21 8.446138e-25 \n\np.adjust(pvals, method=\"BH\")\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n2.488193e-07 1.819483e-03 9.800575e-22 8.446138e-25 \n\n\nAs you can see, the corrected p-values are higher than the original ones, and the BH correction is less conservative (i.e., the p-values are smaller) than the Bonferroni correction.\n\n\nSimple linear models\nWe end with an example of linear regression, and the most important reason for that is the introduction of formulas in R.\nA formula is a weird little construct. It contains variables, and links them using the ~ (tilde) sign. On the left side of the ~ are the dependent variables (the ‚Äúy‚Äù), on the right side are the independent variables (the ‚Äúx‚Äù, or covariates).\nDepending on the particular function, the formula can mean different things and will have different syntax. For example, in a package like the DESeq2 from Bioconductor, there will be nothing on the left side ‚Äì because DESeq2 understands that the formula applies to every single gene in the input matrix.\nHere we will use the lm() function, which is the basic linear regression  included in base R. Somewhat similar to tidyverse, you can use column names of a data frame in the formula, and specify the data frame with the data parameter. We will use it to model regression of the mathematical formlm()\n\\[ y = a + b \\cdot x + \\epsilon \\]\nwhere \\(a\\) is the intercept, \\(b\\) is the slope, and \\(\\epsilon\\) is the error.\nAs an example, we will use the mtcars dataset, which contains information about, you guessed it, cars (it is quite old ‚Äì it comes from 1974). In the data frame, there are two columns that we will use: mpg (miles per gallon, so fuel usage given in the american way), and hp (horsepower). We will try to predict the miles per gallon based on the horsepower. However, rather than model mpg, we will use its inverse ‚Äì gallons per mile, gpm, multiplied by 100 (so, effectively, gallons per 100 miles).\n\nmtcars$gpm &lt;- 100/mtcars$mpg\nmodel &lt;- lm(gpm ~ hp, data=mtcars)\n\nThe lm() function returns a model object, which contains a lot of information of no immediate use for us. To actually know the coefficients and p-values, it‚Äôs best to use either the summary() function, or the tidy() function from the broom package.\n\nsummary(model)\n\n\nCall:\nlm(formula = gpm ~ hp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.19776 -0.56724 -0.07017  0.24239  3.12691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.741786   0.456521   6.006 1.37e-06 ***\nhp          0.018277   0.002827   6.464 3.84e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 30 degrees of freedom\nMultiple R-squared:  0.5821,    Adjusted R-squared:  0.5682 \nF-statistic: 41.79 on 1 and 30 DF,  p-value: 3.839e-07\n\n\nWe have two rows in the ‚ÄúCoefficients‚Äù table, because we had two coefficients in our model: \\(a\\), the intercept, and \\(b\\), the slope. The \\(b\\) coefficient answers the question: how much more miles per gallon do we get if we reduce horse power by 1?\nWe can plot how the model fits our data with ggplot2:\n\nlibrary(ggplot2)\na &lt;- coefficients(model)[1]\nb &lt;- coefficients(model)[2]\nggplot(mtcars, aes(x = hp, y = gpm)) +\n  geom_point() +\n  geom_abline(intercept = a, slope = b)\n\n\n\n\n\n\n\n\n[coefficients()]\nThe coefficients() function extract the vector with the coefficients from the model.\nBut wait. The intercept, \\(a\\), is the fuel usage when the car‚Äôs horsepower is 0. Logically, the fuel usage of a car with 0 horsepower should be precisely 0, and not almost 3. Any value other than 0 simply doesn‚Äôt make sense. We can tell lm() that the intercept should be 0 quite easily:\n\nmodel_0 &lt;- lm(gpm ~ 0 + hp, data=mtcars)\nsummary(model_0)\n\n\nCall:\nlm(formula = gpm ~ 0 + hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6238 -0.5268  0.9824  1.4068  2.7063 \n\nCoefficients:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nhp 0.033703   0.001725   19.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.576 on 31 degrees of freedom\nMultiple R-squared:  0.9249,    Adjusted R-squared:  0.9225 \nF-statistic: 381.7 on 1 and 31 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see, we have now only one coefficient ‚Äì because we forced the other one to be 0 with the 0 + syntax.\nThe nice thing about this type of approach is that it can be easily extended to model much more complex situations. For example, what else does the fuel usage depend on? One of the columns in the mtcars dataset is the weight of the car. We can add it to the model like this:\n\nmodel_2 &lt;- lm(gpm ~ 0 + hp + wt, data=mtcars)\nsummary(model_2)\n\n\nCall:\nlm(formula = gpm ~ 0 + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0183 -0.4441  0.1447  0.5905  1.1068 \n\nCoefficients:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nhp 0.007443   0.002364   3.148   0.0037 ** \nwt 1.330059   0.113669  11.701 1.05e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6791 on 30 degrees of freedom\nMultiple R-squared:  0.9865,    Adjusted R-squared:  0.9856 \nF-statistic:  1096 on 2 and 30 DF,  p-value: &lt; 2.2e-16\n\n\nAgain we are using the 0 + syntax to force the intercept to be 0 ‚Äì which makes sense, since a car with no horsepower and no weight should use no fuel. The summary() function shows us that the weight of the car is also significant in predicting the fuel usage, although the \\(p\\)-value for the hp coefficient is now much higher. Well, there is a correlation between horsepower and weight.\nBut which model is better? If you look at the summaries above, you will find that the R-squared value is given. This is a measure of how well the model fits the data. The closer it is to 1, the better the model. For model_0, the R-squared is 0.92, and for model_2 it is 0.99.\nHowever, adding more variables to the model will always increase the fit, leading to the situation we call overfitting, because while increasing the fit to this particular dataset we will be decreasing the models predictive power.\n\nmodel_huge &lt;- lm(gpm ~ 0 + hp + wt + qsec + drat + disp + cyl, data=mtcars)\nsummary(model_huge)\n\n\nCall:\nlm(formula = gpm ~ 0 + hp + wt + qsec + drat + disp + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6976 -0.4062  0.1508  0.3457  1.3327 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)   \nhp    0.004814   0.004045   1.190  0.24479   \nwt    1.031644   0.329317   3.133  0.00425 **\nqsec -0.011301   0.072827  -0.155  0.87788   \ndrat  0.201028   0.272197   0.739  0.46680   \ndisp  0.002103   0.003253   0.646  0.52370   \ncyl   0.062671   0.164361   0.381  0.70608   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6913 on 26 degrees of freedom\nMultiple R-squared:  0.9879,    Adjusted R-squared:  0.9851 \nF-statistic:   353 on 6 and 26 DF,  p-value: &lt; 2.2e-16\n\n\nOne way we can avoid overfitting is by using another measure of model fit, AIC (Akaike Information Criterion). The AIC() function calculates the AIC for a model, which is a measure of how well the model fits the data, but penalizes the number of parameters. The lower the AIC, the better the model.\n\nAIC(model_0)\n\n[1] 122.8965\n\nAIC(model_2)\n\n[1] 69.97504\n\nAIC(model_huge)\n\n[1] 74.53647\n\n\nIn the above examples we have been using continuous variables, but we can use almost anything with linear models. For example, we can ask how the Sepal.Length of the iris dataset depends on the Species:\n\nmodel_iris &lt;- lm(Sepal.Length ~ Species, data=iris)\nsummary(model_iris)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nIn fact, the above model is equivalent to an ANOVA test. The individual \\(p\\)-values above are actually not of immediate interest, since in ANOVA we want to first test if there is any difference between the groups, and only then test which groups differ. This can be done with the anova() function:\n\nanova(model_iris)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is much more to ANOVA, the lm() function, and to linear models in general. If you are interested, I would recommend reading the R Book by Michael J. Crawley, which is a great resource for learning statistics in R.",
    "crumbs": [
      "Introduction",
      "Appendix: More statistics and visualizations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Five days of R",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#five-days-of-r",
    "href": "index.html#five-days-of-r",
    "title": "Five days of R",
    "section": "Five days of R",
    "text": "Five days of R\nI have been teaching R to biologists and medical students for many years now. At the Core Unit for Bioinformatics at the Berlin Institute of Health, Charit√© - Universit√§tsmedizin Berlin, we have developed a five-day, 5 hour per day R crash course running for the last three years. This book is a companion to that course.\nThis is also the reason for how the materials in the book are arranged. Rather then discussing everything about vectors first, then everything about matrices etc., we start with easy things, and return to them later to build on them. I call this ‚Äúhelical learning‚Äù1 ‚Äì we spiral around the same topics, but each time going a bit deeper, and each time you will understand a bit more. This is also why some topics are spread between the days ‚Äì by trial and error, we have found the amount of material that can be covered in a day of learning.\n1¬†I got this idea from the professor Barbara P≈Çytycz from the Jagiellonian University, who taught me my first ‚Äúhelix‚Äù of the immune system.There are two goals of this book. The first one is that after five days of learning R, you will be able to load, inspect, manipulate and save data files (such as Excel tables or CSV files), make some basic plots and perform simple statistical tests. The second goal is that you are in a good starting position to continue learning R on your own.\nIn other words, this course should give you a jump start, allow to overcome this first hurdle of learning R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Five days of R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstalling R and RStudio\nBefore you dive in to the course, we would like to ask you to install both R and RStudio. The installation depends on your operating system, so please refer to specific instructions which can be found at the following links:\nXXX\n\n\nInstalling R packages\nDuring the course we will use several R packages that you need to install on your computer. On Day 2, we will discuss installing and loading packages, and we will make a note to install the required packages when they are needed. However, you can also install them right now, after you have installed R and RStudio. This can be more effective ‚Äì after all, installing might take some time.\nHere is the list of packages that you will have to install:\n\ntidyverse\nggplot2\ncolorDF\npander\nreadxl\nwritexl\njanitor\nbroom\ncowplot\nggbeeswarm\npheatmap\ntinytex (only if you want to produce PDF output from Rmarkdown)\n\nYou can install them by running the following code in your RStudio:\n\ninstall.packages(c(\"tidyverse\", \"ggplot2\", \"colorDF\", \"pander\", \"readxl\",\n                   \"writexl\", \"janitor\", \"broom\", \"cowplot\", \"tinytex\"))",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#the-structure-of-this-book",
    "href": "index.html#the-structure-of-this-book",
    "title": "Five days of R",
    "section": "The structure of this book",
    "text": "The structure of this book\nThis book is divided into five chapters, each corresponding to one day of the course. At the beginning of each chapter, you will find a short list of topics for the given day.\nSome parts of the book are highlighted:\nCode blocks with output:\n\n# this is a comment\nx &lt;- 1 + 1\n\nThe numbers on the left (if present) are not part of the code ‚Äì they are just line numbers. You can copy the code by clicking on the ‚ÄúCopy‚Äù button (üìã) and it will not copy these numbers.\n\n\n\n\n\n\n\nExercise 1 (Example) ¬†\n\nThis is how an exercise looks like!\nPlease do all exercises. It helps a lot.\nSome things are learned only through exercises.\n\n\n\n\n\n\n\n\n\n\n\nUseful tips\n\n\n\n\nSome exercises have solutions in the ‚ÄúSolutions‚Äù chapter. If they do, please read the solution after you have completed the exercise ‚Äì often there will be a comment or a hint that will help you understand the material.\n\n\n\n\n\n\n\n\n\nRemember!\n\n\n\n\nRun all code chunks in your RStudio.\nDo all exercises.\nGo through the ‚ÄúReview‚Äù section at the end of each chapter.\n\n\n\nTake a look at the right margin! New concepts are highlighted on the right margin of the book\nAnd again2!\n2¬†And also footnotes.In each chapter there are several exercises. However, that does not mean that you should only do the exercises. In fact, you should try out every piece of code that is in the book. Copy it (there is a üìã button next to each code block that will do it for you), paste it into your RStudio and run it. Then try to modify it and see what happens.\nExercises in this book are important. They are not only there to check if you understood the material, but they can also introduce new concepts or ideas. This is because this book is not only about learning R, but also learning how to learn about R. So, for example, sometimes we will want you to figure stuff on your own rather than give you a ready-made answer.\nEach chapter is ended by a ‚ÄúReview‚Äù section, which contains a list of things that you have learned that day. It is really important that you go through that list and make sure that you understand everything on it. Some of the new things appeared in the exercises, so if you skipped them, you might want to go back and do them.\nIf you do all that, I personally guarantee you that by the end of this course you will be able to use R in your work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#general-advice",
    "href": "index.html#general-advice",
    "title": "Five days of R",
    "section": "General advice",
    "text": "General advice\nThe course is a real crash course. There is a lot of material coming at you in a very short time. You will feel overloaded and overwhelmed ‚Äì this is normal. Don‚Äôt worry! It will soon get better, and in a few days you will be able to do fairly advanced things with R.\nThe key is to keep playing with your R; trying out new things, breaking it. Please go through all exercises in that book, even if they seem simple at the first glance (some of them are tricky, others are used to smuggle in new concepts and useful tidbits of information).\nWhenever you feel you don‚Äôt understand something, stop and try to figure it out. Use internet search very liberally. Many answers can be found on sites such as StackOverflow, R-bloggers, or in the R documentation. Try out the code you will find in these sources ‚Äì just copy-paste it into your RStudio and adapt it to your needs. Feel free to use Large Language Models (such as GPT) ‚Äì they are very good at explaining code, especially when you are learning basic concepts.\nHowever, if you want to learn R, simply doing this course will not be enough. You need to start using it in a real world setting. Unfortunately ‚Äì the better you already are at Excel, Word and other such tools, the harder it will be switching to R: tasks that are a breeze in Excel will at first require you to spend substantially more time in R. However, trust me: it pays off in the long run. Therefore, for best results, force yourself to use R even if at first it is less efficient then other tools.",
    "crumbs": [
      "Introduction"
    ]
  }
]