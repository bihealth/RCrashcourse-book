[
  {
    "objectID": "day2-complex-structures.html",
    "href": "day2-complex-structures.html",
    "title": "2  Complex data structures",
    "section": "",
    "text": "2.1 Goals for today",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#goals-for-today",
    "href": "day2-complex-structures.html#goals-for-today",
    "title": "2  Complex data structures",
    "section": "",
    "text": "Complex data structures (matrices, lists and data frames)\nLogical vectors and subsetting",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#logical-vectors",
    "href": "day2-complex-structures.html#logical-vectors",
    "title": "2  Complex data structures",
    "section": "2.2 Logical vectors",
    "text": "2.2 Logical vectors\n\n2.2.1 Truth and falsehood\nYesterday, we mentioned briefly two special values in R: TRUE and FALSE. These are logical constants, and they are used to represent truth and falsehood, respectively. Using these values, it is possible to create logical vectors – vectors that contain only these two values.\n\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\nThe FALSE value is equivalent to 0, and TRUE is equivalent to 1. That means, we can use logical vectors in arithmetic operations. One very common application of this is using sum() to count the number of TRUE values in a logical vector.\n\nsum(logical_vector)\n\n[1] 3\n\n\nLogical vectors are incredibly useful, because they can be used to subset other objects. For example, if you have a vector of numbers, you can use a logical vector to select some of the elements.\n\nnumbers &lt;- 1:5\nnumbers[ logical_vector ]\n\n[1] 1 3 4\n\n\nThe real usefulness of logical vectors comes when you create them using different operators that check for equality, inequality, etc. Let’s look at that in more detail.\n\n\n\n\n\n\nTRUE, FALSE and T and F\n\n\n\nIn R, TRUE and FALSE are the only two logical constants. However, there are also two other constants, T and F, which are equivalent to TRUE and FALSE, respectively. Do not use them. Unlike TRUE and FALSE, they can be overwritten, which can lead to chaos and mayhem.\n\n\n\n\n2.2.2 Comparison operators\nThere are six comparison operators in R:\n\n== – equality\n!= – inequality\n&gt; – greater than\n&lt; – less than\n&gt;= – greater than or equal to\n&lt;= – less than or equal to\n\nThey are vectorized like any other arithmetic operators, but their result is not a number – but a logical value. Take a look:\n\nnumbers &lt;- c(42, 3, -17, 0, -2, 1)\nnumbers &gt; 0\n\n[1]  TRUE  TRUE FALSE FALSE FALSE  TRUE\n\n\nFor each element of the vector, R checks if it is greater than zero. If it is, it returns TRUE, otherwise FALSE, producing, in the end, a vector containing as many elements as there were in the vector numbers. This logical vector can be used to subset the original vector.\n\n# prints only numbers greater than 0\nnumbers[ numbers &gt; 0 ]\n\n[1] 42  3  1\n\n# prints only numbers different from 0\nnumbers[ numbers != 0 ]\n\n[1]  42   3 -17  -2   1\n\n\nBut wait, there is more. There is a number of functions that check the elements of a vector and return logical vectors. One of the most commonly used and most useful of these is the is.na() function, which checks if an element is “not available”. This will come in handy later, when we start reading data from files – data which is full of NA’s!\n\nnumbers &lt;- c(42, 3, NA, 0, -2, 1)\nis.na(numbers)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE\n\n\nBut hey, this does not tell us which elements are NA’s. It is easy to see in the example above, but what if we have a vector with a million elements? Actually, to answer which elements are NA’s you can simply use the which() function:\n\nwhich(is.na(numbers))\n\n[1] 3\n\n\nFine, but what about the numbers which are not NA? What if we want to find all “good” numbers and store them for future use? In this case, we can use the ! operator, which negates the logical vector. That is, each TRUE becomes FALSE and each FALSE becomes TRUE.\n\nnas &lt;- is.na(numbers)\nnas\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE\n\n# change TRUE to FALSE and FALSE to TRUE\n!nas\n\n[1]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\nuseful_numbers &lt;- numbers[!nas]\nuseful_numbers\n\n[1] 42  3  0 -2  1\n\n\nThere is one more thing to mention here. The comparison operators == and != can be used to compare strings as well1.\n\npatient_measurements &lt;- c(1, 16, 7, 42, 3)\npatient_gender &lt;- c(\"male\", \"female\", \"female\", \"male\", \"female\")\npatient_measurements[ patient_gender == \"male\" ]\n\n[1]  1 42",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#matrices",
    "href": "day2-complex-structures.html#matrices",
    "title": "2  Complex data structures",
    "section": "2.3 Matrices",
    "text": "2.3 Matrices\n\n2.3.1 Creating matrices\nMatrices are just what says on the box: 2-dimensional structures; just like in mathematics. They behave in a very similar way to vectors in R, for example, they always hold elements of the same type (either numeric, character, etc., but never elements of both types). If you have your data stored in an Excel spreadsheet, chances are that different columns have different types of data. You can hardly use matrices in such a case. In fact, you will probably rarely use matrices in R, at least at the beginning – however, they are very useful for storing large data sets, for example from a transcriptomic analysis. For storing “Excel-like” data you will be using lists and data frames, which we will discuss later today. Nonetheless, we will spend some time on matrices today – because 90% of what you will learn today about matrices you will be able to use with data frames as well.\nCreating a matrix is very simple. You can use the matrix() function, which takes a vector as input and reshapes it into a matrix:\n\nmtx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nprint(mtx)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nAs you remember from yesterdays course, the 1:12 is a vector of numbers from 1 to 12. The nrow and ncol parameters specify the number of rows and columns, respectively. You don’t really need to specify both of them – if you specify only one, R will calculate the other one for you.\nAs you have noticed above, by default the matrix() function fills the matrix by columns. If you want to fill it by rows, you can use the byrow parameter:\n\nmtx &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nprint(mtx)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nHere we use a special value – TRUE – which is a logical constant. It is equivalent to 1, but it is more readable. You can also use FALSE (or 0) to specify that you want to fill the matrix by columns (this is the default).\nIt is also possible to create a large matrix by passing only one value (typically 0 or NA) and specifying the number of rows and columns. In that case, you have to specify both nrow and ncol.\n\nmtx &lt;- matrix(0, nrow = 3, ncol = 4)\n\nYou can also create a matrix by binding vectors together, either by rows or by columns. The rbind() function binds vectors by rows, and the cbind() function binds vectors by columns. Always make sure that the vectors you bind have the same length and the same element type.\n\na &lt;- 1:3\nb &lt;- 4:6\nmtx &lt;- rbind(a, b)\nprint(mtx)\n\n  [,1] [,2] [,3]\na    1    2    3\nb    4    5    6\n\nmtx &lt;- cbind(a, b)\nprint(mtx)\n\n     a b\n[1,] 1 4\n[2,] 2 5\n[3,] 3 6\n\n\n\n\n\n\n\n\nMatrices and algebra\n\n\n\nR matrices are very powerful for linear algebra operations. If you ever learned linear algebra, you will find that R matrices can do pretty much everything you learned in class. For example, you can multiply matrices, transpose them, invert them, calculate determinants, etc. We will not cover these operations in this course.\n\n\nJust like vectors have a length which you can check with the length() function, matrices have dimensions – the number of rows and the number of columns. You can access them using the dim() function, which returns a vector of length 2 (row and column number), and with functions nrow() and ncol(), which return the number of rows and columns, respectively.\n\ndim(mtx)\n\n[1] 3 2\n\nnrow(mtx)\n\n[1] 3\n\nncol(mtx)\n\n[1] 2\n\n\n\n\n2.3.2 Accessing matrix elements\nFor vectors, we have used the square brackets ([]) to access elements. Same with matrices, really, however we have two dimensions now. Think about that: with vectors we could only select one or more elements. With matrices, it should be possible to select an element, a number of elements from a row (or the whole row), a number of elements from a column (or the whole column), or even a submatrix. All this is possible using the square brackets.\n\nmtx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nprint(mtx)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n# Accessing the element in the third row and the second column\nmtx[3, 2]\n\n[1] 6\n\n# Accessing first three numbers in the second column\nmtx[1:3, 2]\n\n[1] 4 5 6\n\n# Accessing second to fourth numbers in the first row\nmtx[1, 2:4]\n\n[1]  4  7 10\n\n\n\n\n\n\n\n\nRows and columns\n\n\n\nWhen we talk about plotting, the first dimension, “x”, is usually the horizontal one, and the second dimension, “y”, is the vertical one. However, in R matrices, just like in real algebra, the first dimension corresponds to the rows, and the second dimension corresponds to the columns. You need to get used to it – it’s the same for data frames which you will be using extensively.\n\n\nIf you select more then one row and more than one column, you will get a new matrix – albeit smaller than the original one.\n\n# Selecting the first two rows and the last two columns\n# This will create a 2 x 2 matrix\nmtx[1:2, 3:4]\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n\n\nIf you want to access whole rows or columns, you do not need to specify anything – simply leave an empty space before (for selecting whole columsn) or after (for selecting whole rows) the comma.\n\n# Selecting the whole second row\nmtx[2, ]\n\n[1]  2  5  8 11\n\n# Selecting the whole third column\nmtx[, 3]\n\n[1] 7 8 9\n\n# Row 1 and three - returns a matrix\nsel &lt;- c(1, 3)\nmtx[sel, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    3    6    9   12\n\n\nThis last example shows that just like with vectors, we can use a variable to make our selection.\n\n\n\n\n\n\nRemember!\n\n\n\n\nRows first, then columns\nIf you select a single column or a single row, you will get a vector\nIf you select more than one row or column, you will get a (smaller) matrix\nIf you select more rows or columns than are present, you will get a “subscript out of bonds” error\nVectors and matrices always have only one data type (string, numerical, logical etc.)\n\n\n\n\n\n2.3.3 Row and column names\nJust like in case of named vectors, we can name rows and columns of a matrix. However, for this we need two different functions: rownames() and colnames() (for row names and column names, duh).\n\nrownames(mtx) &lt;- c(\"first\", \"second\", \"third\")\ncolnames(mtx) &lt;- LETTERS[1:4]\n\nmtx[\"first\", ]\n\n A  B  C  D \n 1  4  7 10 \n\nmtx[ , \"A\"]\n\n first second  third \n     1      2      3 \n\n\nIn the code above, we use the LETTERS constant, which is a vector containing all the letters of the English alphabet. Just like constants pi and e, LETTERS is available in R by default, along with its lower-case counterpart, letters. It is useful for labeling.\nUnfortunately, we only have 26 letters in the alphabet, so what can we do with a matrix that has more columns than that? Well, we can use the same trick that Excel uses: after Z, we have AA, AB, etc.\nTo create such a long vector, we will use two functions: rep() and paste0().\n\n\n2.3.4 Using rep() to generate column names\n The rep() function is a little and very useful utility function that repeats element of a vector a given number of times. It either repeats the whole vector several times, or, using the each parameter, repeats each element a given number of times:rep()\n\nabc &lt;- LETTERS[1:3] # A, B, C\nabc3 &lt;- rep(abc, 3)\nabc3\n\n[1] \"A\" \"B\" \"C\" \"A\" \"B\" \"C\" \"A\" \"B\" \"C\"\n\na3b3c3 &lt;- rep(abc, each = 3)\na3b3c3\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n\nIn the code above, we created two vectors, each of the length \\(3 \\times 3\\); first one goes “A, B, C, A, B, C, …”, and the second one goes “A, A, A, B, B, …”. To get at our goal, we would have to paste together the first element from the first vector with the first element from the second vector etc.:\n\n\n\na3b3c3\nabc3\nresult\n\n\n\n\nA\nA\nAA\n\n\nA\nB\nAB\n\n\nA\nC\nAC\n\n\nB\nA\nBA\n\n\nB\nB\nBB\n\n\nB\nC\nBC\n\n\n\nTo do this, we will use the paste0() function, which concatenates strings and is vectorized, so it does exactly what we need.\n\ncol_names &lt;- paste0(abc3, a3b3c3)\n\nOf course, we need all the letters (we used only three in the example above for demonstration purposes).\n\nn &lt;- length(LETTERS)\nabc3 &lt;- rep(LETTERS, n)\na3b3c3 &lt;- rep(LETTERS, each = n)\ncol_names &lt;- paste0(abc3, a3b3c3)\nlength(col_names)\n\n[1] 676\n\nhead(col_names)\n\n[1] \"AA\" \"BA\" \"CA\" \"DA\" \"EA\" \"FA\"\n\ntail(col_names)\n\n[1] \"UZ\" \"VZ\" \"WZ\" \"XZ\" \"YZ\" \"ZZ\"\n\n\nThe head() and tail() functions are very useful for inspecting the beginning and the end of a very large object such as a vector, matrix, or data frame. They are very useful for checking if the operation you just performed did what you expected it to do.\n\n\n\n\n\n\n\nExercise 2.1 (Creating column names) Repeat the procedure above, but generate column names for a matrix with more than a 1000 columns. Use the LETTERS constant, but rather then generating two-letter column names, generate three-letter column names: AAA, AAB, AAC, …, ABA, ABC, …, ZZZ. Store the result in a variable called col_names.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2 (Matrices - accessing and changing elements) Assume you have a 48 well-plate for a drug sensitivity analysis with viability scores.\n\nCreate a 48-element vector “drugSensitivity_v” with random numbers between 0 and 1. Use runif(48) to generate these values. These reflect your viability scores.\nWhat does the runif() function do?\nCreate a 6x8 matrix “drugSensitivity” from the vector.\n\nBefore starting you experiment, you decided to leave out the border wells to avoid edge effects:\n\nChange the values of all the border elements to NA.\n\nThe rows are treated with inhibitor 1 with increasing concentrations (control, low, medium, high). Columns 2 to 4 are treated with inhibitor 2 with increasing concentrations (control, low, high) and column 5 to 7 are treated with inhibitor 3 (same concentrations as inhibitor 2).\n\nUse row and column names to reflect treatments.\nWhat are potential problems of this approach?\nSelect all wells with inhibitor 3.\nWhich wells can be used as negative control?\nSelect only wells with a combination of inhibitor 1 and inhibitor 2.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#lists",
    "href": "day2-complex-structures.html#lists",
    "title": "2  Complex data structures",
    "section": "2.4 Lists",
    "text": "2.4 Lists\n\n2.4.1 Creating lists\nThe objects that we have discussed so far – vectors and matrices – can only hold one type of data; you cannot mix strings with numbers, for example. This is obviously a problem – quite frequently you need to store both numbers and strings in one object. This is where lists come in.\nLists are created using the list() function. Lists have elements, just like vectors; but unlike vectors, every element can be of any possible type. It can be a vector, of course, but can also be a matrix, a data frame, even a function – or another list. Actually, it is quite common to have a list of lists (or even list of lists of lists) in R.\n\nlst &lt;- list(numbers=1:3, strings=c(\"a\", \"bu\"), \n            matrix = matrix(1:4, nrow = 2), \n            logical = c(TRUE, FALSE))\nlst\n\n$numbers\n[1] 1 2 3\n\n$strings\n[1] \"a\"  \"bu\"\n\n$matrix\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n$logical\n[1]  TRUE FALSE\n\n\n\n\n2.4.2 Accessing elements of a list\nLike vectors, lists can be named. In fact, they very often are. However, accessing them is a bit different than with vectors.\nYou can use [ and ] square brackets to access elements of a list, but this produces another list, containing only the selected elements. Thus, if you type lst[\"numbers\"], you will get a list with one element, which is the vector of numbers:\n\nlst[\"numbers\"]\n\n$numbers\n[1] 1 2 3\n\n\nYou can see that it is a list because of this weird $ (dollar) sign, which we will discuss in a moment. However, you can also check its type directly:\n\ntypeof(lst[\"numbers\"])\n\n[1] \"list\"\n\n\nIf, however, you want to work with the actual vector that is stored in the “numbers” slot of the list, you need to use one of two approaches. First approach is to use double square brackets:\n\nlst[[\"numbers\"]]\n\n[1] 1 2 3\n\ntypeof(lst[[\"numbers\"]])\n\n[1] \"integer\"\n\n\nThat requires a lot of typing, four times square brackets and then, in addition, the quote marks. But programmers are lazy, and therefore, we have a shortcut: the $ sign. It is used to access elements of a list by name:\n\nlst$numbers\n\n[1] 1 2 3\n\n\nYou will use this construct a lot in R.\nThe elements of a list behave exactly like regular variables. If an element is a vector, you can do with it all the things you can do with a vector; if it is a matrix, you can treat is as a matrix (because it is a matrix).\n\npatient_data &lt;- list(name = \"John Doe\", age = 42, \n                     measurements = runif(5))\npatient_data\n\n$name\n[1] \"John Doe\"\n\n$age\n[1] 42\n\n$measurements\n[1] 0.05721389 0.84496532 0.95358735 0.22773064 0.28702054\n\npatient_data$measurements[1]\n\n[1] 0.05721389\n\npatient_data$measurements[1] &lt;- 42\npatient_data$measurements * 3\n\n[1] 126.0000000   2.5348959   2.8607621   0.6831919   0.8610616\n\n\nSince the lists are named, there must be a way to access and modify these names. And, of course, there is: the names() function.\n\nnames(patient_data)\n\n[1] \"name\"         \"age\"          \"measurements\"\n\nnames(patient_data) &lt;- c(\"patient_name\", \"patient_age\", \"patient_measurements\")\nnames(patient_data)[3] &lt;- \"crp_measurement\"\n\n\n\n\n\n\n\nTab completion with lists and data frames\n\n\n\nIf you type the name of your data frame variable in a script, the $ and press the TAB key, RStudio will show you all the elements of a list (or columns of the data frame) to choose from. No need for tedious typing!\n\n\n\n\n2.4.3 Lists as return values\nA common application of lists has something to do with functions. Remeber that a function can return only one object? But what if a function would like to return several things at once? It can return a list!\nWe will run now our first statistical test. First, we need to generate two groups of measurements to compare. We will simulate them using the rnorm() function which produces normally distributed random numbers. The function takes additional parameters, mean and sd, which specify the mean and the standard deviation of the distribution, respectively. That allows us to ensure that the groups differ:\n\ngroup_a &lt;- rnorm(10, mean = 10, sd = 2)\ngroup_b &lt;- rnorm(10, mean = 14, sd = 2)\n\nOf course, before running any statistical test we usually want to have a look at the data, to see if the groups differ visually. We can do this by using the boxplot() function, which creates a boxplot of the data.\n\nboxplot(group_a, group_b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\n\n\nBoxplots are a great way to visualize the data. The whiskers show the minimum and maximum values (excluding outliers, which are shown as separate points), the box shows the interquartile range (25th to 75th percentile), and the thick line in the middle of the box shows the median. There are better ways, which we will discuss on Day 5, but still, boxplots are pretty cool.\n\n\nOK, now we run a t-test. We will use the t.test() function for that, and store the result in a variable called t_test.\n\nt_test &lt;- t.test(group_a, group_b)\ntypeof(t_test)\n\n[1] \"list\"\n\nt_test\n\n\n    Welch Two Sample t-test\n\ndata:  group_a and group_b\nt = -3.5, df = 17.99, p-value = 0.002559\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.8805146 -0.9692339\nsample estimates:\nmean of x mean of y \n 10.53429  12.95917 \n\n\nAs you can see, the result is a list. However, when we print it to the console, it does not look like one. This is because R has a special function for formatting the results of a t-test2, so that it is easier to read.\nNonetheless, we can access the elements of the list in the usual way:\n\nt_test$p.value\n\n[1] 0.002558707\n\n\n\n\n2.4.4 Replacing, adding and removing elements of a list\nYou can assign elements to a list using the $ sign. If the element does not exist yet in the list, it will be created; if it does, it will be replaced.\nTo remove an element, you need to use the special value NULL.\n\nperson &lt;- list(name = \"January\", age = 117, pets=c(\"cat\", \"dog\"))\n\n# change the age\nperson$age &lt;- 118\n\n# add a new element\nperson$city &lt;- \"Hoppegarten\"\n\n# remove pets\nperson$pets &lt;- NULL\nperson\n\n$name\n[1] \"January\"\n\n$age\n[1] 118\n\n$city\n[1] \"Hoppegarten\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#data-frames",
    "href": "day2-complex-structures.html#data-frames",
    "title": "2  Complex data structures",
    "section": "2.5 Data Frames",
    "text": "2.5 Data Frames\n\n2.5.1 Creating data frames\nFinally, we come to possibly the most important data structure in R – at least for us, biologists and medical researchers: data frames. Data frames are the closest thing to an Excel spreadsheet in R. They are used to store data in a tabular form, where each column can be of a different type. This makes them perfect for storing data from experiments, clinical trials, etc. You will be using them a lot.\nIn R, data frames are lists that were made to behave a lot like matrices. Thus, everything that you learned so far about lists can be applied to data frames, including accessing their elements (columns) using the $ operator. However, there are some differences between data frames and matrices.\nThe main feature of data frame that makes them a bit like matrices is the fact that each element of a data frame is a vector3 and that these vectors have always the same length. This means that one of the major differences between data frames and, say, Excel spreadsheets, is that a column of a data frame contains only elements of a single type. If a “cell” in a data frame is a character string, then the whole column is a character vector; if it is numerical, then the whole column is a numerical vector, etc.\nThis may seem like a limitation, but it is, in fact, a good thing. It makes data more consistent and less prone to errors.\nLike lists, data frames can be named or not, but typically they are. The names of a data frame are precisely the column names and you can access them (and modify) using both, names and colnames functions.\n\nnames &lt;- c(\"January\", \"William\", \"Bill\")\nlastn &lt;- c(\"Weiner\", \"Shakespeare\", \"Gates\")\nage   &lt;- c(NA, 460, 65)\n\npeople &lt;- data.frame(names=names, last_names=lastn, age=age)\npeople\n\n    names  last_names age\n1 January      Weiner  NA\n2 William Shakespeare 460\n3    Bill       Gates  65\n\nnames(people)\n\n[1] \"names\"      \"last_names\" \"age\"       \n\ncolnames(people)\n\n[1] \"names\"      \"last_names\" \"age\"       \n\n\nLike matrices, data frames have dimensions, which you can access using the dim(), nrow() and ncol() functions.\n\ndim(people)\n\n[1] 3 3\n\nnrow(people)\n\n[1] 3\n\nncol(people)\n\n[1] 3\n\n\nAlso, like matrices, the data frames can have rownames; however, for reasons that will be clear later, they are not as important as column names and in fact, we will not be using them4.\n\n\n\n\n\n\n\nExercise 2.3  \n\nCreate a 5x3 matrix with random numbers. Use matrix and rnorm.\nTurn the matrix into a data frame. Use as.data.frame for that.\nAdd column and row names.\nAdd a column. Each value in the column should be “A” (a string). Use the rep function for that.\nAdd a column with five numbers from 0 to 1. Use the seq function for that. Hint: look at the help for the seq function (?seq).\n\n\n\n\n\n\n\n2.5.2 Accessing and modifying columns of a data frame\nSince data frames are lists, you can access their columns using the $ operator. This is the most common way of accessing columns of a data frame:\n\npeople$names\n\n[1] \"January\" \"William\" \"Bill\"   \n\n\nYou can also access columns using the square brackets, just like with matrices, using a comma to denote the columns and rows. However, there is a fine difference between matrices and data frames: if you select a single row, you will not get a vector, but a data frame with one row. If you think about that, it makes perfect sense: the different columns can have different data types, so they cannot be easily combined into a vector without losing some information.\n\npeople[1, ]\n\n    names last_names age\n1 January     Weiner  NA\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere is an issue when you try to access a single column of a data frame. We will discuss it at length in the following, but basically, this behavior is different for different flavors of data frames: base R data frames created with data.frame() return a vector, while others may return a data frame. Watch out for this!\n\n\n\n\n2.5.3 Subsetting data frames with logical vectors\nJust like with vectors, you can use logical vectors to subset data frames. This is extremely useful and very common in R. For example, we might want to select only rows that do not contain NA values in the age column.\n\npeople_with_age &lt;- people[!is.na(people$age), ]\npeople_with_age\n\n    names  last_names age\n2 William Shakespeare 460\n3    Bill       Gates  65\n\n\nThat way we have filtered the data frame, leaving only persons with known age.\nActually, for data frames, you will commonly use the filter() function (which we will introduce tomorrow). However:\n\nthe filter() function also uses logical vectors;\nyou can subset many different data types using logical vectors (including matrices, lists, vectors, data frames), but filter() works only with data frames; and\nsometimes using logical vectors is just more convenient.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#libraries-in-r",
    "href": "day2-complex-structures.html#libraries-in-r",
    "title": "2  Complex data structures",
    "section": "2.6 Libraries in R",
    "text": "2.6 Libraries in R\n\n2.6.1 Installing and loading packages\nStarting tomorrow, we will cease to only use “base” R functions (that is, functions that are available in R “out of the box”) and start using additional packages. Packages in R are collections of functions (and often some other things, like data sets, documents and more).\nPackages need to be installed before they can be used – but once you have installed a package, you don’t need to install it again (unless you upgrade R or want to update the package to a newer version).\nHowever, to use a package, you also need to load it using the library() function. This you must do every time you start a new R session, because R “forgets” which packages have been loaded the previous time. It is a bit like with the software for your operating system: you need to install your browser only once, but you have to start each each time after you start your computer.\nInstalling packages usually is straightforward, however at times it can be tricky.\n\n\n\n\n\n\npackage ‘—’ was built under R version x.y.z\n\n\n\nAt times you will get a warning that a package was “built” under a different version of R than the one that you are running. It is not an error (just a warning), and most of the time it can be ignored. It means what it says: that the installed package was built (e.g., compiled) with a different version of R. This can sometimes lead to problems, but most of the time it does not.\n\n\nWe will return to installing packages from different sources on Day 5.\n\n\n\n\n\n\nRemember!\n\n\n\nRemember: you need to install a package only once with install.package(), but you need to load it every time you start a new R session with library().\n\n\n\n\n\n\n\n\n\nExercise 2.4 Use install.packages to install the package colorDF from CRAN. Then, load the package using library(colorDF). What does that package do? How can you check that? (Hint: use ??colorDF).\n\n\n\n\n\n\n2.6.2 Data frames, tibbles & co.: different flavors of R\nIt is now time to reveal some ugly truths about R. R is open source, and everyone can modify it, add new packages etc. This is great and resulted in the vibrant user community that R has. Also, there is hardly a statistical method or framework that is not represented in R. In addition, developing and publishing new packages for R is incredibly easy, at least compared to some other languages.\nHowever, this has a downside. There are many different flavors of R, and, unfortunately, some of the most popular packages or groups of packages can clash. We will spend now some time with one particular example: data frames. Firstly, because you will be using data frames a lot, secondly, because it neatly illustrates the problem, and thirdly to introduce a new type of data – tibbles.\nBase R data frames are created using the data.frame() function. They are useful and we use them a lot, but they have one tiny inconsistency that can cause a lot of trouble. Say, you define your data frame like this:\n\ndf &lt;- data.frame(a = 1:3, b = c(\"a\", \"b\", \"c\"))\n\nWhen you access a single column of this data frame using the square brackets [ ], you will get a vector. We can check it with the is.data.frame() function:\n\nis.data.frame(df[ , 1])\n\n[1] FALSE\n\n\nThis is consistent with the behavior of matrices (where one column or one row becomes a vector), but not with the behavior of the data frames themselves: because when you access a single row, you are getting a data frame, not a vector:\n\nis.data.frame(df[1, ])\n\n[1] TRUE\n\n\nThis inconsistency can mess up your code. Imagine that you have somehow automatically selected some columns of a data frame – for example, by selecting columns that start with a certain letter (we will learn how to do that on Day 4). You store it in the variable called sel_cols. You can select the columns from the data frame using the df[ , sel_cols] syntax. However, depending on whether there was a single column selected or more, you will get either a vector or a data frame. This is annoying and in the worst case scenario, it can break your program.\nMany people noticed this, and proposed solutions. In R, it is possible to take a class of an object (like data.frame) and modify it. One of such most commonly used modifications is called a tibble and has been implemented in the Tidyverse group of packages, which we will be using extensively in the days to come.\n\n\n2.6.3 Data frames and tibbles\nThe Tidyverse data frame is called a tibble. It behaves almost exactly like a data frame, with a few crucial differences:\n\nwhen printing the tibble to the console, the output is nicer\ntibbles never have row names (and that is why we will not be using them)\nwhen you access a single column of a tibble, you always get a tibble, not a vector\n\nTo use tibbles, you need the tidyverse package5. You can install it using install.packages(\"tidyverse\") and load it using library(tidyverse).\n\nlibrary(tidyverse)\ntbl &lt;- tibble(a = 1:3, b = c(\"a\", \"b\", \"c\"))\ntbl\n\n# A tibble: 3 × 2\n      a b    \n  &lt;int&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 c    \n\nis.data.frame(tbl)\n\n[1] TRUE\n\nis_tibble(tbl)\n\n[1] TRUE\n\n\nAs you can see, a tibble is both a data frame and a tibble. You can use tibbles as a drop-in replacement for data frames. We will be seeing it a lot, because many useful functions from the Tidyverse family produce tibbles, not data frames. However, for you, as the user, the differences will be mostly cosmetic. So far, so good.\nHowever, if you are reading this book, chances are that you are a biologist or medical researcher, and that means that sooner or later you will be using packages from the Bioconductor project. Bioconductor is a collection of R packages for bioinformatics, genomics, and related fields. They are incredibly valuable, you can hardly do bioinformatics without them. However, Bioconductor defines its own alternative to data.frames, called DataFrame. It is a bit different from data frames, and it is not compatible with tibbles. If you want to process DataFrames produced by Bioconductor packages with Tidyverse functions, you need to convert them to a regular data.frame using the as.data.frame() function.\nThe code below will not work until you install the Bioconductor package S4Vectors. Don’t worry if it does not work – we will not be using Bioconductor in this course, but I wanted to show you where the problem is.\n\nlibrary(S4Vectors)\nDF &lt;- DataFrame(a = 1:3, b = c(\"a\", \"b\", \"c\"))\nDF\n\nDataFrame with 3 rows and 2 columns\n          a           b\n  &lt;integer&gt; &lt;character&gt;\n1         1           a\n2         2           b\n3         3           c\n\nis.data.frame(DF)\n\n[1] FALSE\n\n\nAs you can see, the DataFrame object is not a data.frame. And this means that the Tidyverse function filter() does not see it as one (you will learn about the filter() function on Day 4):\n\nlibrary(tidyverse)\nfilter(DF, a &gt; 1)\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"c('DFrame', 'DataFrame', 'SimpleList', 'RectangularData', 'List', 'DataFrame_OR_NULL', 'Vector', 'list_OR_List', 'Annotated', 'vector_OR_Vector')\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#summary",
    "href": "day2-complex-structures.html#summary",
    "title": "2  Day 2: Complex data structures",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nThings you learned today:\n\nLogical vectors:\n\nsubsetting with logical vectors\nusing comparison operators like &gt; to create logical vectors\nusing the is.na() function to check for NA values\nusing the which() function to find the positions of TRUE values\nusing the ! operator to negate logical vectors\n\nMatrices:\n\ncreating matrices using the matrix() function\nmeasuring matrices with dim(), nrow() and ncol()\nrows first, then columns\naccessing elements, rows, columns and submatrices of a matrix\nnaming rows and columns of a matrix\n\nLists:\n\ncreating lists using the list() function\naccessing elements of a list using [, [[ and $\nlists as return values from functions\nreplacing, adding and removing elements of a list\n\nData frames:\n\ncreating data frames using the data.frame() function\naccessing and modifying column names of a data frame\naccessing and modifying elements of a data frame\nsubsetting data frames\nadding and removing columns from a data frame\nmerging data frames\ncreating tibbles with Tidyverse and tibble()\n\nOther:\n\nconstant vectors LETTERS and letters\nusing the rep() function\ngenerating random numbers with rnorm() and runif()\ngenerating sequences with seq()\nrunnning a t-test using function t.test()\nmaking a boxplot with boxplot()\nthe special value NULL\nconverting matrices to data frames with as.data.frame()\ninstalling packages with install.packages()\nloading packages with library()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day 2: Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#footnotes",
    "href": "day2-complex-structures.html#footnotes",
    "title": "2  Complex data structures",
    "section": "",
    "text": "Actually, they can be used to compare any two objects in R. Also, the &gt; and &lt; operators can be used to compare strings, but the result is not always what you might expect. Can you guess what it does? Hint: pick up a dictionary… or any other alphabetically sorted list.↩︎\nOK, this is way beyond the scope of this course, but the result returned by t.test has the class “htest”. This is a list, but also it is something special, which is why R knows to print it in a different way. R is a functional language, but it also allows object oriented (OO) programming.↩︎\nActually, it is a bit more complicated than that, but for now, let’s just say that each column of a data frame is a vector.↩︎\nRow names in R data frames are very old school. Many people still use them, and many R functions produce them. However, we will be using the packages from the tidyverse family further down the line, which ignore the rownames, and for good reasons.↩︎\nActually, the tidyverse package is a meta-package: it just loads a collection of packages that are often used together. The tibble package which defines tibbles is one of them.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day2-complex-structures.html#review",
    "href": "day2-complex-structures.html#review",
    "title": "2  Complex data structures",
    "section": "2.7 Review",
    "text": "2.7 Review\nThings you learned today:\n\nLogical vectors:\n\nsubsetting with logical vectors\nusing comparison operators like &gt; to create logical vectors\nusing the is.na() function to check for NA values\nusing the which() function to find the positions of TRUE values\nusing the ! operator to negate logical vectors\n\nMatrices:\n\ncreating matrices using the matrix() function\nmeasuring matrices with dim(), nrow() and ncol()\nrows first, then columns\naccessing elements, rows, columns and submatrices of a matrix\nnaming rows and columns of a matrix\n\nLists:\n\ncreating lists using the list() function\naccessing elements of a list using [, [[ and $\nlists as return values from functions\nreplacing, adding and removing elements of a list\n\nData frames:\n\ncreating data frames using the data.frame() function\naccessing and modifying column names of a data frame\naccessing and modifying elements of a data frame\nsubsetting data frames\nadding and removing columns from a data frame\nmerging data frames\ncreating tibbles with Tidyverse and tibble()\n\nOther:\n\nconstant vectors LETTERS and letters\nusing the rep() function\ngenerating random numbers with rnorm() and runif()\ngenerating sequences with seq()\nrunnning a t-test using function t.test()\nmaking a boxplot with boxplot()\nthe special value NULL\nconverting matrices to data frames with as.data.frame()\ninstalling packages with install.packages()\nloading packages with library()",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Complex data structures</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html",
    "href": "day1-intro-to-r.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Goals for today\nWhat you should know after today:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#goals-for-today",
    "href": "day1-intro-to-r.html#goals-for-today",
    "title": "1  Introduction to R",
    "section": "",
    "text": "what is R?\nwhy use R?\nfirst steps in R\n\n\n\nwhat R is\nhow to start R\nhow to use R as a calculator\nhow to assign variables\nhow to use functions\nhow to use vectors\nhow to use data frames\nhow to use packages",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#r-rstudio-and-other-languages",
    "href": "day1-intro-to-r.html#r-rstudio-and-other-languages",
    "title": "1  Introduction to R",
    "section": "1.2 R, RStudio and other languages",
    "text": "1.2 R, RStudio and other languages\n\n1.2.1 Why R?\n\n\n1.2.2 R and RStudio\nR is the name of both, the programming language and of the language interpreter. When you start RStudio, you can see the R language interpreter working in the part of the window left and bottom - called “console”. So yes, you don’t need RStudio to work with R and, in fact, many people prefer to work with R in a different environment.\nRStudio is a so called IDE, an Integrated Development Environment. That is, it provides a lot of goodies that help make your work easier, faster and more efficient.\n\n\n1.2.3 R and other languages\nR is not the only language that you can use for data analysis. There are many other languages that are used for this purpose, including Python, Matlab and many others. Each of these languages has its own strengths and weaknesses, and the choice of language depends on your needs. In fact, most bioinformaticians know more than one language, and use the one that is best suited for the task at hand.\nWe think that R is a particularly good choice for all those who just need a tool to use from time to time to help them with their work. It is relatively easy to learn, and it is very powerful. However, other choices are also worth mentioning.\nMatlab is a language that is in many ways similar to R. The main differnce is maybe that unlike R, Matlab is not free – it is closed source and you have to pay for a license. This has some advantages. For example, and as you will see during this course, R development is not centralized and so there are many packages that do the same thing. Matlab is in some aspects more consistent and more polished than R, and in some comparisons appears to be faster – and for this, it is often the language of choice for areas such as image analysis.\nPython is completely different story. This is a powerful, fast, general purpose programming language. It is more versatile than R, has a much more standardized syntax and development process. However, it is harder to learn and it is not really meant to be used interactively (although it can be – especially when combined with Quarto or Jupyter Notebook). While many statistical modules exist for Python, it is not as strong in this area as R.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#projects-and-workspaces",
    "href": "day1-intro-to-r.html#projects-and-workspaces",
    "title": "1  Introduction to R",
    "section": "1.3 Projects and Workspaces",
    "text": "1.3 Projects and Workspaces\n\n1.3.1 Creating a project: start here!\nWhen starting work with a new project, do the following: (i) create a new directory for the project, (ii) open an R script file and save it in the directory you created and (iii) copy necessary data files.\nTo create a new directory in RStudio, go to File -&gt; New Project. When the dialog window appears, select first “New Directory” and then “New Project”. Select the location where you would like to have the directory created and click on “Create Project”.\n\n\n\n\n\n\n\nExercise 1.1  \n\nCreate a new project in RStudio. Do it now, you will continue to work with this project over the next few days (we hope).\nInspect the contents of the project directory. What files are there?\n\n\n\n\n\nWhen Rstudio creates a new project, it creates a new directory with the same name as the project. Furthermore, it creates a new file in this directory called projectname.Rproj. This file is used by RStudio to keep track of project-specific settings.\nYou can open this file by double-clicking on it in the Files pane in RStudio. Like most of the files that you will be working with, it is a simple text file: you can open it in any text editor, including RStudio.\nLater on, if you choose to do so, R can create two hidden files, Rhistory (called .Rhistory on Unix-like systems and _Rhistory on Windows) and .RData (or _RData). This files save the state of your R session (of your R workspace, to be specific).\n\n\n1.3.2 Workspaces, history and environments\n\n\n1.3.3 Tab completion",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#lets-start-with-r",
    "href": "day1-intro-to-r.html#lets-start-with-r",
    "title": "1  Introduction to R",
    "section": "1.4 Let’s start with R",
    "text": "1.4 Let’s start with R\n\n1.4.1 R as a calculator\nYou can use R as a very powerful calculator. For example, do you want to know what \\(\\sin(\\pi/2)\\) is? Just type sin(pi/2) in the console and press Enter. Addition and subtraction work, as expected, with + and -. To multiply two numbers, type 2*3; to divide, type 2/3. You can get exponents (powers, eg. \\(2^3\\)) by typing 2^3. If the ^ symbol (called “caret”) is not available on your keyboard, you can use ** instead. Parentheses () are used to group expressions, just like in mathematics. To logarithmize, you can use log(), log2() and log10() functions. For example, to calculate \\(\\log_{10}(100)\\), type log10(100). Can you guess how to calculate \\(\\sqrt{2}\\)? Yes, you are right: sqrt(2). Or 2^(1/2), that will also do.\n\n\n\n\n\n\n\nExercise 1.2 Calculate the following expressions in R:\n\n\\(\\log_{2}(8)\\)\n\\(\\sin(\\pi)\\)\n\\(2^{10}\\)\n\\(\\sqrt{e}\\)\n\n\n\n\n\n\n\n1.4.2 Using script files\nOn the left side of the RStudio window you have (by default) two panels: the lower one is called “Console”. When you create a new script file, it appears above.\n\n\n\n\n\n\nTyping in console\n\n\n\nYou can type your commands (properly called “expressions”) directly into the console, but it is generally not a good idea. Why? The truthful answer is: because it is messy and sooner or later you will regret it. You can save the history of what you type in the console, but it is easier (and cleaner) to save your program in a script file.\n\n\nWhen you open or create an R script file and type something into it, you can run it by pressing Ctrl+Enter (or Cmd+Enter on Mac). This will send the line of code to the console, where it will be executed. You can also select a fragment of the code before you press Ctrl+Enter, and then the whole selected fragment will be sent to the console.\nHowever, if you start your line with #, it will not be executed. This is called a comment and we will spend some time later on convincing you that you should use a lot of comments in your code.\n\n\n\n\n\n\n\nExercise 1.3 Repeat Exercise 1.2, but now type the expressions into the script which you have created in Exercise 1.1. Run the script by pressing Ctrl+Enter after each line.\n\n\n\n\nScript files are also text documents. You can open them in any text editor, for example Notepad or even Word (but don’t do that). In RStudio, you see the script file in many colors: for example, comments can be green, strings (text in quotes) can be red, and so on. This is called syntax highlighting and is done by RStudio to make your code more readable. You will not see the colors when you open your R script in Notepad.\n\n\n\n\n\n\nImportant\n\n\n\nFrom now on, you should only type your code in script files.\n\n\n\n\n1.4.3 Variables\nWhat if we want to store the result of a calculation for later use? We can do this by assigning the result to a variable. In R, you assign a value to a variable using the &lt;- operator:\n\nx &lt;- 2\ny &lt;- sin(pi/2)\nz &lt;- x + y\n\nIf you want to see the value of a variable, just type its name in the console and press Enter, or use print() function:\n\nprint(z)\n\n[1] 3\n\n\n\n\n\n\n\n\nAssignment operator\n\n\n\nMany other languages use = as an assignment operator. In R, you can use = as well, but do yourself a favour and don’t. Use &lt;- instead. Why? Your code will be more readable and you will avoid many common mistakes.\n\n\nVariables are like boxes in which you can store values. However, unlike boxes, when you assign one variable to another, the first variable keeps its content:\n\nx &lt;- 2\ny &lt;- x\nprint(x)\n\n[1] 2\n\n\nWe now come to a very important point which we will revisit often, as it is one of the most common beginner (and not only beginner) mistakes. When you forget to assign the value to a variable, R will print it to the console, but the variable will not be modified:\n\nx &lt;- 2\n# prints 0.9092974:\nsin(x) \n# prints 2\nprint(x) \n\nIn the code above, the value of x is not changed by the sin() function. To store the value of a function, you need to assign it to a variable:\n\nx &lt;- 2\n# does not print anything:\nx &lt;- sin(x) \n# prints 0.9092974:\nprint(x)    \n\nPlease spend some time on this, as it is a very common source of errors.\n\n\n\n\n\n\nImportant\n\n\n\nAs a rule of thumb1, if the expression you type in your script does not contain the &lt;- operator, it will not modify any variables.\n\n\n\n\n\n\n\n\n\nExercise 1.4 Create a variable using x &lt;- 42. Take a look at the Environment pane in RStudio (top left part of the window). Do you notice anything?\n\n\n\n\n\n\n1.4.4 Character variables\nVariables can store not only numbers, but also text. Text in R is called a character string. To create a character string, you need to enclose the text in quotes (both single and double quotes are allowed, but try to be consistent and use only one type). For example:\n\nname &lt;- \"January\"\ncity &lt;- \"Hoppegarten\"\ngreeting &lt;- \"Hello, world!\"\n\nCharacter variables cannot be used with algebraic computations, the following code will throw an error:\n\n# this does not work!\nname + city\n\nHowever, if you want to “add” two character strings (that is, concatenate them), you can use the paste() function:\n\npaste(name, city)\n\n[1] \"January Hoppegarten\"\n\n\nQuite often, you don’t want to have a space between the two strings. This is such a common operation that R has a shortcut for it:\n\npaste0(name, city)\n\n[1] \"JanuaryHoppegarten\"\n\n\n\n\n\n\n\n\nOther types\n\n\n\nThere are other types of data types in R. Later on, we will briefly touch on factors, which look like character strings but behave like numbers. Another important data type is a logical type, which can have only two values: TRUE and FALSE. We will talk about logical types in more detail tomorrow. And under the hood, numeric vectors can be either integers (numbers like 1, 2, …) or floating point numbers (numbers like 1.1, 2.2 or \\(\\pi\\)).\n\n\n\n\n\n\n\n\n\nExercise 1.5 (Variables) Create the following variables in your script:\n\nname with the value of your first name\ncity with the value of the city where you live\nage with the value of your age\ngreeting with the value “Hello,”\nconcatenate the variables greeting and name and store the result in a new variable message",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#vectors-and-vectorization",
    "href": "day1-intro-to-r.html#vectors-and-vectorization",
    "title": "1  Introduction to R",
    "section": "1.5 Vectors and vectorization",
    "text": "1.5 Vectors and vectorization\n\n1.5.1 Vectors\nVariables can (and do) store a lot more than single values. One of the most basic and important data types in R is a vector. A vector is simply a sequence of values – just like in maths. And you know what? You have already created vectors in R. In mathematics, any scalar value can be treated as a one-dimensional vector and it is exactly like that in R: any single value is a 1-element vector, including all the variables that you have created in the previous exercise.\nTo create a vector with more than one value, you can use the c() function (“c” stands for “combine”). For integer numbers, you can use the : operator to create a sequence of numbers. For example:\n\nsequence &lt;- 5:15\nnumbers &lt;- c(10, 42, 33, 14, 25)\nperson &lt;- c(\"January\", \"Weiner\", \"Hoppegarten\")\n\nIt is also possible to combine two vectors longer than 1 into one:\n\nfirst_v &lt;- c(1, 2, 3)\nsecond_v &lt;- c(4, 5)\ncombined_v &lt;- c(first_v, second_v)\nprint(combined_v)\n\n[1] 1 2 3 4 5\n\n\nYou can access individual elements of a vector using the [ ] operator:\n\nnumbers[1]\n\n[1] 10\n\nperson[2]\n\n[1] \"Weiner\"\n\n\nBut hey, I told you that every value is a vector in R, right? And that includes the indices 1 and 2 that you have just used. So, what would happen if we used more than two values as an index? Try it:\n\nnumbers[1:3]\n\n[1] 10 42 33\n\nperson[3:1]\n\n[1] \"Hoppegarten\" \"Weiner\"      \"January\"    \n\nsel &lt;- c(1, 5, 3)\nnumbers[sel]\n\n[1] 10 25 33\n\n\nAs you can see, not only can you use a vector as an index, but you can also use a variable as an index.\n\n\n\n\n\n\nDo not use a comma\n\n\n\nIt is tempting to select, say, first and the third element of a vector numbers by writing numbers[1, 3]. This will not work! As you will see tomorrow, this way of writing is for two-dimensional objects. You must use a vector as an index: numbers[c(1, 3)].\n\n\n\n\n\n\n\n\nVectors and indices\n\n\n\nIn many (most?) programming languages, the first element of a vector is accessed using the index 0. For example in Python, to access the first element of an array, you need to type array[0]. This has something to do with how computers work. In R, the first element is always 1 – R was designed by statisticians, and in mathematics we always start counting from 1. For some reason, this seems to make some computer scientists angry.\n\n\n\n\n1.5.2 Named vectors\nAccessing elements of a vector using indices is all well and good, but sometimes it can be very inconvenient, especially if the vectors are very long. Or maybe you do not remember the order in which you have stored the elements of the vector – was the last name first, or second element of the person vector?\nVectors allow you to name their elements. We can either define the names at the very beginning, when we create the vector, or we can add them later using the names() function. Here is how you can do it:\n\nperson &lt;- c(first=\"January\", last=\"Weiner\", city=\"Hoppegarten\")\n\nOnce you have named the elements of a vector, you can access them using their names:\n\nperson[\"city\"]\n\n         city \n\"Hoppegarten\" \n\nperson[c(\"first\", \"last\")]\n\n    first      last \n\"January\"  \"Weiner\" \n\n\n\n\n1.5.3 Assigning values to selected elements\nOK, one more thing about vectors. Above we have selected elements from a vector. It turns out, we can do more with that selections then just print it to a console:\n\nnumbers &lt;- c(10, 42, 33, 14, 25)\nsel &lt;- c(1, 5)\nnumbers[sel] &lt;- c(100, 500)\nnumbers\n\n[1] 100  42  33  14 500\n\n\nHere is what happened: we assigned new values to the first and the fifth element of the vector numbers. This is a very powerful feature of R and you will be using it a lot.\n\n\n\n\n\n\n\nExercise 1.6 (Accessing and modifying vectors)  \n\nCreate a vector with the first 10 prime numbers. Call it primes.\nHow do you access the 3rd, 5th and 7th prime number?\nWhat happens when you do primes[11]?\nWhat happens when you do primes[11] &lt;- 11?\nWhat happens when you do primes[-1]?\nChange the 3rd, 5th and 7th prime number to 100, 500 and 700, respectively.\n\n\n\n\n\n\n\n1.5.4 Vectorization\nVectors are very useful – but wait, there is more. What happens if we add a value to a vector? Try it:\n\nnumbers &lt;- c(10, 42, 33, 14, 25)\nnumbers + 10\n\n[1] 20 52 43 24 35\n\n\nAs you can see, R has added the value 10 to every single element of the vector numbers. The same thing happens with other operators, like -, * and /. Try it yourself.\nThis is called vectorization and it is one of the most powerful features of R compared to other languages. It will allow you to write very concise and, at the same time, readable code.\nThe vectorization works not only with operators like +, -, * and /, but with many functions. For example, it works with most of the mathematical functions like sin() or log(). Try it:\n\nlog10(numbers)\n\n[1] 1.000000 1.623249 1.518514 1.146128 1.397940\n\nsin(numbers)\n\n[1] -0.5440211 -0.9165215  0.9999119  0.9906074 -0.1323518\n\n\nHowever, there is a catch. What happens if you try to add two vectors when both of them with more than one element? First, let us try to add two vectors of the same length:\n\nnumbers1 &lt;- c(1, 2, 3)\nnumbers2 &lt;- c(4, 5, 6)\nnumbers1 + numbers2\n\n[1] 5 7 9\n\n\nAs you can see, R has added the first element of the first vector to the first element of the second vector, the second element of the first vector to the second element of the second vector, and so on. Makes sense, right? Same would happen if we were to subtract, multiply or divide the vectors (or use logical operations, which you will learn on Day 3). This does not look like much now, but trust me, it will be extremely useful in the future.\nHowever, if the vectors have different lengths, it is a different story altogher. Take a look:\n\nnumbers1 &lt;- c(1, 2, 3)\nnumbers2 &lt;- c(4, 5)\nnumbers1 + numbers2\n\nWarning in numbers1 + numbers2: longer object length is not a multiple of shorter object length\n\n\n[1] 5 7 7\n\n\nOoops, what exactly happened here? First thing to note is that there was no error. There was a warning, but still our code executed and produced a result. But what is that result? For the first element of the result, it is clear enough: 1 + 4 = 5. Same for the second, 2 + 5 = 7. But what about the third? It seems that R added 3 + 4 = 7. But why?\nR noticed that it is missing an element to be added to the third element of the vector numbers1. So, it did what is called recycling. It “rewound” the vector numbers2 to the beginning and added the first element of numbers2 to the third element of numbers1. However, since after the rewinding and adding one element of vector numbers2 was left (because numbers1 did not have any more elements), R issued a warning. If the length of the first vector was a multiple of the length of the second vector, R would not have complained:\n\nnumbers1 &lt;- c(1, 2, 3, 4, 5, 6)\nnumbers2 &lt;- c(7, 8)\nnumbers1 + numbers2\n\n[1]  8 10 10 12 12 14\n\n\nSee? No warning. R was recycling the second vector over and over again. Recycling is a dangerous business: if you are not careful, you can get results which you have not expected.\n\n\n\n\n\n\nRecycling advice\n\n\n\nHere is our advice to you: either use a vector and a single element vector, or two vectors of the same length. And in the cases where, for some reason, you need to recycle, make sure that you know what you are doing. For example, check the length of both vectors.\n\n\nWith vectors that have only a couple of numbers it is quite easy to see what is happening, but what if you have thousands of variables? In other words, how to check the lenght of a vector? You can use the length():\n\nlength(numbers1)\n\n[1] 6\n\nlength(numbers2)\n\n[1] 2\n\n\n\n\n\n\n\n\n\nExercise 1.7 (Vectorisation)  \n\nCreate a vector with several numbers and try to add, subtract, multiply and divide it by a single number. What happens?\nSay, you have three values which are the diameters of three circles: 1, 5 and 13. You would like to have a vector containing the areas of these circles. What is the simplest way of doing that?\nHow do you check the length of this vector?\nOne vector, lengths, contains the lengths of the sides of three rectangles, and the other, widths, contains their widths. Create a vector containing the areas of these rectangles.\n\n\n\n\n\n\n\n1.5.5 The special value NA\nOne more thing: there are a couple of special values in R that you should know about. One of the most prominent, useful and frequently encountered is the NA value, which stands for “Not Available”. You will see it frequently when you work with data.\nIt is possible to apply mathematical operations to NA values, but the result is inadvertently NA:\n\nNA + 1\n\n[1] NA\n\nnumbers &lt;- c(1, 2, NA, 4)\nnumbers * 3\n\n[1]  3  6 NA 12\n\n\nThis also goes for some functions, which, quite often, have a special parameter to omit the NA values. For example, the mean() function calculates the mean of a vector:\n\nnumbers &lt;- c(1, 2, NA, 4)\nmean(numbers)\n\n[1] NA\n\nmean(numbers, na.rm=TRUE)\n\n[1] 2.333333\n\n\n\n\n\n\n\n\nUseful functions\n\n\n\nThere is a whole bunch of functions that you can use to work with vectors, and here are some of them – with mostly self-explanatory names: sum(), min(), max(), range(), sd(), var(), median(), quantile(). Look them up in the help system by typing, for example, ?sum in the console.\n\n\nThe NA value very frequently pop up when you try to convert a character vector holding what looks like numbers into a numeric vector. We will see many such examples in the days to come; the conversion is often done using the as.numeric() function. For example, it is quite common that values  typed in a spreadsheet contain comments or values which look like this &gt; 50 (measurement out of range).as.numeric()\n\nimported_data &lt;- c(\"10\", \"20\", \"30\", \"&gt; 50\", \"40\", \"N.A.\", \"60 (unsure)\")\n# this will generate a warning\nimported_data &lt;- as.numeric(imported_data)\n\nWarning: NAs introduced by coercion\n\nimported_data\n\n[1] 10 20 30 NA 40 NA NA\n\n\nAs you can see, R conveniently warns you that some elements of the vector were changed to NA. Look out for that warning!\n\n\n\n\n\n\nSpecial values\n\n\n\nThere are a few other special values in R that behave similarly to NA. Inf stands for infinity, you will get it when you divide a positive number by zero: 1/0. -Inf is the negative infinity (when you divide a negative number by 0), and NaN stands for “Not a Number” – this is what you get when you try to subtracting Inf - Inf or dividing 0/0. They have also other uses – for example, if a function wants to know how many rows of output you would like to see, and your answer is “all of them”, you can use Inf as the number of rows.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#putting-it-all-together",
    "href": "day1-intro-to-r.html#putting-it-all-together",
    "title": "1  Introduction to R",
    "section": "1.6 Putting it all together",
    "text": "1.6 Putting it all together\n\n1.6.1 Water lillies on a lake\nThere is an old puzzle that goes: “On a lake, there is a water lily. Each day the lily doubles in size. After 30 days, the lily covers the entire lake. On which day was the lily covering half of the lake?”.\nIn the following section we will model the behavior of the lily using R. Let us start with some assumptions. First: we designate the first day as Day 1. Second: every day, the lily doubles the area it covers. Third: on the first day, the lily covers 1% of the lake.\n\n\n\n\n\n\n\nExercise 1.8 (Modelling water lilies) Take a piece of paper and a pen. Your task is to come up with a formula to describe the area of the lily on the \\(n\\)-th day. Write down the formula.\nHint: if you are stuck, try to calculate the area of the lily on the first few days.\n\n\n\n\nThere is an important point that we wish to demonstrate here. Quite often it pays off to close your laptop and think for a moment what it is it that you want to do, rather than start coding right away. Pen and paper are helpful (we will be making this point again when it comes to visualizations). If you do not have a clear idea of what you want to do, you can get stuck thinking about what you already know how to do.\nThe formula for calculating the area of the lily on the \\(n\\)-th day is \\(0.01 \\times 2^{n-1}\\). You can come up with that result quite easily if you consider the first few days. On day 1, the area is \\(1\\% = 0.01\\). On day 2, is twice that, that is, \\(0.01 \\times 2 = 0.01 \\times 2^1 = 0.02\\). On day 3, it is twice the area from the previous day: \\(0.02 \\times 2 = 0.04 = 0.01 \\times 2\n\\times 2 = 0.01 \\times 2^2\\). And again, on day 4, it is \\(0.01 \\times 2^3\\). And so on2. We can show it in a table:\n\n\n\n\n\n\n\n\n\nDay\nArea\nCalculation\nFormula\n\n\n\n\n1\n0.01\n\\(0.01\\)\n\\(0.01 \\times 2^0\\)\n\n\n2\n0.02\n\\(0.01 \\times 2\\)\n\\(0.01 \\times 2^1\\)\n\n\n3\n0.04\n\\(0.01 \\times 2 \\times 2\\)\n\\(0.01 \\times 2^2\\)\n\n\n4\n0.08\n\\(0.01 \\times 2 \\times 2 \\times 2\\)\n\\(0.01 \\times 2^3\\)\n\n\n5\n0.16\n\\(0.01 \\times 2 \\times 2 \\times 2 \\times 2\\)\n\\(0.01 \\times 2^4\\)\n\n\n\nOnce we have the formula, it is very easy to calculate the area covered by water lillies on the first 10 days. We will use vectorization to do this:\n\ndays &lt;- 1:10\narea &lt;- 0.01 * 2^(days - 1)\narea\n\n [1] 0.01 0.02 0.04 0.08 0.16 0.32 0.64 1.28 2.56 5.12\n\n\nThis calls out for a plot. We will talk about visualizations more extensively on Day 5, but for now, we will use a very basic and simple function to plot the area of the lily on the first 30 days. The function is called plot() and can be used to plot a graph of two vectors. The first vector is the days, the second vector is the area.\n\nplot(days, area, type=\"b\")\n\n\n\n\n\n\n\n\nO-K, days and area are clear, but what is this type=\"b\"? This is a so-called named argument3. The plot() function has many arguments, and if you want to use only some of them, you can use their names with an equal sign. You will see that a lot in the days to come. This particular argument, type, tells R what kind of plot to draw. The \"b\" stands for “both” and tells R to draw both points and lines. If you want only points, you can use \"p\" (or simple leave the argument out), if you want only lines, you can use \"l\".\nNote another thing on this plot: after day 7, the area is greater than 1. But 1 means 100%, so after day 7, the lily covers more than the entire lake. Obviously, this is not possible – and it shows a limitation of our model. We can show it by adding a horizontal line to the plot:\n\nplot(days, area, type=\"b\")\nabline(h=1, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.9 (Plotting water lilies)  \n\nCreate the same plot using the plot() function, but add, as a parameter, col=\"blue\". What happens?\nUse the argument xlab to label the x-axis with “Day”. Use the argument ylab to label the y-axis with “Area”. Use the argument main to give the plot a title.\nUse the argument ylim=c(0, 1) to change the range of the y-axis. How would you change the limit of the x axis?\nWhat is the formula for the area of the lily assuming that each day, the lily covers 1.5 times the area of the previous day?\nCreate a new area vector (call it area_slow) which will be calculated with the new formula.\nAdd the new vector to the plot using the function lines(). What does the lines() function do? (Hint: type ?lines in the console).\n\n\n\n\n\n\n\n1.6.2 Functions in R\nR is a so-called functional language. This is different from many other languages (including Python). It has some interesting implications which we will partially explore over the next few days. For now, however, we will be content with one important statement: in R, most of the stuff you do, you do using functions. A function takes zero or more arguments and returns exactly one argument.\nDuring this course, we will not really discuss or require creating your own functions. However, we would nonetheless like to show you how it is done. There are two reasons for that. Firstly, it is really, really easy. Secondly, it will help you understand how functions work in R, and that will help you understand how to use functions that others have created.\nIn the water lilies example we have used a formula to calculate the area of the lily on the \\(n\\)-th day. The formula includes two parameters: the initial fraction of the area covered by the lily on day 1, and the factor by which the area is increased each day. We can create a function that will take three parameters: the day (or days), the initial fraction and the factor, and return the area of the lily on that day. Here is how you can do it:\n\narea_lily &lt;- function(day, fct) {\n  ret &lt;- 0.1 * fct^(day - 1)\n  return(ret)\n}\n\nAs you can see, the function is created using the function keyword. In parentheses (( and )), you specify the arguments that the function takes, separated by commas. Then comes the body of the function, enclosed in the curly braces ({ and }). On the last line of the function code, the return() function is used to return the value of the area_lily function.\nOne interesting and important fact about the code above is that you use the assignment operator &lt;- to assign the function to a variable. In other words, area_lily is, in fact, a variable! A variable which holds not a value or character string, but computer code that can be used to do stuff.\nOnce you have run the code above, you can use it to calculate the area of the lily on the first 10 days like this:\n\narea &lt;- area_lily(1:10, 2)\nprint(area)\n\n [1]  0.1  0.2  0.4  0.8  1.6  3.2  6.4 12.8 25.6 51.2\n\n\n\n\n\n\n\n\n\nExercise 1.10 (Creating your own function) Modify the area_lily function so that it takes three arguments: the day, the initial fraction and the factor. Use the new function to calculate the area of the lily on the first 10 days with the initial fraction of 0.001 and the factor of 1.5. What is the area on the 10th day?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#coding-practices",
    "href": "day1-intro-to-r.html#coding-practices",
    "title": "1  Introduction to R",
    "section": "1.7 Coding practices",
    "text": "1.7 Coding practices\n\n1.7.1 Computer programs as means for communication\nIt is now time to conclude today’s lesson with a bit of philosophy. When you write an R script, the first goal you have in mind is to analyse your data – in other words, by means of what you write you are trying the computer to do something for you. That is correct and fine, but there is an important aspect of programming that is often overlooked.\nWhen you write a program, you are writing it not only for the computer, but also for other people. These other people may include your colleagues, readers of your scientific articles, your students, and, last but not least, a future version of yourself. All these people need more then just a piece of code that works. You will quickly find it out yourself when you open a script or a project that you have not been working on for a few months – trust me on this, you will not know what it does, how it does and sometimes even whether you have written it or copied from somewhere.\nYou might be thinking that you are never willing to show your code to another person. You are wrong, and not only because it is useful to you for another person to review your code. Firstly, you will want to share your code because as a scientist you will want to share your results, and results are nothing if the methods to obtain them are unknown. And secondly, you will need to share your code because you will be asked to do so by your colleagues (yes, I was as surprised as you will be when I was asked to share my code for the first time). And thirdly, your code is part of your methods and you will have to share it when you publish your results4.\nFor communication with another human being to be efficient, you need to make it as clear as possible. There are several ways how to make your code more readable and understandable. Here are some of them.\nComments. Comments are lines in your code that are not executed. In R, they start with a # sign. Comments help to explain what exactly are you trying to achieve with your code. The old saying goes: “Code tells you how, comments tell you why”. You can hardly overdo with comments, but you can easily underdo.\nNaming. The names of your variables, functions and files should be meaningful. If you have a variable that stores the number of days, call it days, not x. If you have a function that calculates the area of a circle, call it calculate_circle_area(), not f(). If you have a comma separated values (CSV) file that contains the CRP values, call it crp_values.csv, not data.txt.\nFormatting. Your code should be formatted in a consistent way. For example, you should always put a space around your operators, like x &lt;- 2 (and not x&lt;-2), you should always put a space after a comma, like c(1, 2) and not c(1,2) (and also not c( 1 , 2 )). Lines should not be too long – 80 characters at most is a good rule of thumb. If a line is too long, you can split it into several lines – R will not mind. See here for a more detailed guide on how to format your code.\n\n\n1.7.2 Example\nThe following fragment of code shows how you should not format your code:\n\na&lt;- 4\nb &lt;-c(1,10, \n20, 21, 5)\nr&lt;-sqrt(sum((b-mean(b))^2)/\n               a)\n\nThe code is correct, but it is hard to read. What does it do, quickly? If you carefuly examine it, you will see that it calculates the standard deviation of the vector b, following the formula\n\\[SD = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\]\nwhere \\(n\\) is the length of the vector b, \\(x_i\\) is the \\(i\\)-th element of the vector b and \\(\\bar{x}\\) is the mean of the vector b. However, there are several issues.\nFirstly, there are no comments in the code which would give a hint of what it does. Secondly, \\(n - 1\\) (the variable a) is hard encoded - if you modify the vector b by adding one number, the code will execute, but the result will be incorrect.\nThirdly, line 4 combines several operations making it very hard to read. It should be split for clarity. The following code is much more readable:\n\n# ---------------------------------------------------------\n# Calculating the standard deviation of a sample\n# ---------------------------------------------------------\n\n# example values for five samples\nsamples &lt;- c(1, 10, 20, 21, 5)\nsamples_n &lt;- length(samples)\n\n# calculate standard deviation of samples manually\nsamples_mean &lt;- mean(samples)\nsamples_devs &lt;- samples - samples_mean\n\n# samples variance\nsamples_var &lt;- sum(samples_devs^2) / \n                (samples_n - 1)\n\nsamples_sd &lt;- sqrt(samples_var)\n\nThis makes it absolutely clear what you are trying to do, and, in addition, calculates the mean, the deviations, sample length and sample variance – all of which might come in handy later on. Also note of the use of # ----… comments. Programmers often use these to highlight the beginning of a new section of code. This is not necessary, but adds to readability. Lines 14 and 15 show how you can split a line of code in a way that is both readable and clear.\nOf course, the example is a bit silly – R has a lot of built-in statistical functions, and standard deviation naturally is one of them. You can calculate the standard deviation of a vector b using the sd() function:\n\nsamples_sd &lt;- sd(samples)\n\nNonetheless, the principle stands.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#review",
    "href": "day1-intro-to-r.html#review",
    "title": "1  Introduction to R",
    "section": "1.8 Review",
    "text": "1.8 Review\nThings that you learned today:\n\nWorking with R and RStudio\n\nCreating and running scripts\nUsing the console\nUsing the “Environment” panel of RStudio\nUsing comments with the # sign\nUsing tab completion\n\nVariables\n\nAssigning values to variables\nUsing variables in calculations\nUsing character variables\npasting together character variables with paste() and paste0()\n\nVectors\n\nCreating vectors with c() and :\nAccessing elements of a vector with [ ]\nNamed vectors\nAssigning values to selected elements\nVectorization\nRecycling\n\nUseful functions\n\nsqrt(), log(), log2(), log10(), sin(), cos(), tan()\nsum(), mean(), min(), max(), range(), sd(), var(), median(), quantile()\n\nOther\n\nThe special values NA, Inf, -Inf, NaN\nLogical values TRUE and FALSE\nFunctions in R\nPlotting with plot()\nAdding parameters to functions\nCoding practices\n\n\nXXX",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "day1-intro-to-r.html#footnotes",
    "href": "day1-intro-to-r.html#footnotes",
    "title": "1  Introduction to R",
    "section": "",
    "text": "There are exceptions to this rule, but they are relatively rare and we will not discuss them here.↩︎\nIf started counting from 0 – that is, if we designated the first day as Day 0 – the formula would be \\(0.01 \\times 2^n\\).↩︎\nFull disclosure: all arguments can be named in R. However, some of the arguments have a default value, so we do not have to specify them unless we need them. The type argument is one of them. Others must always be specified. We will talk about this in more detail later.↩︎\nTop journals already require that you share your code when you publish your results. This will become more and more common in the future.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Five days of R\nI have been teaching R to biologists and medical students for many years now. At the Core Unit for Bioinformatics at the Berlin Institute of Health, Charité - Universitätsmedizin Berlin, we have developed a five-day, 5 hour per day crash course running for the last three years. This book is a companion to that course.\nThis is also the reason for how the materials in the book are arranged. Rather then discussing everything about vectors first, then everything about matrices etc., we start with easy things, and return to them later to build on them. I call this “helical learning”1 – we spiral around the same topics, but each time going a bit deeper, and each time you will understand a bit more. This is also why some topics are spread between the days – by trial and error, we have found the amount of material that can be covered in a day of learning.\nThere are two goals of this book. The first one is that after five days of learning R, you will be able to load, inspect, manipulate and save data files (such as Excel tables or CSV files), make some basic plots and perform simple statistical tests. The second goal is that you are in a good starting position to continue learning R on your own.\nIn other words, this course should give you a jump start, allow to overcome this first hurdle of learning R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstalling R and RStudio\nBefore you dive in to the course, we would like to ask you to install both R and RStudio. The installation depends on your operating system, so please refer to specific instructions which can be found at the following links:\nXXX\n\n\nInstalling R packages\nDuring the course we will use several R packages that you need to install on your computer. On Day 2, we will discuss installing and loading packages, and we will make a note to install the required packages when they are needed. However, you can also install them right now, after you have installed R and RStudio. This can be more effective – after all, installing might take some time.\nHere is the list of packages that you will have to install:\n\ntidyverse\nggplot2\ncolorDF\npander\nreadxl\nwritexl\ntinytex (only if you want to produce PDF output from Rmarkdown)\n\nYou can install them by running the following code in your RStudio:\n\ninstall.packages(c(\"tidyverse\", \"ggplot2\", \"colorDF\",\n                  \"pander\", \"readxl\", \"writexl\",\n                  \"tinytex\"))\n\n\n\nInstalling Quarto\nOn Day 5, we will be discussing Rmarkdown. Your basic RStudio setup will allow you to create Rmarkdown documents, but if you wish, you can also use Quarto – which is a sort of “Rmarkdown on steroids”, much nicer looking and efficient. However, for this you need to install the Quarto program. See the Quarto website (https://quarto.org/docs/get-started/) for details.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-structure-of-this-book",
    "href": "intro.html#the-structure-of-this-book",
    "title": "Introduction",
    "section": "The structure of this book",
    "text": "The structure of this book\nThis book is divided into five chapters, each corresponding to one day of the course. At the beginning of each chapter, you will find a short list of topics for the given day.\nIn each chapter there are several exercises. However, that does not mean that you should only do the exercises. In fact, you should try out every piece of code that is in the book. Copy it (there is a button next to each code block that will do it for you), paste it into your RStudio and run it. Then try to modify it and see what happens.\nExercises in this book are important. They are not only there to check if you understood the material, but they can also introduce new concepts or ideas. This is because this book is not only about learning R, but also learning how to learn about R. So, for example, sometimes we will want you to figure stuff on your own rather than give you a ready-made answer.\nEach chapter is ended by a “Review” section, which contains a list of things that you have learned that day. It is really important that you go through that list and make sure that you understand everything on it. Some of the new things appeared in the exercises, so if you skipped them, you might want to go back and do them.\nIf you do all that, I personally guarantee you that by the end of this course you will be able to use R in your work.\n\n\n\n\n\n\nRemember!\n\n\n\n\nRun all code chunks in your RStudio.\nDo all exercises.\nGo through the “Review” section at the end of each chapter.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#general-advice",
    "href": "intro.html#general-advice",
    "title": "Introduction",
    "section": "General advice",
    "text": "General advice\nThe course is a real crash course. There is a lot of material coming at you in a very short time. You will feel overloaded and overwhelmed – this is normal. Don’t worry! It will soon get better, and in a few days you will be able to do fairly advanced things with R.\nThe key is to keep playing with your R; trying out new things, breaking it. Please go through all exercises in that book, even if they seem simple at the first glance (some of them are tricky, others are used to smuggle in new concepts and useful tidbits of information).\nWhenever you feel you don’t understand something, stop and try to figure it out. Use internet search very liberally. Many answers can be found on sites such as StackOverflow, R-bloggers, or in the R documentation. Try out the code you will find in these sources – just copy-paste it into your RStudio and adapt it to your needs. Feel free to use Large Language Models (such as GPT) – they are very good at explaining code, especially when you are learning basic concepts.\nHowever, if you want to learn R, simply doing this course will not be enough. You need to start using it in a real world setting. Unfortunately – the better you already are at Excel, Word and other such tools, the harder it will be switching to R: tasks that are a breeze in Excel will at first require you to spend substantially more time in R. However, trust me: it pays off in the long run. Therefore, for best results, force yourself to use R even if at first it is less efficient then other tools.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "I got this idea from the professor Barbara Płytycz from the Jagiellonian University, who taught me my first “helix” of the immune system.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html",
    "href": "day3-reading-your-data.html",
    "title": "3  Reading and Writing Files",
    "section": "",
    "text": "3.1 Aims for today\nToday is a special day. If there is one thing that I would like you to learn from this course, it is how to read and write the data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#aims-for-today",
    "href": "day3-reading-your-data.html#aims-for-today",
    "title": "3  Reading and Writing Files",
    "section": "",
    "text": "Reading data\nCleaning data\nRegular expressions\nData management",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#reading-data",
    "href": "day3-reading-your-data.html#reading-data",
    "title": "3  Reading and Writing Files",
    "section": "3.2 Reading data",
    "text": "3.2 Reading data\n\n3.2.1 Data types\nIn your work you will encounter many different types of data. Most frequently, you will work with tabular data, either as Excel files or as comma or tab separated values (CSV and TSV files, respectively). I am sure you have worked with such files before.\n To read these files, we will use two packages: readr and readxl. The former, readr, is part of the tidyverse package, so when you load the tidyverse using library(tidyverse), readr is loaded as well. The latter, readxl, is a separate package that you need to install and load separately.readr and readxl packages\n There are also “base R” functions read.table, read.csv, read.tsv (there is no function for reading XLS[X] files in base R). These are always available when you start R, but don’t use them. The tidyverse functions are not only faster, but also much better behaving and, which is most important, they are safer – it is less likely to mess up your data with them.read.table(), read.csv(), read.tsv()\n tidyverse functions return tibbles, which, as you remember from yesterday, are a special flavor of data frames. Just to refresh your memory, here are key facts about tibbles:tibbles\n\nin most of the cases, they behave exactly like data frames\nwhen you print them, they are nicer\ntibbles have no row names\nwhen you select columns using [ , sel ], you always get a tibble, even if you selected only one column\n\nread_table(), read_tsv(), read_csv(), read_delim(), read_xls(), read_xlsx(), read_excel()\n\n\n\n\n\n\n\n\n\nData type\nFunction\nPackage\nNotes\n\n\n\n\nColumns separated by spaces\nread_table()\nreadr/tidyverse\none or more spaces separate each column\n\n\nTSV / TAB separated values\nread_tsv()\nreadr/tidyverse\nDelimiter is tab (\\t).\n\n\nCSV / comma separated\nread_csv()\nreadr/tidyverse\nComma separated values\n\n\nAny delimiter\nread_delim()\nreadr/tidyverse\nCustomizable\n\n\nXLS (old Excel)\nread_xls() read_excel()\nreadxl\nAvoid using XLS files. From the readxl package.\n\n\nXLSX (new Excel)\nread_xlsx() read_excel()\nreadxl\nFrom the readxl package. You need to provide the sheet number you wish to read. Note: returns a tibble, not a data frame!\n\n\n\nAs you can see, the functions we recommend to use can be used by loading the packages tidyverse and readxl. If you haven’t done that yet, please install these packages now:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")\n\nHowever, before we start using these functions, we need to dive into a very important problem: where are your files?\n\n\n\n\n\n\nRemember!\n\n\n\n\nFor reading text files (csv, tsv etc.), use the readr package. This package is loaded automatically when you load the tidyverse package: library(tidyverse). Then, use the functions read_csv, read_tsv etc.\nFor reading Excel files, use the readxl package: library(readxl). Then, use the function read_excel.\n\n\n\n\n\n3.2.2 Where is my file? Relative and absolute paths\nWe are coming now to a crucial problem and a source of endless frustration for beginning R programmers: how to tell R where your file is. Fortunately, there are many ways to deal with that, both with R and RStudio. Still, this is a key problem and we would like you to spend some time on the following chapter.\n In order to access a file, a program (any program) needs what is known a path. Path is a character string that tells the program how to get to the file and looks, for example, like this on a Windows computer: C:/Users/johndoe/Documents/RProjects/MyFirstRProject/data.csv, and like this on a Mac: /Users/johndoe/Documents/RProjects/MyFirstRProject/data.csv.Path\n\n\n\n\n\n\nPath separators on different systems\n\n\n\nMost computer systems separate the directories and files in a path using a slash (/). However, Windows uses a backslash (\\). This is quite annoying for R users, because in R character vectors, the slash has a special meaning. To use backslashes, you need to “escape” them by putting another backslash in front of each backslash. So instead of C:\\Users\\johndoe\\Documents, you need to write C:\\\\Users\\\\johndoe\\\\Documents. Alternatively, you can use the forward slash even on a Windows system, so type C:/Users/johndoe/Documents. We recommend the latter approach.\nThis is also why simply copying a path from the Windows Explorer to your R script will not work in R – because the copied text contains single backslashes.\n\n\nIf R cannot find your file, it will return an error message. At first, you will be seeing this error a lot:\nError: file not found\n\nlibrary(tidyverse)\nmydata &lt;- read_csv(\"data.csv\")\n\nError: 'data.csv' not found in the current working directory\n('C:/Users/johndoe/Documents/RProjects/MyFirstRProject/').\n\n\n Before we proceed, you need to understand one important thing. When you start your RStudio and R session, the R session runs in a specific directory. This is called the working directory. You can check what is using the getwd() function1:getwd()\n\ngetwd()\n\n[1] \"C:/Users/johndoe/Documents/RProjects/MyFirstRProject\"\n\n\nOf course, the result will be different on your computer. By the way, the above is called an absolute path. That means that it works no matter where you are in your system, because a program can always find the file or directory using this path.\nThe easiest way to read the data is this: copy your data files to the directory returned by getwd().\n\n\n\n\n\n\n\nExercise 3.1 (Reading your first file)  \n\nCheck your working directory using getwd()\nLoad the tidyverse package using library(tidyverse)\nGo to the URL https://github.com/bihealth/RCrashcourse-book/Datasets\nClick on “iris.csv”\nClick on the “Download raw file” button on the right side of the screen\nSave the file in the directory returned by getwd()\nRead the file using read_csv(\"iris.csv\")\n\n\n\n\n\nThe following code should now work without an error:\n\nlibrary(tidyverse)\niris_data &lt;- read_csv(\"iris.csv\")\n\nNow the contents of the file are stored in the iris_data object. There are many ways to have a look at it:\n\ntype iris_data or print(iris_data) in the console\ntype View(iris_data) in the console\nclick on the little spreadsheet icon next to the iris_data object in the Environment tab in RStudio (upper right panel)\n\nPlease make sure that the above works for you. If it does not, read the instructions again. In RStudio, there is a “Files” tab in the lower right panel. You should see your working directory as well as the “iris.csv” file there.\n\n\n3.2.3 Reading with relative paths\nOK, so far, so good. That was easy. Now comes a slightly harder part.\nSaving your data files in the working directory works well if you have one or two. However, the more files you read and write, the more cluttered your project directory becomes. You will soon find yourself in a situation where you have no idea which file is which.\nIt is generally a good idea to keep your data files in a separate directory, or even multiple directories.\n\n\n\n\n\n\n\nExercise 3.2 (Reading your first file from a data directory)  \n\nIn the working directory, create a new directory called “Datasets”\nMove the “iris.csv” file to the “Datasets” directory\nRead the file using read_csv(\"Datasets/iris.csv\")\n\n\n\n\n\n The path “Datasets/iris.csv” is called a relative path, because it is relative to the working directory and will not work from another location. So why should we use relative paths? Wouldn’t it be easier to use absolute paths all the time, for example read_csv(\"C:/Users/johndoe/Documents/RProjects/MyFirstRProject/Datasets/iris.csv\")?Relative path\nActually, no. The problem is that if you move your R project to another location, of if you share it with someone else, the absolute path will no longer work. In other words, the absolute path is not portable.\n\n\n\n\n\n\nRemember!\n\n\n\nDo not use absolute paths in your code. Always use relative paths.\n\n\n\n\n3.2.4 More on relative paths\nSome times the data files are not in your R project directory. For example, you are writing your PhD thesis. You have created a directory called “PhD”, which contains directories “manuscript”, “data”, “images”, “R_project” and so on. You use R to do a part of the calculations, but you want to keep the data files in the “data” directory. How to read them?\nWhen you type getwd(), you will get the path to the “R_project” directory, something like C:/Users/johndoe/Documents/PhD/R_project. The date files are in the directory C:/Users/johndoe/Documents/PhD/data. To get the relative path from the R project directory to the data directory, think about how you would navigate from one directory to another in the Windows Explorer or Finder. You need to go up one level, to get into “PhD”, and then down again to “data”.\n Getting “up one level” in a path is done by using ... So the relative path to the file “iris.csv” in your “data” directory is ../data/iris.csv.Up one level with ..\n\n\n\n\n\n\n\nExercise 3.3 (Reading a file from a data directory using relative paths)  \n\nIn the directory that contains the working directory create a directory called “Data”. That is, if your working directory is C:/Users/johndoe/Documents/PhD/R_project, create the directory C:/Users/johndoe/Documents/PhD/Data\nMove the “iris.csv” file to the new “Data” directory\nRead the file using read_csv(\"../Data/iris.csv\")\n\n\n\n\n\nBut what about the case when your data directory is at a completely different location? For example, on a different drive, or maybe on your desktop?\nFirst, I don’t recommend keeping your data files separately from the R project directory. In general, try to put everything in one place, as part of one structure. This structure can be complex, but it should be coherent. If necessary, copy the data files into your R project directory.\nHowever, sometimes it simply isn’t possible. Maybe the files are huge and you need to read them from a special drive. In this case, there are three options.\nUsing absolute paths. Yes, I told you not to use absolute paths, but sometimes you have no choice.\nCreate shortcuts. In all systems it is possible to create shortcuts to your data directories (on Unix-like systems like MacOS they are called “symbolic links”). You can put these shortcuts in your R project directory – R will treat them as normal directories.\nCreate a complex relative path. Depending on how far “away” your data directory is, you can create a complex relative path. For example, if your R project directory is C:/Users/johndoe/Documents/PhD/R_project and your data directory is D:/Data, you can use the path ../../../../Data/iris.csv. Unfortunately, despite being a relative path, this is not very portable (and it is easy to lose count on the ..’s).\n\n\n3.2.5 Using RStudio to import files\nRStudio has a very nice feature that allows you to import files using a point-and-click interface. When you click on a data file in the “Files” tab (lower right panel), you will see two options: “View File” and “Import Dataset”. Choosing the latter opens a dialog window which shows a preview of the file and allows you to construct your read_csv, read_tsv or another command using a point-and-click interface. You also see a preview of the expression that will be executed.\nThis feature is particularly useful when you are not sure about the format of the data to import, e.g. what type of delimiter is used, how many lines should be skipped etc.\nThen you can click on “Import” to actually run the command, however I would recommend another approach. Clicking on “Import” runs the command directly in the console, bypassing your script – and you should always enter the code in your script before executing it.\nRather than clicking on “Import”, click on the little Copy icon next to the “Code preview” field, and then cancel the dialog. Paste the copied code into your script, and then run it.\nThere is one more thing to modify. The code generated by RStudio often uses absolute paths. Try to modify it to use relative paths to ensure portability of your script.\n\n\n\n\n\n\n\nExercise 3.4 (Reading data)  \n\nGo to the URL https://github.com/bihealth/RCrashcourse-book/Datasets\nDownload the following files:\n\nTB_ORD_Gambia_Sutherland_biochemicals.csv\niris.tsv\nmeta_data_botched.xlsx\n\nAlternatively, you can download the whole repository as a ZIP file and unpack it.\nSave the files in the “Data” directory in your working directory – or another location of your choice.\nRead the files using the appriopriate functions. Consult the table above for the correct function names, or use the RStudio data import feature. Make sure that you are using relative paths.\n\n\n\n\n\n\n\n3.2.6 Reading Excel files\nReading files sometimes requires diligence. This is especially true for Excel files – they can contain multiple sheets, tables often do not start on the first row etc.\n The package readxl (which you hopefully successfully used to read the XLSX file in the previous exercise) contains several example files. They have been installed on your system when you installed the package. Manually finding these example files is annoying, and that is why the readxl package provides a convenience function, readxl_example(), that returns the absolute path to the file (yes, I know what I said about absolute paths; this is an exception).Reading excel files with readxl package\nreadxl_example(),read_excel()\n\nlibrary(readxl)\nfn &lt;- readxl_example(\"deaths.xls\")\nprint(fn)\n\n[1] \"/home/january/R/x86_64-pc-linux-gnu-library/4.4/readxl/extdata/deaths.xls\"\n\ndeaths &lt;- read_excel(fn)\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n\n\nIf you view this file (or if you use the RStudio data import feature), you will notice that the actual data in this file starts on line 5; the first 4 lines contain the text “Lots of people simply cannot resist writing some notes at the top of their spreadsheets or merging cells”. To skip these lines, we need to use an argument to the read_excel function. If you look up the help file for the function (e.g. using ?read_excel), you will find the following line:\nskipping lines with read_excel(skip=...)\nskip    Minimum number of rows to skip before reading anything,\n        be it column names or data. Leading empty rows are \n        automatically skipped, so this is a lower bound. Ignored\n        if range is given.\nTherefore, we can modify the code above to skip the first four lines:\n\ndeaths &lt;- read_excel(fn, skip=4)\n\n\n\n\n\n\n\n\nExercise 3.5 (Reading data with options) If you take a closer look at the file deaths.xls, you will notice that there is some extra text at the bottom of the file as well. How can you omit that part when reading? Hint: there are two ways to do that. If in doubt, look up the “examples” section of the readxl helpfile.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#footnotes",
    "href": "day3-reading-your-data.html#footnotes",
    "title": "3  Reading and Writing Files",
    "section": "",
    "text": "You can change it using the setwd() function, but avoid doing that. This path leads to madness, trust me on that.↩︎\nIn this case, it was me.↩︎\nSee Day 1 for more information on NA values.↩︎\nIn base R, you can use the gsub function. However, it has a slightly different syntax, which makes it less convenient when you start using pipes tomorrow.↩︎\nIn base R, there is the grep() function. It is very similar, but has a different syntax. We will stick to str_detect() for now.↩︎\nIn R, it is two backslashes. In other programming languages, it is usually a single backslash.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#review",
    "href": "day3-reading-your-data.html#review",
    "title": "3  Reading and Writing Files",
    "section": "3.8 Review",
    "text": "3.8 Review\nThings that you learned today:\n\nReading and writing\n\nread_csv, read_tsv, read_delim, read_xls, read_xlsx\nreading a file from working directory\nreading a file from a different directory using relative paths\n\nPaths\n\nGet the current working directory with getwd()\nrelative and absolute paths\n\nDiagnosing datasets\n\ndata types: str, class, typeof\nsummaries: summary, glimpse\ncategorical data: unique, table\nsummary_colorDF\ndiagnosis checklist\n\nCorrecting datasets\n\nconverting character vectors to numbers with as.numeric()\nchecking a vector with is.numeric()\nchecking a logical vector with any()\nreplacing values with str_replace_all()\n\nPrinciples of data management\n\nkeeping originals\nversioning\ndata editing\nworking with Excel\n\nOther\n\nusing help pages to find out about functions and their parameters\naccessing functions in a package without loading it (colorDF::summary_colorDF)\n\n\n\n\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann. 2021. “Gene Name Errors: Lessons Not Learned.” PLoS Computational Biology 17 (7): e1008984.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#reading-diagnosing-and-cleaning-data",
    "href": "day3-reading-your-data.html#reading-diagnosing-and-cleaning-data",
    "title": "3  Day 3: Reading and Writing Files",
    "section": "3.3 Reading, diagnosing and cleaning data",
    "text": "3.3 Reading, diagnosing and cleaning data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day 3: Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#diagnosing-and-cleaning-data",
    "href": "day3-reading-your-data.html#diagnosing-and-cleaning-data",
    "title": "3  Reading and Writing Files",
    "section": "3.3 Diagnosing and cleaning data",
    "text": "3.3 Diagnosing and cleaning data\n\n3.3.1 Diagnosing datasets\nCongratulations! You are now proficient in reading data into R. Believe me or not, this alone is an important step forward – many who try their hand at R get stuck at this point.\nHowever, reading data is only the first step. In most of the cases, the data requires some treatment: cleaning, transformation, filtering etc. In many projects that I have worked on as a bioinformatician, data import, diagnosis and cleaning took up the majority of time spent on the project. Unfortunately, this is necessary before any real fun with the data can start.\n Let us examine the file iris.csv that you have just read. The dataset comes from a famous paper by Ronald Fisher, who used it to demonstrate his newly developed method called linear discriminant analysis – an early machine learning algorithm, if you will. The dataset contains measurements (in cm) of 150 flowers of three species of irises: Iris setosa, Iris versicolor and Iris virginica. The measurements are the sepal and petal length and width, four measurements in total. Each row consists of these four measurements, plus a column which contains the species name.The iris dataset\nI have doctored the file to mimick typical problems with imported data, especially in clinical trial settings. Humans who enter data make errors, this is normal and expected, all of us do. Before we analyse them, we need to correct them. Before we correct them, we need to find them.\nFirst, note that when you read the data using read_csv, the function conveniently shows what types of data were assigned to each column:\n\niris_data &lt;- read_csv(\"Datasets/iris.csv\")\n\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Sepal Length, Petal?Length, Species\ndbl (2): Sepal Width, Petal.Width\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis should already ring some alarm bells. First, it looks like the columns are not consistently named: we have Sepal Length and Petal?Length, and also Petal.Width.\n Secondly, we would expect that the measurements are imported as numbers. However, both sepal and petal lengths are imported as characters. We can confirm this by using the class function:class()\n\nclass(iris_data[[\"Sepal Length\"]])\n\n[1] \"character\"\n\nclass(iris_data[[\"Sepal Width\"]])\n\n[1] \"numeric\"\n\nclass(iris_data[[\"Petal?Length\"]])\n\n[1] \"character\"\n\nclass(iris_data[[\"Petal.Width\"]])\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nclass and typeof\n\n\n\nThe class function returns the class of an object, which is a higher-level classification of the object. An object can have multiple classes. The typeof function returns the internal storage type of the object, which is a lower level classification. For example, both tibbles and data frames have the type list, but their classes are different. Another example:i if mtx is a matrix of numbers, typeof(mtx) is double, and class(mtx) is matrix.\n\n\nNote that instead of using iris_data$Sepal Length (which will not work, because there is a space in the column name), we use the double bracket notation. An alternative would be to use quotes: iris_data$'Sepal Length'. This a reason why want to avoid spaces and special characters in column names (in a moment we will show you how to standardize column names). If you use tab-completion with RStudio, the quotes will be inserted automatically.\n We can also use the summary function on the whole dataset:summary()\n\nsummary(iris_data)\n\n Sepal Length        Sepal Width     Petal?Length        Petal.Width   \n Length:150         Min.   : 2.000   Length:150         Min.   :0.100  \n Class :character   1st Qu.: 2.800   Class :character   1st Qu.:0.300  \n Mode  :character   Median : 3.000   Mode  :character   Median :1.300  \n                    Mean   : 3.411                      Mean   :1.199  \n                    3rd Qu.: 3.375                      3rd Qu.:1.800  \n                    Max.   :36.000                      Max.   :2.500  \n   Species         \n Length:150        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nOps, another problem. Did you notice the maximum value for sepal widths? It is 36 cm. This cannot be, way too large for the flowers. Also, both the mean and median are around 3 cm, so this must be either an outlier or a clerical error.\n\n\n\n\n\n\nThe summary() functions\n\n\n\nUnder the hood, there is no single summary() function. Instead, different classes can have different types of summaries. Whenever you produce a result, always try the summary function with it.\n\n\n Alternatively, we can use the tidyverse glimpse function. Rather then providing a summary, this function uses a terse format to show the data type for each column and first few values:glimpse()\n\nglimpse(iris_data)\n\nRows: 150\nColumns: 5\n$ `Sepal Length` &lt;chr&gt; \"5.1\", \"4.9\", \"4.7\", \"4.6\", \"5\", \"5.4\", \"4.6\", \"5\", \"4.…\n$ `Sepal Width`  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, …\n$ `Petal?Length` &lt;chr&gt; \"1.4\", \"1.4\", \"1.3\", \"1.5\", \"1.4\", \"1.7\", \"1.4\", \"1.5\",…\n$ Petal.Width    &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, …\n$ Species        &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"seto…\n\n\n Another way how we can diagnose the dataset is to use the str function. This provides a more detailed summary for each column:str()\n\nstr(iris_data)\n\nspc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Sepal Length: chr [1:150] \"5.1\" \"4.9\" \"4.7\" \"4.6\" ...\n $ Sepal Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal?Length: chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Sepal Length` = col_character(),\n  ..   `Sepal Width` = col_double(),\n  ..   `Petal?Length` = col_character(),\n  ..   Petal.Width = col_double(),\n  ..   Species = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n3.3.2 Using colorDF to diagnose datasets\n I have written a package called colorDF that creates yet another flavor of data frames, which is displayed in the console in color (if you haven’t installed it yet, do so now with install.packages(\"colorDF\"). However, we will only use one convenient function from this data set, summary_colorDF. Because summary_colorDF is too much to type, we will use a shortcut, scdf.colorDF,summary_colorDF()\n\nscdf &lt;- colorDF::summary_colorDF\nscdf(iris_data)\n\n# Color data frame (class colorDF) 5 x 5:\n │Col         │Class│NAs  │unique│Summary                                   \n1│Sepal Length│&lt;chr&gt;│    0│    37│5: 10, 5.1: 9, 6.3: 9, 5.7: 8, 6.7: 8, 5.…\n2│Sepal Width │&lt;dbl&gt;│    0│    25│ 2.00 [ 2.80 &lt; 3.00&gt;  3.38] 36.00         \n3│Petal?Length│&lt;chr&gt;│    0│    47│1.5: 13, 1.4: 12, 4.5: 8, 1.3: 7, 5.1: 7,…\n4│Petal.Width │&lt;dbl&gt;│    0│    22│0.1 [0.3 &lt;1.3&gt; 1.8] 2.5                   \n5│Species     │&lt;chr&gt;│    0│     6│virginica: 46, setosa: 45, versicolor: 42…\n\n\n Above we have created a copy of the summary_colorDF function in the variable scdf. This new variable behaves exactly like the original and you can use it just as you would use summary_colorDF. The colorDF::  prefix is there so that we don’t have to load the colorDF package. Instead we tell R to look for the function directly in the colorDF package.Copying functionspackage::function()\nThis summary function shows more than just the str function. In addition to column types, for character and factor columns it shows the unique values, ordered by their frequency. For numerical values, it shows their non-parametric summary statistics (range, median, quartiles).\n Hm, when you look at the output you might notice one more thing: for the last column, “Species”, summary_colorDF shows that there are six unique values. However, how can that be? We know that there are only three species in this dataset. We can check this using the unique function:unique()\n\nunique(iris_data[[\"Species\"]])\n\n[1] \"setosa\"     \"Setosa\"     \"versicolor\" \"Versicolor\" \"virginica\" \n[6] \"Virginica\" \n\n\nThere it is. Looks like whoever typed the data2, sometimes used a lower-case species designation, and sometimes upper-case. However, for R (and for computers in general), “versicolor” and “Versicolor” are two different things.\n More information can be gained using the table() function:table()\n\ntable(iris_data[[\"Species\"]])\n\n\n    setosa     Setosa versicolor Versicolor  virginica  Virginica \n        45          5         42          8         46          4 \n\n\nHere we not only see the unique values, but also their frequency. The table() function is very useful for any categorical data, as you will see later.\n\n\n\n\n\n\nLower and upper case\n\n\n\nFor computers in general and R in particular, “lowercase” and “Uppercase” are two different things. Variables a and A are different, as are versicolor and Versicolor. This is called case sensitivity.\n\n\n\n\n3.3.3 Checking individual values\nWe have seen before that some measurement columns were imported as character vectors, while others were correctly imported as numbers. To understand why, we need to have a closer look at the individual columns.\nOne of the columns that has been imported as a character vector is the “Sepal Length”. We will extract the column as a vector and then try to manually convert it to a number:\nas.numeric()\n\nsepal_length &lt;- iris_data[[\"Sepal Length\"]]\nas.numeric(sepal_length)\n\nWarning: NAs introduced by coercion\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0\n [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5\n [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2  NA 5.9 6.1\n [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5\n [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n[109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7  NA 6.0 6.9 5.6 7.7 6.3 6.7 7.2\n[127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\n\nTwo things to note: first, R issued a warning, “NAs introduced by coercion”. That means that some values could not be converted to numbers, and were replaced by the special value NA3. Second, there was no error. R happily allowed you to create a vector of numbers from a vector even though there were some problems.\nWhen you look at the output, you will easily spot the NA values. You could even figure out at which positions in the vector they occur. However, this is not the way to go – what if you have not 150 values, but 150,000?  Instead, use the is.na function:is.na()\n\n# convert to numbers\nsepal_length_n &lt;- as.numeric(sepal_length)\n\nWarning: NAs introduced by coercion\n\n# how many are NA?\nsum(is.na(sepal_length_n))\n\n[1] 2\n\n# which are NA?\nwhich(is.na(sepal_length_n))\n\n[1]  70 119\n\n# show the values\nsepal_length[is.na(sepal_length_n)]\n\n[1] \"5,6\"   \"&gt; 7.7\"\n\n\nOne more problem remains: the value “36” in the sepal width column. We can check which values are greater than 10 with a similar approach:\n\nsepal_width &lt;- iris_data[[\"Sepal Width\"]]\n\n# how many are greater than 10?\nsum(sepal_width &gt; 10)\n\n[1] 2\n\n# which are greater than 10?\nwhich(sepal_width &gt; 10)\n\n[1]  42 110\n\n# show the values\nsepal_width[sepal_width &gt; 10]\n\n[1] 23 36\n\n\n\n\n\n\n\n\n\nExercise 3.6 (Petal lengths) Repeat the steps above for the Petal length column.\n\n\n\n\n\n\n3.3.4 Diagnosing datasets: a checklist\nWhenever you read a data set, you should check the following things:\nColumn names. Are they consistent? Are they easy to type (no spaces, no special characters)? Are they in the correct order? Are they what you expect them to be? Do you understand what each column is?\nData types. Are the columns of the correct type (e.g. numerical columns imported as numbers)?\nCategorical variables. Do you see what you expect? Is the number of categories correct? Do you understand what each category is?\nNumerical variables. Do the summary statistics make sense? Are there any outliers? Missing values? Were the variables imported incorrectly as characters? Why?\nMissing values. Are there any missing values? Why? Are they important? How to deal with them?\n\n\n\n\n\n\nChecklist for importing data\n\n\n\n\nColumn names\nData types\nCategorical variables\nNumerical variables\nMissing values\n\n\n\n\n\n\n\n\n\n\nExercise 3.7 (Botched metadata) Load the file meta_data_botched.xlsx using the readxl package. Diagnose the problems.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Five days of R",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#five-days-of-r",
    "href": "index.html#five-days-of-r",
    "title": "Five days of R",
    "section": "Five days of R",
    "text": "Five days of R\nI have been teaching R to biologists and medical students for many years now. At the Core Unit for Bioinformatics at the Berlin Institute of Health, Charité - Universitätsmedizin Berlin, we have developed a five-day, 5 hour per day crash course running for the last three years. This book is a companion to that course.\nThis is also the reason for how the materials in the book are arranged. Rather then discussing everything about vectors first, then everything about matrices etc., we start with easy things, and return to them later to build on them. I call this “helical learning”1 – we spiral around the same topics, but each time going a bit deeper, and each time you will understand a bit more. This is also why some topics are spread between the days – by trial and error, we have found the amount of material that can be covered in a day of learning.\nThere are two goals of this book. The first one is that after five days of learning R, you will be able to load, inspect, manipulate and save data files (such as Excel tables or CSV files), make some basic plots and perform simple statistical tests. The second goal is that you are in a good starting position to continue learning R on your own.\nIn other words, this course should give you a jump start, allow to overcome this first hurdle of learning R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Five days of R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstalling R and RStudio\nBefore you dive in to the course, we would like to ask you to install both R and RStudio. The installation depends on your operating system, so please refer to specific instructions which can be found at the following links:\nXXX\n\n\nInstalling R packages\nDuring the course we will use several R packages that you need to install on your computer. On Day 2, we will discuss installing and loading packages, and we will make a note to install the required packages when they are needed. However, you can also install them right now, after you have installed R and RStudio. This can be more effective – after all, installing might take some time.\nHere is the list of packages that you will have to install:\n\ntidyverse\nggplot2\ncolorDF\npander\nreadxl\nwritexl\njanitor\ntinytex (only if you want to produce PDF output from Rmarkdown)\n\nYou can install them by running the following code in your RStudio:\n\ninstall.packages(c(\"tidyverse\", \"ggplot2\", \"colorDF\",\n                  \"pander\", \"readxl\", \"writexl\",\n                  \"tinytex\"))\n\n\n\nInstalling Quarto\nOn Day 5, we will be discussing Rmarkdown. Your basic RStudio setup will allow you to create Rmarkdown documents, but if you wish, you can also use Quarto – which is a sort of “Rmarkdown on steroids”, much nicer looking and efficient. However, for this you need to install the Quarto program. See the Quarto website (https://quarto.org/docs/get-started/) for details.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#the-structure-of-this-book",
    "href": "index.html#the-structure-of-this-book",
    "title": "Five days of R",
    "section": "The structure of this book",
    "text": "The structure of this book\nThis book is divided into five chapters, each corresponding to one day of the course. At the beginning of each chapter, you will find a short list of topics for the given day.\nSome parts of the book are highlighted:\nCode blocks with output:\n\n# this is a comment\nx &lt;- 1 + 1\n\nThe numbers on the left (if present) are not part of the code – they are just line numbers. You can copy the code by clicking on the “Copy” button and it will not copy these numbers.\nExercises:\n\n\n\n\n\n\n\nExercise 1 (Example)  \n\nThis is how an exercise looks like!\nPlease do all exercises. It helps a lot.\nSome things are learned only through exercises.\n\n\n\n\n\nTips:\n\n\n\n\n\n\nUseful tips\n\n\n\n\nSome exercises have solutions in the “Solutions” chapter. If they do, please read the solution after you have completed the exercise – often there will be a comment or a hint that will help you understand the material.\n\n\n\n\n\n\n\n\n\nRemember!\n\n\n\n\nRun all code chunks in your RStudio.\nDo all exercises.\nGo through the “Review” section at the end of each chapter.\n\n\n\nNew concepts are highlighted on the right margin of the book\nTake a look at the right margin!\nIn each chapter there are several exercises. However, that does not mean that you should only do the exercises. In fact, you should try out every piece of code that is in the book. Copy it (there is a button next to each code block that will do it for you), paste it into your RStudio and run it. Then try to modify it and see what happens.\nExercises in this book are important. They are not only there to check if you understood the material, but they can also introduce new concepts or ideas. This is because this book is not only about learning R, but also learning how to learn about R. So, for example, sometimes we will want you to figure stuff on your own rather than give you a ready-made answer.\nEach chapter is ended by a “Review” section, which contains a list of things that you have learned that day. It is really important that you go through that list and make sure that you understand everything on it. Some of the new things appeared in the exercises, so if you skipped them, you might want to go back and do them.\nIf you do all that, I personally guarantee you that by the end of this course you will be able to use R in your work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#general-advice",
    "href": "index.html#general-advice",
    "title": "Five days of R",
    "section": "General advice",
    "text": "General advice\nThe course is a real crash course. There is a lot of material coming at you in a very short time. You will feel overloaded and overwhelmed – this is normal. Don’t worry! It will soon get better, and in a few days you will be able to do fairly advanced things with R.\nThe key is to keep playing with your R; trying out new things, breaking it. Please go through all exercises in that book, even if they seem simple at the first glance (some of them are tricky, others are used to smuggle in new concepts and useful tidbits of information).\nWhenever you feel you don’t understand something, stop and try to figure it out. Use internet search very liberally. Many answers can be found on sites such as StackOverflow, R-bloggers, or in the R documentation. Try out the code you will find in these sources – just copy-paste it into your RStudio and adapt it to your needs. Feel free to use Large Language Models (such as GPT) – they are very good at explaining code, especially when you are learning basic concepts.\nHowever, if you want to learn R, simply doing this course will not be enough. You need to start using it in a real world setting. Unfortunately – the better you already are at Excel, Word and other such tools, the harder it will be switching to R: tasks that are a breeze in Excel will at first require you to spend substantially more time in R. However, trust me: it pays off in the long run. Therefore, for best results, force yourself to use R even if at first it is less efficient then other tools.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Five days of R",
    "section": "",
    "text": "I got this idea from the professor Barbara Płytycz from the Jagiellonian University, who taught me my first “helix” of the immune system.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#principles-of-data-management",
    "href": "day3-reading-your-data.html#principles-of-data-management",
    "title": "3  Reading and Writing Files",
    "section": "3.7 Principles of data management",
    "text": "3.7 Principles of data management\n\n3.7.1 Keeping originals\nAs every day, we end the day with some food for thought. Today, we would like to introduce a few important principles that you should follow in your everyday work with data – not only in R, but in general.\nThe first rule is: always keep the originals. Whether you have received the data from someone, extracted them from a database such as RedCap or created it yourself, once you start working with R you should freeze your data, version it and not touch it anymore.\nLet me reiterate: do not touch the original data. If you absolutely have to edit it by hand, make a copy and add a suffix like _edited or your initials to the file. However, it is better to perform all editing operations in R.\nVersioning. Always include a date, and possibly your initials and version in the file name. If you ever submitted a manuscript of a scientific paper, you know how quickly versions can accumulate. It is absolutely crucial to know which version of the data was used for which analysis.\nData editing. Every edit operation on your data should be somehow recorded. This is not different from a lab notebook, where you write down every step of your experiment. When working with a spreadsheet this is hard to do, therefore…\n\n\n3.7.2 Use R for data editing\nOptimally, your mode of operation should never involve editing data directly in a spreadsheet. You should read your data in R and perform all operations here, recording them in a script. This has two huge advantages:\n\nReproducibility. You and others will be able to trace back all your operations. There will be no doubt whether and which data were edited and how.\nAutomation. When you get updated data (and you will), for example when new samples arrived, you will be able to re-run your script (maybe with a few changes) and get the updated results with minimal effort. Otherwise, you will have to repeat every. Single. Edit. Again.\n\nConsider the example of the iris.csv file. We have seen that one of the values of sepal width seems to be missing a decimal dot. You could edit that in the Excel or CSV file directly. You might even make a note of that somewhere. However, chances are that you were wrong – maybe that was a flower wiht a 36 sepal width after all. This change influences your data and your analysis results and must be documented if you were to do science. If you did it in R, you probably have entered a comment in your script, something like this:\n\n# There seems to be a missing decimal dot in some of the sepal widths\n# Changing it to 1/10th of the value\n\nsepal_width &lt;- iris_data[[\"Sepal Width\"]]\nsel &lt;- which(sepal_width &gt; 10)\nsepal_width[sel] &lt;- sepal_width[sel] / 10\n\niris_data[[\"Sepal Width\"]] &lt;- sepal_width\n\nThis concisely informs the reader that a change was made, how and why.\nAnd what happens when Ronald Fisher raises from his grave and collects  another 150 flowers? You will get an updated data set with 300 flowers and import it again. In the best case scenario, you will find the problem again (or remember it). Then you will have to go into Excel and make the same edit again. And all the other edits as well. If you did it in R, you simply use your script again.undead Ronald Fisher\n\n\n3.7.3 Working with Excel and other spreadsheets\n We all use Excel, LibreOffice Calc or Google Sheets. They are great tools, no doubt about that. However, by now you can see that certain operations make working with R harder, and some even that can mess up your data.How to work with Excel\nIf possible, you should actually avoid working with Excel when working with scientific data. Excel does certain operations automatically (depending on the settings), so it can change your data without leaving a trace of the changes. This is the precise opposite of what you want in science.\n\n\n\n\n\n\nAvoid working with Excel\n\n\n\nExcel can automatically replace strings with dates, like changing MARCH9 into 9th of March. However, MARCH9 is a valid gene name. In fact, it turns out that a substantial proportion (about a third!) of all Excel files containing gene names and published as supplementary materials online contain gene names transformed to dates. Not only that, but even though that this has been discovered many years ago and even some genes were officialy renamed because of Excel, this is still a problem. And Excel is now able to recognize dates in many languages, exacerbating the problem (Abeysooriya et al. 2021).\n\n\nHowever, sometimes it is unavoidable to work with Excel.\nDon’t’s:\n\nUsing color and font to encode information, for example marking the different treatments with different background color or formatting controls in bold. While it is possible to read this information in R, it is not trivial. In addition, even outside of R formatting of cells can be lost when the data is passed around, and this would mean that the information is lost.\nComments in the same cells as values, for example 50 (measured twice). This prevents the columns to be interpreted as numbers and has to be dealt with in R. Create a separate column for comments, even it is just a single comment in a row.\nMeta-data information in the header. Very often, column names contain additional information, for example units, dates, encoding (like “0 for male, 1 for female”). This makes it hard to both, import the data and actually use that information. Create a separate sheet in your spreadsheet for meta data, with one row per column name.\nAdding header and tail information. Just like in the example with deaths.xlsx file, additional lines in spreadsheets require you to carefully select what you wish to import. Avoid it if you can, use another sheet.\nMore than one table on a spreadsheet. A spreadsheet should contain a single table. Otherwise, just like in case of header and tail information, you will have to spend some time cautiously selecting the areas that you want to import.\nMerging cells. Merged cells are a nightmare to work with in R. If you have to, unmerge them before importing the data.\n\nDo’s:\n\nUse a single table per sheet. If you have more than one table, use more than one sheet.\nUse a single header row. If you have more than one header row, use create another sheet with meta-data on the column names.\nSwitch off automatic conversion. Excel can automatically change your data in many ways. For example, it can change gene names to dates. It can also change the decimal separator, which can mess up your data. Newer versions of Excel allow to switch off this behaviour, so do it (here are the instructions).\nControl your import. When importing data from CSV files to TSV files, Excel allows you to control which fields are imported as text, which as dates etc. Use this feature to make sure that your data is correctly imported.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#mending-the-data",
    "href": "day3-reading-your-data.html#mending-the-data",
    "title": "3  Reading and Writing Files",
    "section": "3.4 Mending the data",
    "text": "3.4 Mending the data\n\n3.4.1 Correcting column names\nColumn names should contain only letters, numbers and underscores. While all other characters are also allowed (so a column name can be 豚肉 \"temp.\"? (°C) 😀), it is way easier to work with columns with simple, standardized names.\nYou already know one way of dealing with this – simply assign new names to the columns:\n\ncolnames(iris_data) &lt;- c(\"sepal_length\", \"sepal_width\", \"petal_length\",\n                      \"petal_width\", \"species\")\ncolnames(iris_data)\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\n However, there is a quicker way. The janitor package contains a function called clean_names that does exactly that, automatically and for all  columns. Install it with install.packages(\"janitor\") if you haven’t installed it yet.janitor packageclean_names()\n\nlibrary(janitor)\niris_data &lt;- clean_names(iris_data)\ncolnames(iris_data)\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\n\nMy advice is to use clean_names on every dataset you import.\n\n\n3.4.2 Correcting outliers\nIn the column with sepal widths, which now has the name sepal_width, we found two values that probably lack the decimal point: 23 and 36. They are at positions 42 and 110, respectively.\nIt is tempting to simply substitute the data in the original data frame:\n\n# works, but don't do it\niris_data$sepal_width[42] &lt;- 2.3\n\nUnfortunately, this is not a good idea, because this solution is a bit like manual editing in a spreadsheet. There is a better way. First, generalize the issue. What seems to be the problem? The values that are larger than 10 are probably missing a decimal point. The simplest solution is to divide them by 10. Using logical vectors, we can do that for all the values that are larger than 10.\n\n# numbers that are missing a decimal point\ntoo_large &lt;- iris_data$sepal_width &gt; 10\n\niris_data$sepal_width[too_large] &lt;- iris_data$sepal_width[too_large] / 10\n\nThis a bit hard to read, so we make it more explicit:\n\n# numbers that are missing a decimal point\nsepal_width &lt;- iris_data$sepal_width\ntoo_large &lt;- sepal_width &gt; 10\n\nsepal_width[too_large] &lt;- sepal_width[too_large] / 10\nsepal_width[too_large]\n\nnumeric(0)\n\n\nLooks good. Finally, we assign the corrected values back to the data frame:\n\niris_data$sepal_width &lt;- sepal_width\n\nAs the last step, we need to check whether our approach worked for all  values in the column. For this, we will use the any() function, which returns TRUE if any of the values in a logical vector is TRUE:any()\n\nany(iris_data$sepal_width &gt; 10)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\nExercise 3.8 Can you spot what the potential problem with this approach is? Hint: what would happen if the original value was 1.10? Can you think of a better approach? Is it actually possible to make sure that our guess is correct?\n\n\n\n\n\n\n3.4.3 Correcting categorical data\nIn the column “species”, we found that there are two different values for the same species. There are many ways we can handle it. Obviously, we could manually assign the correct labels, but that would be a lot of work.\nA better way would be to somehow automatically convert all the values. In our case, the problem is quite simple: it is sufficient to put all the values in uniform lower case, so that Versicolor becomes versicolor.\n We can do that in one line of code thanks to the tolower function:tolower()\n\niris_data$species &lt;- tolower(iris_data$species)\n\n There is also a corresponding function, toupper, that converts all the letters to upper case.toupper()\nRemember to check that your changes were successful:\n\ntable(iris_data$species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nThis was a simple case, but what if we had more complex typos? For example, “i.versicolor”, “vers.”, “V” etc. In such cases, you would need to use a more sophisticated approach. This approach involves regular expressions, and will also help us to correct the sepal and petal lengths.\n\n\n3.4.4 Incorrectly imported numeric data\nWe have two columns which were imported as characters, but should be numbers. Their brand-new names are sepal_length and petal_length. All in all, there are 2 problems in the sepal length column and 5 in the petal length column, as shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\nColumn\nPosition\nOriginal\nCorrect\n\n\n\n\nsepal_length\n70\n5,6\n5.6\n\n\nsepal_length\n119\n&gt; 7.7\n7.7\n\n\npetal_length\n12\n1.6!\n1.6\n\n\npetal_length\n13\n1 .4\n1.4\n\n\npetal_length\n14\n1.1?\n1.1\n\n\npetal_length\n115\n5k.1\n5.1\n\n\npetal_length\n149\n5. 4\n5.4\n\n\n\n\n\nThe last column in the table above is our target – this is how we would like to correct the data. How can we do that?\n\n\n\n\n\n\nCorrecting values\n\n\n\nOf course, without additional information we can only guess that 5,6 should be 5.6. Maybe there were two measurements, 5 and 6? Maybe 5. 4 should be 5.04, and not 5.4? In a real-world scenario, you would need to consult the author(s) of the data, or check your lab book.\n\n\nIt is tempting to simply substitute the data in the original data frame:\n\n# works, but don't do it\niris_data$sepal_length[70] &lt;- 5.6\n\nAs before, that is not a good idea. If you receive another version of the file, with added lines before the 70th, the position will change and you will have to correct the version manually again. Also, if there are new problems, like yet another typo in the data, you will have to spot it and manually add a line to correct it. Again, we need to generalize the issue.\nWhat is the problem on line 70? A comma instead of a decimal dot – maybe someone who typed it was used to entering numbers in a German version of Excel, where a comma is used instead of a dot. So why not replace all the commas by a dot? This should do the trick not only for line 70, but also for possible future problems.\n We will use for that the function str_replace_all from tidyverse4. Before we do that, however, we will make a copy of the column. That will allow us to easily see the changes will make, check that everything is OK and only then replace the column in the data frame. Later on, you will learn more efficient ways of doing this.str_replace_all()\n\n# make a copy\nsepal_length &lt;- iris_data$sepal_length\n\n# record the problematic places\nproblems &lt;- is.na(as.numeric(sepal_length))\n\nWarning: NAs introduced by coercion\n\nsepal_length[problems]\n\n[1] \"5,6\"   \"&gt; 7.7\"\n\n# replace the comma \nsepal_length &lt;- str_replace_all(sepal_length, \",\", \".\")\n\n# what was the result?\nsepal_length[problems]\n\n[1] \"5.6\"   \"&gt; 7.7\"\n\n\nIt worked! The str_replace_all is a general search-and-replace function. You can always use it with character vectors to replace stuff.\nHowever, we still have the problem with &gt; 7.7. You will find such values quite often in clinical settings, along with the counterpart &lt; 7.7. These may indicate that the measurement was out of range of an instrument. However, we cannot usually treat them as missing values, because they contain some information. Depending on whether or not we want to keep this information, we could either replace them by NAs, or decide to keep the value as is (in this case, change &gt; 7.7 to 7.7).\nIn the latter case, we can use the str_replace_all function again:\n\nsepal_length &lt;- str_replace_all(sepal_length, \"&gt; \", \"\")\n\nIn any case, the last point that remains is to convert the vector to numbers and assign it to the column holding sepal lengths. We do it in one go and also check if everything is OK:\n\n# finalize\niris_data$sepal_length &lt;- as.numeric(sepal_length)\n\n# check whether the column is numeric\nis.numeric(iris_data$sepal_length)\n\n[1] TRUE\n\n# check whether our problems are gone\niris_data$sepal_length[problems]\n\n[1] 5.6 7.7\n\n# check whether there are any NA's\nany(is.na(iris_data$sepal_length))\n\n[1] FALSE\n\n\n The new function, is.numeric(), checks whether the column sepal_length is indeed numeric. Finally, we make sure that no NA’s were produced in the conversion.as.numeric()\n\n\n\n\n\n\n\nExercise 3.9 Find the problems in the following vector and correct them:\nvec &lt;- c(\" 5\", \"5,6\", \"5.7\", \"5. 4\", \"&gt; 5.0\", \"6.O\")",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#writing-data",
    "href": "day3-reading-your-data.html#writing-data",
    "title": "3  Reading and Writing Files",
    "section": "3.6 Writing data",
    "text": "3.6 Writing data\n\n3.6.1 Keep your data organized\nWriting data is way easier than reading it. However, you always have to come up with a name for the file, and I would like you to follow a few points.\n\nNever overwrite the original file (we will discuss it in more detail shortly).\nCreate an output directory and save your files only there, do not mix the original and the output files.\nUse a meaningful name, so not output, but iris_cleaned. No spaces or special characters (use an underscore _ instead).\nThirdly, always version your files: add a date, your initials, a version number and a date, e.g. iris_cleaned_2021-09-01_JW_v1.csv.\nDo not use the old Excel XLS format (file extension .xls). Use the newer XLSX format (file extension .xlsx).\n\nThis latter point warrants an explanation. There are two main problems with that format. Firstly, it is less portable then XLSX (the new one) – so many programs can read XLSX, but not XLS (or, which is worse, they can, but read it incorrectly). Secondly, the number of rows and columns in an XLS file is severly limited (65536 rows and 256 columns). These limits are easily reached in modern bioinformatic datasets.\n\n\n3.6.2 Functions to use for writing data\nwrite_tsv(), write_csv(),write_xlsx()\n\n\n\n\n\n\n\n\n\nData type\nFunction\nPackage\nNotes\n\n\n\n\nTSV / TAB separated values\nwrite_tsv()\nreadr\nNo rownames!\n\n\nCSV / comma separated\nwrite_csv()\nreadr\nNo rownames!\n\n\nXLS (old Excel)\n\n\nJust don’t use it. No, seriously, don’t.\n\n\nXLSX (new Excel)\nwrite_xlsx()\nwritexl\nNo rownames!\n\n\n\nJust as in the case of reading data, there are several functions to write data. There are also functions in the base R that can be used for writing (e.g. write.csv), but they are not recommended. We advise you to only use the readr package for writing TSV and CSV files, and the writexl package for writing XLSX files.\nHowever, keep in mind that row names are not exported with these packages. That is why we do not use row names in this course.\n\n\n\n\n\n\n\nExercise 3.14 (Writing data)  \n\nIn your project directory, create the directory “Data_clean” (if you want, you can use the R function dir.create() for that).\nWrite the cleaned iris_data to a new file in the “Data_clean” directory. Use the write_csv function from the readr package. Use a meaningful name, version it and use a date.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day3-reading-your-data.html#regular-expressions",
    "href": "day3-reading-your-data.html#regular-expressions",
    "title": "3  Reading and Writing Files",
    "section": "3.5 Regular expressions",
    "text": "3.5 Regular expressions\n\n3.5.1 Introduction to regular expressions\n Regular expressions, regexps for short, are for finding certain patterns in the data and handling them automatically. They are quite old (going back at least to 1960’s) and have remained virtually the same for many decades. You will find them in almost all programming languages, and while details may differ, the principles are usually the same. The regexps in R are very similar to regular expressions in other programming languages such as Python. As you may guess, they have a different syntax then R – they are something else altogether.Regular expressions\nWe will not go in depth in regular expressions in R here. Here, just a short primer.\n Before we start, let us introduce a new function called str_detect()5. The function is available as soon as you load the tidyverse package. This function is used to search for a pattern in a character vector. For every element that matches the pattern, it will return a TRUE, otherwise a FALSE. Say, we have a vector (or data frame column) containing sample names, and we want to find all the controls.str_detect()\n\nsamples &lt;- c(\"ko_1_ctrl\", \"ko_2_ctrl\", \"ko_1_treat\", \"ko_2_treat\",\n            \"wt_1_ctrl\", \"wt_2_ctrl\", \"wt_1_treat\", \"wt_2_treat\")\nstr_detect(samples, \"ctrl\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\nsamples[str_detect(samples, \"ctrl\")]\n\n[1] \"ko_1_ctrl\" \"ko_2_ctrl\" \"wt_1_ctrl\" \"wt_2_ctrl\"\n\n\nIn case that someone did not pay attention to lower or upper case, we can tell str_detect to ignore the case:\n\nsamples &lt;- c(\"ko_1_ctrl\", \"ko_2_CTRL\", \"ko_1_treat\", \"ko_2_treat\",\n            \"wt_1_CTRL\", \"wt_2_ctrl\", \"wt_1_treat\", \"wt_2_treat\")\n# this does not work\nstr_detect(samples, \"ctrl\")\n\n[1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n# but this does\nstr_detect(samples, regex(\"ctrl\", ignore_case=TRUE))\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\nHere we are looking for a literal string “ctrl”. However, we can use regular expressions to search for more complex patterns. What if the sample names are more hap-hazard, like ko_1_control, ko_2_kontrol, ko_1_ctrl?\n\nsamples &lt;- c(\"ko_1_control\", \"ko_2_ctrl\", \"ko_1_trt\", \"ko_2_treatment\",\n            \"wt_1_Control\", \"wt_2_kontrol\", \"wt_1_Trtmt\", \"wt_2_treated\")\n\nOK, so we got control, ctrl, Control and kontrol. A pattern emerges:\n\nfirst letter is either k or c or C\nthen we might have an on (or not)\nnext always comes tr\nthen we might have an o\nthen we always have an l\nthen the word ends.\n\nAll this “mights” and “ors” and “eithers” and “always” can be encoded in a regular expression:\n\n\n\nPattern\nExplanation\n\n\n\n\n[kcC]\nmust have one of the letters k, c or C\n\n\no?\nzero or one o (i.e., “there might be an o”)\n\n\nn?\nzero or one n (i.e., “there might be an n”)\n\n\ntr\nmust have literally the string tr\n\n\no?\nzero or one o (i.e., “there might be an o”)\n\n\nl\nmust have literally the string l\n\n\n$\nhere must be the end of the string\n\n\n\nTaken together, this gives us the regular expression [kcC]o?n?tro?l$. This looks weird, but has the magic power of matching all the control-like strings in our vector. Let us try it:\n\nstr_detect(samples, \"[kcC]o?n?tro?l$\")\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\nsamples[str_detect(samples, \"[kcC]o?n?tro?l$\")]\n\n[1] \"ko_1_control\" \"ko_2_ctrl\"    \"wt_1_Control\" \"wt_2_kontrol\"\n\n\nThis works, but… what does that all mean?\n\n\n\n\n\n\n\nExercise 3.10 (Quick question) Look at the table above. Can you think of other patterns that would match this regular expression? Would kotrl work? What about cotro?\n\n\n\n\n\n\n3.5.2 How regexp works\n A regular expression is a sequence of characters – a string. However, some of these strings have a special meaning. Here is a short table, we will discuss these in more detail in a moment:regexp rules\n\n\n\nCharacter\nMeaning\n\n\n\n\na\na literal character a\n\n\n.\nany character\n\n\n[abc]\none of the characters a, b or c\n\n\n[^abc]\nany character except a, b or c\n\n\n[a-z]\nany character from a to z\n\n\n[0-9]\nany digit\n\n\n\nThese are the so-called “atoms”. They stand for something that we want to match. In the simplest case (like the a above), they stand for themselves. Also, did you notice the square brackets? They have nothing to do with how we use square brackets in R.\nIn addition we can specify how many times we want to match the atom:\n\n\n\nQuantifier\nMeaning\n\n\n\n\n?\nzero or one\n\n\n*\nzero or more\n\n\n+\nat least one\n\n\n\nThen, we can “anchor” the pattern in our string:\n\n\n\nAnchor\nMeaning\n\n\n\n\n^\nstart of the string\n\n\n$\nend of the string\n\n\n\nLet us put that together. Say we have the following strings:\n\nstrings &lt;- c(\"a\", \"ab\", \"ac\", \"ad\", \"bc\", \"abc\", \"bac\", \"cab\")\n\nTo match these which contain an a, we can use the regular expression a:\n\nstrings[str_detect(strings, \"a\")]\n\n[1] \"a\"   \"ab\"  \"ac\"  \"ad\"  \"abc\" \"bac\" \"cab\"\n\n\nTo match these which start with an a, we can use the regular expression ^a:\n\nstrings[str_detect(strings, \"^a\")]\n\n[1] \"a\"   \"ab\"  \"ac\"  \"ad\"  \"abc\"\n\n\nTo find these which start with an a followed by b or c, but not d, we can use the square brackets:\n\nstrings[str_detect(strings, \"^a[bc]\")]\n\n[1] \"ab\"  \"ac\"  \"abc\"\n\n\nOK, that was a lot. Take some time to digest it. Regulare expressions are what you call in German gewöhnungsbedürftig – they require some getting used to, but they are worth it.\n\n\n3.5.3 Usage of regular expressions in R\nMost commonly you will use regular expressions in R for two things:\n\nSearching for something in a character vector (like we did above)\nReplacing something in a character vector, for example using str_replace_all\n\nWhat we did not tell you when we introduced str_replace_all is that it actually, its second argument is a regular expression. Therefore, we can use it to find a pattern and replace it with something else. For example, we can unify the following messed up vector denoting the gender of patients:\n\ngender &lt;- c(\"m\", \"f\", \"m\", \"w\", \"frau\",\n            \"female\", \"other\", \"male\", \n            \"männlich\", \"x\", \"weiblich\")\n\nGerman and English are mixed here, but we can see a pattern: if the strings starts with m, it is male, if it starts with either f or w, it is female. And in remaining cases it is “other”. We can clean up this mess with just three lines of code:\n\ngender &lt;- str_replace(gender, \"^m.*\", \"male\")\ngender &lt;- str_replace(gender, \"^[fw].*\", \"female\")\ngender &lt;- str_replace(gender, \"^[^mfw].*\", \"other\")\n\nNote that the ^ character has two different meanings on line 3 above. As the first character of a regular expression, it anchors it to the beginning of the string. However, inside the square brackets, it negates the selection. So, [^mfw] means “any character except m, f or w”.\nThe .* idiom is a common one in regular expressions. It means “zero or more of any character”. So, ^m.* matches both the string “m” and the string “male”. And because we used ^, it will only match the strings where m is the first letter (so it does not match “female”).\n\n\n\n\n\n\n\nExercise 3.11 (Quick question) What would happen if we ommited the ^ at the beginning of the strings above? For example, if we used [^mfw].* instead of ^[^mfw].*? Think first, then try it out.\n\n\n\n\nBut wait, if some characters have a special meaning, how can we replace them? For example, how can we replace a dot in a string? The following will not work as intended:\n\nvec &lt;- c(\"5.6\", \"5.7\", \"5.8\")\nstr_replace_all(vec, \".\", \",\")\n\n[1] \",,,\" \",,,\" \",,,\"\n\n\nSince the character . means “any character”, every character will be replaced by a comma in the example above. In order to search or replace  special characters, we must escape them – which in R means putting two backslashes6 in front of them in the regular expression.escaping characters in a regexp\n\nvec &lt;- c(\"5.6\", \"5.7\", \"5.8\")\nstr_replace_all(vec, \"\\\\.\", \",\")\n\n[1] \"5,6\" \"5,7\" \"5,8\"\n\n\n\n\n\n\n\n\n\nExercise 3.12  \n\nUse str_replace_all() to make the following uniform: c(\"male\", \"Male \", \"M\", \"F\", \"female\", \" Female\")\nUsing str_replace_all() and toupper(), clean up the gene names such that they conform to the HGNC (all capital letters, no spaces, no dashes): c(\"ankrd22\", \"ifng\", \"Nf-kb\", \" Cxcl 5\", \"CCL 6.\", \"ANK.r.d. 12\")\nWhat regular expression matches all of the ankyrin repeat genes (but not other genes) in the following vector: c(\"ANKRD22\", \"ank.rep.d. 12\", \"ANKRD-33\", \"ankrd23\", \"ANKEN\", \"MAPK\", \"ifng-1\", \"ANKA-REP-6\")? Ankyrin repeat domain genes are the first 4 in the vector.\n\n\n\n\n\n\n\n3.5.4 Correcting columns in iris_data with regular expressions\nLet us now turn to another column in the data frame, the petal lengths. Using the approach we have just learned, we can find the problematic values:\n\npetal_length &lt;- iris_data$petal_length\nproblems &lt;- is.na(as.numeric(petal_length))\n\nWarning: NAs introduced by coercion\n\nwhich(problems)\n\n[1]  12  13  14 115 149\n\npetal_length[problems]\n\n[1] \"1.6!\" \"1 .4\" \"1.1?\" \"5k.1\" \"5. 4\"\n\n\nWe could use for example str_replace_all(sepal_length, \"k\", \"\"), to change 5k.1 to 5.1, but that would not be a general solution. What if it is j in the new data? We should actually remove everything that is not a number and not a decimal dot. To this end, we will use the regular expressions.\nWe need to remove anything but numbers and decimal dots. We can use the square brackets for that:\n\npetal_length &lt;- str_replace_all(petal_length, \"[^0-9.]\", \"\")\n\nThe 0-9 means anything between 0 and 9, and the . is a literal dot. As you can see, you don’t have to escape the dot if it is already in the  square brackets. The ^ inside the square brackets negates the selection, so [^0-9.] means exactly what we wanted.[^0-9]\n\n# check the problems\npetal_length[problems]\n\n[1] \"1.6\" \"1.4\" \"1.1\" \"5.1\" \"5.4\"\n\n# convert to numbers\npetal_length &lt;- as.numeric(petal_length)\n\n# check for remaining NA's\nany(is.na(petal_length))\n\n[1] FALSE\n\n# assign\niris_data$petal_length &lt;- as.numeric(petal_length)\n\nDone! The iris data set is now clean.\n\nscdf(iris_data)\n\n# Color data frame (class colorDF) 5 x 5:\n │Col         │Class│NAs  │unique│Summary                                  \n1│sepal_length│&lt;dbl&gt;│    0│    35│4.3 [5.1 &lt;5.8&gt; 6.4] 7.9                  \n2│sepal_width │&lt;dbl&gt;│    0│    23│2.0 [2.8 &lt;3.0&gt; 3.3] 4.4                  \n3│petal_length│&lt;dbl&gt;│    0│    43│1.00 [1.60 &lt;4.35&gt; 5.10] 6.90             \n4│petal_width │&lt;dbl&gt;│    0│    22│0.1 [0.3 &lt;1.3&gt; 1.8] 2.5                  \n5│species     │&lt;chr&gt;│    0│     3│setosa: 50, versicolor: 50, virginica: 50\n\n\n\n\n\n\n\n\n\nExercise 3.13 (Correcting metadata) In Exercise 3.7, you have diagnosed the file meta_data_botched.xlsx. Now go ahead and correct the problems you have found.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reading and Writing Files</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html",
    "href": "day4-manipulating-your-data.html",
    "title": "4  Manipulating data frames",
    "section": "",
    "text": "4.1 Aims for today",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#aims-for-today",
    "href": "day4-manipulating-your-data.html#aims-for-today",
    "title": "4  Manipulating data frames",
    "section": "",
    "text": "Searching, sorting and selecting\nMatching and merging data\nPipes - writing readable code\nWide and long format",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#selecting-columns-in-data-frames",
    "href": "day4-manipulating-your-data.html#selecting-columns-in-data-frames",
    "title": "4  Day 4: Manipulating data frames",
    "section": "4.2 Selecting columns in data frames",
    "text": "4.2 Selecting columns in data frames",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day 4: Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#pipes-in-r",
    "href": "day4-manipulating-your-data.html#pipes-in-r",
    "title": "4  Manipulating data frames",
    "section": "4.6 Pipes in R",
    "text": "4.6 Pipes in R\n\n4.6.1 Too many parentheses\nRemember how functions work? Each function takes some arguments and returns a value. And we can use that value to plug in back into another function. This allows us to write terse code, for example:\n\nvec &lt;- c(1, NA, 3, 10)\nwhich(is.na(as.numeric(vec)))\n\n[1] 2\n\n\nThis is quite easy to understand. Unfortunately, this can quickly become unreadable. Let us consider cleaning up the data from yesterday. We can do it one by one, like this:\n\nlibrary(tidyverse)\nlibrary(janitor)\niris &lt;- read_csv(\"Datasets/iris.csv\")\niris &lt;- clean_names(iris)\niris$species &lt;- tolower(iris$species)\niris$petal_length &lt;- str_replace_all(iris$petal_length ,\"[^0-9.]\", \"\") \niris$petal_length &lt;- as.numeric(iris$petal_length)\niris$sepal_length &lt;- str_replace_all(iris$sepal_length, \",\", \".\")\niris$sepal_length &lt;- str_replace_all(iris$sepal_length ,\"&gt; \", \"\") \niris$sepal_length &lt;- as.numeric(iris$sepal_length)\n\nThat is a lot of code. However, we could do it all on one line:\n\niris &lt;- \n  mutate(mutate(mutate(clean_names(read_csv(\"Datasets/iris.csv\")),\n  petal_length = as.numeric(\n  str_replace_all(petal_length, \"[^0-9.]\", \"\"))),\n  sepal_length = as.numeric(str_replace_all(\n  str_replace_all(sepal_length, \",\", \".\"), \"&gt; \", \"\"))),\n  species = tolower(species))\n\nIf you spend some time deciphering this, you will see that it does precisely the same operations as the previous beat of code. However, it is unreadable and unmaintainable3.\n3 I got a headache just trying to figure out the correct number of parentheses in this code.\n\n4.6.2 Pipes\n Fortunately, there is a dirty trick that results in clean and readable code: the pipe operator |&gt; (pronounced “pipe”; in older code you might see “%&gt;%” instead4).Pipe operator |&gt;4 The idea of the pipe operator was popularized by the magrittr package, which is a part of Tidyverse, as %&gt;%. R users found it so cool that with R version 4.1.0, the pipe operator |&gt; was included in the base R. There are some differences between |&gt; and %&gt;%, but for our purposes, they are equivalent.\nPipe operator allows for writing very clean, very convenient and very readable code. The basic idea behind the pipe operator is as follows: a |&gt; f(b) is the same as f(a, b). We can say it pipes the value of a to the function f. And if a is already the result of a function, say g(a), then g(a) |&gt; f(b) is the same as f(g(a), b). We build a pipe from the output of g() to the input of f().\nSo, for example, the following two lines are equivalent:\n\nvec &lt;- c(1, NA, 3, 10)\nas.numeric(vec)\n\n[1]  1 NA  3 10\n\nvec |&gt; as.numeric()\n\n[1]  1 NA  3 10\n\n\nPipe takes whatever is on its left side and inserts it in the parentheses of the function on its right side. Now think for a moment what it does to constructs like f1(f2(f3(f4(f5(x))))). Now compare this:\n\nwhich(is.na(as.numeric(vec)))\n\n[1] 2\n\nvec |&gt; as.numeric() |&gt; is.na() |&gt; which()\n\n[1] 2\n\n\nThe two lines are equivalent. However, the latter is much more readable: it shows how the value goes into as.numeric(), then the output of as.numeric() goes into is.na(), and the resulting logical vector goes into which().\nSee how that works for our iris dataset:\n\niris &lt;- read_csv(\"Datasets/iris.csv\") |&gt;\n        clean_names() |&gt;\n        mutate(species = tolower(species)) |&gt;\n        mutate(petal_length = str_replace_all(petal_length ,\"[^0-9.]\", \"\")) |&gt;\n        mutate(petal_length = as.numeric(petal_length)) |&gt;\n        mutate(sepal_length = str_replace_all(sepal_length, \",\", \".\")) |&gt;\n        mutate(sepal_length = str_replace_all(sepal_length ,\"&gt; \", \"\")) |&gt;\n        mutate(sepal_length = as.numeric(sepal_length))\n\nIt is just as clear and readable as the first example, but with some additional benefits.\nFirst, think what happens if you want to change the name of the variable, say, from iris to flowers. In the first example, you would have to change every single occurence of iris to flowers. Sure, it would not be all too bad if you can do it automatically with a search-and-replace, but what if the variable name was a?\nSecond, the pipe facilitates development. You build a pipe line by line, and each time you execute the code, all operations are repeated. This is a good thing – you avoid the situation where you forget to execute one line and the code does not work.\nThere is an alternative to the above code which uses pipes, but not all on the same line:\n\niris &lt;- read_csv(\"Datasets/iris.csv\") |&gt;\n        clean_names() |&gt;\n        mutate(species = tolower(species))\n\niris$petal_length &lt;- iris$petal_length |&gt; \n        str_replace_all(\"[^0-9.]\", \"\") |&gt;\n        as.numeric()\n\niris$sepal_length &lt;- iris$sepal_length |&gt;\n        str_replace_all(\",\", \".\") |&gt;\n        str_replace_all(\"&gt; \", \"\") |&gt;\n        as.numeric()\n\nWhether you prefer this one or the previous is a matter of both, taste and context. The second is more appropriate if for each column you have to do a lot of manipulations.\nOne limitation is that in the form that is used here5, the value passed on from function to function is always the first argument. This is why we are doing the substitutions with the Tidyverse function str_replace_all() (which takes the character vector as the first argument) rather than the base R function gsub() (which takes the pattern as the first argument).\n5 This limitation can be circumvented either by using only named arguments, or by using a placeholder, _. See here for an explanation.\n\n\n\n\n\n\nExercise 4.10 (Botched meta data) Go back to the code that you have used yesterday for cleaning up the dataset from file meta_data_botched.xlsx. Use pipes to make it more readable.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#footnotes",
    "href": "day4-manipulating-your-data.html#footnotes",
    "title": "4  Manipulating data frames",
    "section": "",
    "text": "Unfortunately, there are many functions in R that are named rename(). Someone should really rename them (badum-tss).↩︎\nStrictly speaking, the recycling rules apply. See Day 1. This can be sometimes useful, but usually it is better to be more explicit.↩︎\nI got a headache just trying to figure out the correct number of parentheses in this code.↩︎\nThe idea of the pipe operator was popularized by the magrittr package, which is a part of Tidyverse, as %&gt;%. R users found it so cool that with R version 4.1.0, the pipe operator |&gt; was included in the base R. There are some differences between |&gt; and %&gt;%, but for our purposes, they are equivalent.↩︎\nThis limitation can be circumvented either by using only named arguments, or by using a placeholder, _. See here for an explanation.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#selecting-and-filtering",
    "href": "day4-manipulating-your-data.html#selecting-and-filtering",
    "title": "4  Day 4: Manipulating data frames",
    "section": "4.2 Selecting and filtering",
    "text": "4.2 Selecting and filtering\n\n4.2.1 Selecting columns in data frames\nAs you know, if we want the actual column as vector, we use the $ operator:\n\ndf &lt;- data.frame(ara=1:5, bera=6:10, cora=11:15, dora=16:20)\ndf$ara # same as df[[\"ara\"]]\n\n[1] 1 2 3 4 5\n\n\nJust a reminder: you don’t want to try to select a single column using square brackets (df[, 1]), because the behavior is different for data frames and tibbles. Better use df$a or df[[\"a\"]].\nWe will now discuss selecting and searching for more than one column. Quite often, you want to select only some columns from a data set, or maybe you want to change their order. As you learned on Day 2, you can select columns from a data frame using vectors. This is very similar to how you select elements from a matrix, or even a vector. You can use integers, negative integers, or column names.\n\n# select columns 1 to 2\ndf2 &lt;- df[ , 1:2]\n\n# select anything but column 2\ndf2 &lt;- df[ , -2]\n\n# select all columns in reverse order\ndf2 &lt;- df[ , ncol(df):1]\n\n# select columns ara and cora\ndf2 &lt;- df[ , c(\"ara\", \"cora\")]\n\n# select all columns ara and cora, but in reverse order\ndf2 &lt;- df[ , c(\"cora\", \"ara\")]\n\nThis is very similar to what we did when dealing with matrices, and actually similar to how we select elements from a vector. Note, however, that rather than using 4:1 in the line 8 above, we use ncol(df):1. This ensures that if data frame grows for some reason, we still get all the columns.\n\n\n4.2.2 Selecting columns using tidyverse\nTidyverse has the select() function, which is a bit more explicit and readable. Most importantly, it also has extra features that make it easier to work with.\n\nlibrary(tidyverse)\n# select columns ara and cora\ndf2 &lt;- select(df, ara, cora)\n\n# select columns ara to cora\ndf2 &lt;- select(df, ara:cora)\n\n# select anything but column bera\ndf2 &lt;- select(df, -bera)\n\nCan you spot what is weird about the code above? Exactly! There are no quotes around the column names. One would expect that R should throw an error – after all, there is no variable “ara” defined yet. However, this is an extra feature of tidyverse. It can be confusing at first, but you will soon get to like it.\nWhat’s more, you see the constructs like ara:cora or -bera. In the base R, ara:cora means “look up variables ara and cora and return all integers between these two values”. In Tidyverse, that means “select all columns from ara to cora”. Similarly, -bera means “select all columns except bera”. This is called tidy evaluation and it works only with some tidyverse functions.\n\n\n\n\n\n\nRemember!\n\n\n\nTidy evaluation only works with tidyverse functions!”\n\n\nAnother nice feature of select() is that you can use the a few specialized helper functions, saving you tedious regular expression and column searches. For example, how would you select for columns that end with a particular suffix? This is a common enough task in data science – many clinical data sets have hundreds of columns and a systematic way of naming them, thus allowing to search for example all columns related to laboratory diagnostics. This can easily be done with the ends_with() function:\n\ndf2 &lt;- select(df, ends_with(\"ora\"))\ncolnames(df2)\n\n[1] \"cora\" \"dora\"\n\n\nThere are many more such functions, like starts_with(), contains(), matches(), one_of(), everything() and even last_col() (for selecting the last column). You can find them all in the help page for select().\n\n\n\n\n\n\nTo quote or not to quote?\n\n\n\nThe fact that you don’t use quotes around column names in tidyverse is confusing for many. As a rule of thumb, at this stage you should use quotes around column names, unless:\n\nyou are using $ operator (e.g. df$ara)\nyou are using select(), mutate() or filter() from tidyverse\nyou are using another tidyverse function that uses tidy evaluation (look up the help page if unsure)\n\nOnce you start to program your own functions, the tidy evaluation will be an additional source of confusion, but burn that bridge when you get to it.\n\n\n\n\n4.2.3 Renaming columns\nThe select function has one more useful feature: you can directly rename the variables in the same call.\n\ndf2 &lt;- select(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"Gamma\"\n\n\nHowever, there is another function which can be used for renaming specific columns, aptly named rename()1. The way its syntax works it is well suitable for working with pipes (see below), and is a great deal easier than renaming the columns using colnames().\n\ndf2 &lt;- rename(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"bera\"  \"Gamma\" \"dora\" \n\n\nAs you can see, no selection was made here, only renaming. Again, no quotes are needed around the column names.\n\n\n\n\n\n\n\nExercise 4.1  \n\nRead the file ‘Datasets/transcriptomics_results.csv’\nWhat columns are in the file?\nSelect only the columns ‘GeneName’, ‘Description’, ‘logFC.F.D1’ and ‘qval.F.D1’\nRename the columns to ‘Gene’, ‘Description’, ‘LFC’ and ‘FDR’",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day 4: Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#selecting-columns-using-tidyverse",
    "href": "day4-manipulating-your-data.html#selecting-columns-using-tidyverse",
    "title": "4  Day 4: Manipulating data frames",
    "section": "4.3 Selecting columns using tidyverse",
    "text": "4.3 Selecting columns using tidyverse\nTidyverse has the select() function, which is a bit more explicit and readable. Most importantly, it also has extra features that make it easier to work with.\n\nlibrary(tidyverse)\n# select columns ara and cora\ndf2 &lt;- select(df, ara, cora)\n\n# select columns ara to cora\ndf2 &lt;- select(df, ara:cora)\n\n# select anything but column bera\ndf2 &lt;- select(df, -bera)\n\nCan you spot what is weird about the code above? Exactly! There are no quotes around the column names. One would expect that R should throw an error – after all, there is no variable “ara” defined yet. However, this is an extra feature of tidyverse. It can be confusing at first, but you will soon get to like it.\nWhat’s more, you see the constructs like ara:cora or -bera. In the base R, ara:cora means “look up variables ara and cora and return all integers between these two values”. In Tidyverse, that means “select all columns from ara to cora”. Similarly, -bera means “select all columns except bera”. This is called tidy evaluation and it works only with some tidyverse functions.\n\n\n\n\n\n\nRemember!\n\n\n\nTidy evaluation only works with tidyverse functions!”\n\n\nThe select function has one more useful feature: you can directly rename the variables in the same call.\n\ndf2 &lt;- select(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"Gamma\"\n\n\n\n\n\n\n\n\nTo quote or not to quote?\n\n\n\nThe fact that you don’t use quotes around column names in tidyverse is confusing for many. As a rule of thumb, you should use quotes around column names, unless:\n\nyou are using $ operator (e.g. df$ara)\nyou are using select(), mutate() or filter() from tidyverse\nyou are using another tidyverse function that uses tidy evaluation (look up the help page if unsure)\n\nOnce you start to program your own functions, the tidy evaluation will be an additional source of confusion, but burn that bridge when you get to it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day 4: Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#tidyverse-and-quotes",
    "href": "day4-manipulating-your-data.html#tidyverse-and-quotes",
    "title": "4  Day 4: Manipulating data frames",
    "section": "4.4 Tidyverse and quotes",
    "text": "4.4 Tidyverse and quotes\n\nselect(df, ara, cora)\n\n  ara cora\n1   1   11\n2   2   12\n3   3   13\n4   4   14\n5   5   15\n\n\nNote the lack of quotes around a and c! This is a feature in tidyverse which has two effects:\n\nit is easier to type (you save the typing of df$\"\"! imagine how much time you have now)\nit is confusing for beginners (“why are there no quotes?”, “when should I use quotes and when not?”, “how does it know that it is df$a and not some other a?”)\nmakes programming confusing (what if “a” holds the name of the column that you would like to sort by? - use .data[[a]]; Or is some other vector by which you wish to sort?)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day 4: Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#exercise-4.1",
    "href": "day4-manipulating-your-data.html#exercise-4.1",
    "title": "4  Day 4: Manipulating data frames",
    "section": "4.4 Exercise 4.1",
    "text": "4.4 Exercise 4.1\n\nRead the file ‘Datasets/transcriptomics_results.csv’\nWhat columns are in the file?\nSelect only the columns ‘GeneName’, ‘Description’, ‘logFC.F.D1’ and ‘qval.F.D1’\nRename the columns to ‘Gene’, ‘Description’, ‘LFC’ and ‘FDR’",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day 4: Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#review",
    "href": "day4-manipulating-your-data.html#review",
    "title": "4  Manipulating data frames",
    "section": "4.7 Review",
    "text": "4.7 Review\nThings that you learned today:\n\nSelecting columns in data frames:\n\nusing square brackets\nusing select() from tidyverse\ntidy evaluation\nrenaming columns with rename()\n\nSorting, ordering, filtering\n\nsort() to sort a vector\norder() to get the order of a vector\norder() to sort a data frame\narrange() from tidyverse to sort a data frame\nfiltering with logical vectors\nfiltering with filter() from tidyverse\nhandling duplicates with duplicated()\ncombining logical vectors with &\nusing %in% operator\n\nMerging data frames\n\nrbind() and cbind()\nmerge() inner, left, right and full joins\nmerging by more than one column\n\nPipes in R\n\nusing |&gt; operator\nbuilding pipelines\n\nOther\n\nusing rnorm() to generate random numbers\nusing round() to round numbers\nsample() for generating random samples",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#sorting-and-ordering",
    "href": "day4-manipulating-your-data.html#sorting-and-ordering",
    "title": "4  Manipulating data frames",
    "section": "4.3 Sorting and ordering",
    "text": "4.3 Sorting and ordering\n\n4.3.1 Sorting and ordering vectors\nBefore we jump to sorting, filtering and ordering data frames, we should first discuss these operations on vectors. This is important, because although data frames can easily be sorted with functions such as arrange() from tidyverse, you will have situations in which you need to sort a vector, a list or another data type. Secondly, understanding the concepts of ordering will aid you also with your work with data frames.\n The basic R function for sorting is, appropriately, sort(). It is as simple as it gets:sort()\n\nvec &lt;- round(rnorm(10), 2)\nvec\n\n [1] -0.70 -0.33 -0.69 -0.75 -0.26  1.20 -0.39 -0.16 -0.26  1.23\n\nsort(vec)\n\n [1] -0.75 -0.70 -0.69 -0.39 -0.33 -0.26 -0.26 -0.16  1.20  1.23\n\nsort(vec, decreasing=TRUE)\n\n [1]  1.23  1.20 -0.16 -0.26 -0.26 -0.33 -0.39 -0.69 -0.70 -0.75\n\n\nThe rnorm() function generates a vector of random numbers from a normal distribution (you met it on Day 2). The round() function with parameter 2 rounds the numbers to two decimal places. The sort() with the decreasing=TRUE argument sorts the vector in descending order.\n Amazingly, it is hardly ever used. Mostly we need the order() function. Rather than returning the sorted vector, it returns the indices of the sorted vector:order()\n\nord &lt;- order(vec)\nord\n\n [1]  4  1  3  7  2  5  9  8  6 10\n\n\nSo, what does that mean? Take the first element of ord. It is 4. This means that the first value to appear in the sorted vector should be the element number 4 of the original vector, which is -0.75 and the smallest number in vec (check it!). Likewise, the second element of ord is 1, which points to -0.7, the second smallest element of vec.\nSo what happens if we use the vector ord to select element from the vector vec? We get the sorted vector, that’s what:\n\nvec[ord]\n\n [1] -0.75 -0.70 -0.69 -0.39 -0.33 -0.26 -0.26 -0.16  1.20  1.23\n\n\n\n\n\n\n\n\n\nExercise 4.2 (Reverse sorting) How can you get the reverse order vector for vec?\n\n\n\n\n\n\n4.3.2 Sorting character vectors\n We can also sort character vectors, but the behavior might not be exactly what you expect. Take a look:Sorting character vectors\n\nchvec &lt;- c(\"b\", \"a\", \"Zorro\", \"10\", \"Anton\", \"A\", \"2\", \"100\", \"zxy\")\nsort(chvec)\n\n[1] \"10\"    \"100\"   \"2\"     \"a\"     \"A\"     \"Anton\" \"b\"     \"Zorro\" \"zxy\"  \n\n\nOK, so what happened here? The sort() function sorts the vector in lexicographical order – just like you sort words in a dictionary (or, in the olden days, in a phone book). For the most part that seems quite intuitive, however take a look at the numbers: 100 comes before 2.\nLexicographical order means that the words are sorted first by first letter, then by second etc. So a word starting with a 1 always comes before a word starting with a 2, even if it means a number that is larger than 2.\nThis has one important implication for all of us who work with data. Identifiers, especially sample and patient identifiers are quite often a combination of letters and numbers, e.g. ID1 or Patient10. This can cause problems when sorting, as ID10 will come before ID2:\n\nchvec &lt;- c(\"ID1\", \"ID2\", \"ID3\", \"ID4\", \"ID10\", \"ID20\", \"ID100\")\nsort(chvec)\n\n[1] \"ID1\"   \"ID10\"  \"ID100\" \"ID2\"   \"ID20\"  \"ID3\"   \"ID4\"  \n\n\nThere are several solutions for that, but here is one with the tools that you already know. First, we will extract the numbers from the strings, then we will sort the vector based on these numbers:\n\n# extract numbers\nchvec_n &lt;- str_replace_all(chvec, \"ID\", \"\")\n\n# convert to numeric\nchvec_n &lt;- as.numeric(chvec_n)\n\n# get the order\nord &lt;- order(chvec_n)\n\n# sort the original vector\nchvec[ord]\n\n[1] \"ID1\"   \"ID2\"   \"ID3\"   \"ID4\"   \"ID10\"  \"ID20\"  \"ID100\"\n\n\n\n\n\n\n\n\n\nExercise 4.3 (Sorting by last name) Here is a vector of names. Can you think of how to sort it by last name?\npersons &lt;- c(\"Henry Fonda\", \"Bob Marley\", \"Robert F. Kennedy\", \n  \"Bob Dylan\", \"Alan Rickman\")\n\n\n\n\n\n\n4.3.3 Sorting data frames with order()\nBy now it should be clear that you can use this method to order data from data frames (or matrices, or other data types). We will show it on example of the transcriptomics_results.csv file that you have read in the Exercise 4.1. I assume that you have read the file and selected the columns with something like this:\n\nlibrary(tidyverse)\ntr_res &lt;- read_csv(\"Datasets/transcriptomics_results.csv\")\ntr_res &lt;- select(tr_res, \n                 Gene=GeneName, Description,\n                 logFC=logFC.F.D1, FDR=qval.F.D1)\n\nThe transcriptomics data set comes from a vaccine study (Weiner et al. 2019) and show the changes in gene expression after vaccination with a particular adjuvanted fluad vaccine. logFC stands for log fold change, and FDR is the false discovery rate (resulting from raw p-values being corrected using the Benjamini-Hochberg procedure). We will sort the data frame by decreasing logFC:\n\nord &lt;- order(tr_res$logFC, decreasing=TRUE)\nhead(tr_res[ord, ])\n\n# A tibble: 6 × 4\n  Gene     Description                                            logFC      FDR\n  &lt;chr&gt;    &lt;chr&gt;                                                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Q5D1D6   tc|Q5D1D6_CERAE (Q5D1D6) Guanylate binding protein 1,…  4.85 1.80e-14\n2 ANKRD22  ref|Homo sapiens ankyrin repeat domain 22 (ANKRD22), …  4.81 5.55e-15\n3 ETV7     ref|Homo sapiens ets variant 7 (ETV7), transcript var…  4.65 2.53e-15\n4 SERPING1 ref|Homo sapiens serpin peptidase inhibitor, clade G …  4.52 5.95e-16\n5 CXCL10   ref|Homo sapiens chemokine (C-X-C motif) ligand 10 (C…  4.38 1.51e-10\n6 GBP1P1   ref|Homo sapiens guanylate binding protein 1, interfe…  4.11 2.33e-17\n\n\n\n\n\n\n\n\n\nExercise 4.4 (Sorting data frames with order())  \n\nSort the data frame tr_res by increasing FDR\nSort the data frame alphabetically by Gene\n\n\n\n\n\n\n\n4.3.4 Sorting data frames with tidyverse\n Tidyverse makes it simple to sort the data frames with the arrange() function:arrange()\n\ntr_res &lt;- arrange(tr_res, desc(logFC))\nhead(tr_res)\n\n# A tibble: 6 × 4\n  Gene     Description                                            logFC      FDR\n  &lt;chr&gt;    &lt;chr&gt;                                                  &lt;dbl&gt;    &lt;dbl&gt;\n1 Q5D1D6   tc|Q5D1D6_CERAE (Q5D1D6) Guanylate binding protein 1,…  4.85 1.80e-14\n2 ANKRD22  ref|Homo sapiens ankyrin repeat domain 22 (ANKRD22), …  4.81 5.55e-15\n3 ETV7     ref|Homo sapiens ets variant 7 (ETV7), transcript var…  4.65 2.53e-15\n4 SERPING1 ref|Homo sapiens serpin peptidase inhibitor, clade G …  4.52 5.95e-16\n5 CXCL10   ref|Homo sapiens chemokine (C-X-C motif) ligand 10 (C…  4.38 1.51e-10\n6 GBP1P1   ref|Homo sapiens guanylate binding protein 1, interfe…  4.11 2.33e-17\n\n\n Note the use of desc() function – rather than specifying an argument like decreasing=TRUE.desc()\nUsing arrange(), it is easy to sort by multiple columns:\n\ntr_res &lt;- arrange(tr_res, logFC, FDR)\nhead(tr_res)\n\n# A tibble: 6 × 4\n  Gene            Description                                      logFC     FDR\n  &lt;chr&gt;           &lt;chr&gt;                                            &lt;dbl&gt;   &lt;dbl&gt;\n1 DSC1            ref|Homo sapiens desmocollin 1 (DSC1), transcri… -1.97 3.33e-4\n2 XLOC_010084     tc|Q6FUE3_CANGA (Q6FUE3) Similarity, partial (1… -1.76 7.59e-3\n3 LRRC69          ref|Homo sapiens leucine rich repeat containing… -1.68 5.06e-2\n4 ZMAT4           ref|Homo sapiens zinc finger, matrin-type 4 (ZM… -1.67 4.61e-2\n5 PCSK6           ref|Homo sapiens proprotein convertase subtilis… -1.63 3.13e-2\n6 ENST00000456460 Unknown                                          -1.59 2.53e-2\n\n\nIt is also allowed to use the expressions like logFC * FDR to sort by the sum of the two columns (that doesn’t make much sense in this context, but you get the idea):\n\ntr_res &lt;- arrange(tr_res, logFC * FDR)\n\nabs()\n\n\n\n\n\n\n\nExercise 4.5 (Sorting data frames with arrange()) The abs() function returns the absolute value of a number, for example abs(c(-3, 7)) returns 3, 7. Use the arrange() function to sort the data frame tr_res by the decreasing absolute value of logFC.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann.\n2021. “Gene Name Errors: Lessons Not Learned.” PLoS\nComputational Biology 17 (7): e1008984. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984.\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022.\n“Venn Diagrams May Indicate Erroneous Statistical Reasoning in\nTranscriptomics.” Frontiers in Genetics 13: 818683. https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.818683/full.\n\n\nWeiner, January, David JM Lewis, Jeroen Maertzdorf, Hans-Joachim\nMollenkopf, Caroline Bodinham, Kat Pizzoferro, Catherine Linley, et al.\n2019. “Characterization of Potential Biomarkers of Reactogenicity\nof Licensed Antiviral Vaccines: Randomized Controlled Clinical Trials\nConducted by the BIOVACSAFE Consortium.” Scientific\nReports 9 (1): 20362. https://www.nature.com/articles/s41598-019-56994-8.",
    "crumbs": [
      "Introduction",
      "References"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#selecting-columns",
    "href": "day4-manipulating-your-data.html#selecting-columns",
    "title": "4  Manipulating data frames",
    "section": "4.2 Selecting columns",
    "text": "4.2 Selecting columns\n\n4.2.1 Selecting columns using square brackets\nAs you know, if we want the actual column as vector, we use the $ operator:\n\ndf &lt;- data.frame(ara=1:5, bera=6:10, cora=11:15, dora=16:20)\ndf$ara # same as df[[\"ara\"]]\n\n[1] 1 2 3 4 5\n\n\nJust a reminder: you don’t want to try to select a single column using square brackets (df[, 1]), because the behavior is different for data frames and tibbles. Better use df$a or df[[\"a\"]].\nWe will now discuss selecting and searching for more than one column. Quite often, you want to select only some columns from a data set, or maybe you want to change their order. As you learned on Day 2, you can select columns from a data frame using vectors. This is very similar to how you select elements from a matrix, or even a vector. You can use integers, negative integers, or column names.\n\n# select columns 1 to 2\ndf2 &lt;- df[ , 1:2]\n\n# select anything but column 2\ndf2 &lt;- df[ , -2]\n\n# select all columns in reverse order\ndf2 &lt;- df[ , ncol(df):1]\n\n# select columns ara and cora\ndf2 &lt;- df[ , c(\"ara\", \"cora\")]\n\n# select all columns ara and cora, but in reverse order\ndf2 &lt;- df[ , c(\"cora\", \"ara\")]\n\nThis is very similar to what we did when dealing with matrices, and actually similar to how we select elements from a vector. Note, however, that rather than using 4:1 in the line 8 above, we use ncol(df):1. This ensures that if data frame grows for some reason, we still get all the columns.\n\n\n4.2.2 Selecting columns using tidyverse\n Tidyverse has the select() function, which is a bit more explicit and readable. Most importantly, it also has extra features that make it easier to work with.select()\n\nlibrary(tidyverse)\n# select columns ara and cora\ndf2 &lt;- select(df, ara, cora)\n\n# select columns ara to cora\ndf2 &lt;- select(df, ara:cora)\n\n# select anything but column bera\ndf2 &lt;- select(df, -bera)\n\nCan you spot what is weird about the code above? Exactly! There are no quotes around the column names. One would expect that R should throw an error – after all, there is no variable “ara” defined yet. However, this is an extra feature of tidyverse. It can be confusing at first, but you will soon get to like it.\n What’s more, you see the constructs like ara:cora or -bera. In the base R, ara:cora means “look up variables ara and cora and return all integers between these two values”. In Tidyverse, that means “select all columns from ara to cora”. Similarly, -bera means “select all columns except bera”. This is called tidy evaluation and it works only with some tidyverse functions.Tidy evaluation\n\n\n\n\n\n\nRemember!\n\n\n\nTidy evaluation only works with tidyverse functions!\n\n\n Another nice feature of select() is that you can use the a few specialized helper functions, saving you tedious regular expression and column searches. For example, how would you select for columns that end with a particular suffix? This is a common enough task in data science – many clinical data sets have hundreds of columns and a systematic way of naming them, thus allowing to search for example all columns related to laboratory diagnostics. This can easily be done with the ends_with() function:ends_with()\n\ndf2 &lt;- select(df, ends_with(\"ora\"))\ncolnames(df2)\n\n[1] \"cora\" \"dora\"\n\n\n There are many more such functions, like starts_with(), contains(), matches(), one_of(), everything() and even last_col() (for selecting the last column). You can find them all in the help page for select().Other tidy selection functions\n\n\n\n\n\n\nTo quote or not to quote?\n\n\n\nThe fact that you don’t use quotes around column names in tidyverse is confusing for many. As a rule of thumb, at this stage you should use quotes around column names, unless:\n\nyou are using $ operator (e.g. df$ara)\nyou are using select(), mutate() or filter() from tidyverse\nyou are using another tidyverse function that uses tidy evaluation (look up the help page if unsure)\n\nOnce you start to program your own functions, the tidy evaluation will be an additional source of confusion, but burn that bridge when you get to it.\n\n\n\n\n4.2.3 Renaming columns\nThe select function has one more useful feature: you can directly rename the variables in the same call.\n\ndf2 &lt;- select(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"Gamma\"\n\n\n However, there is another function which can be used for renaming specific columns, aptly named rename()1. The way its syntax works it is well suitable for working with pipes (see below), and is a great deal easier than renaming the columns using colnames().rename()1 Unfortunately, there are many functions in R that are named rename(). Someone should really rename them (badum-tss).\n\ndf2 &lt;- rename(df, Alpha=ara, Gamma=cora)\ncolnames(df2)\n\n[1] \"Alpha\" \"bera\"  \"Gamma\" \"dora\" \n\n\nAs you can see, no selection was made here, only renaming. Again, no quotes are needed around the column names.\n\n\n\n\n\n\n\nExercise 4.1  \n\nRead the file ‘Datasets/transcriptomics_results.csv’\nWhat columns are in the file?\nSelect only the columns ‘GeneName’, ‘Description’, ‘logFC.F.D1’ and ‘qval.F.D1’\nRename the columns to ‘Gene’, ‘Description’, ‘LFC’ and ‘FDR’",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#modifying-and-filtering-data-frames-in-tidyverse",
    "href": "day4-manipulating-your-data.html#modifying-and-filtering-data-frames-in-tidyverse",
    "title": "4  Manipulating data frames",
    "section": "4.4 Modifying and filtering data frames in tidyverse",
    "text": "4.4 Modifying and filtering data frames in tidyverse\n\n4.4.1 Modifying with mutate()\n Yesteday we have been busy cleaning up data sets. To modify the columns of the data frame, we have been using simple assignment. This works, but there is another method in tidyverse. Just like with regular assignments, the mutate() function allows both, creating new columns and modifying existing ones.mutate()\nBelow we create a new column logFC_abs which contains the absolute value of the logFC column:\n\ntr_res &lt;- mutate(tr_res, logFC_abs = abs(logFC))\n\nArguably, this is not much simpler than using simple assignment:\n\ntr_res$logFC_abs &lt;- abs(tr_res$logFC)\n\nThe main difference, as you see, is that mutate() operates on data frames – it takes a data frame as the first argument and returns a data frame. The advantages will be obvious later, when you start to use pipes (see below).\n\n\n4.4.2 Using ifelse() with mutate()\n Say, you would like to create a new column that contains “up” for genes that are upregulated (i.e. logFC &gt;= 0) and “down” for genes that are downregulated (i.e. logFC &lt; 0). We could do it with simple assignment and logical vectors:ifelse()\n\n# default value\ntr_res$direction &lt;- \"up\"\n\n# create logical vector\nnegative &lt;- tr_res$logFC &lt; 0\n\n# change \"up\" to \"down\" where logFC &lt; 0\ntr_res$direction[negative] &lt;- \"down\"\n\nHowever, this is a frequent operation and a much more convenient way exists. It is often used in combination with mutate, although it also can work in a normal assignment:\n\ntr_res &lt;- mutate(tr_res, \n                 direction = ifelse(logFC &lt; 0, \"down\", \"up\"))\n\nThe ifelse() function takes three arguments: a logical vector, a value to return if the logical vector is TRUE, and a value to return if the logical vector is FALSE. In other words, it “converts” a logical vector replacing TRUE with the second argument and FALSE with the third.\n\n\n4.4.3 Filtering with logical vectors and filter()\nAs you have learned, you can filter data frames using logical vectors. This is relatively straightforward. For example, we might want to create a table with only genes that are significant, that is have an FDR &lt; 0.05:\n\nsmall_fdr &lt;- tr_res$FDR &lt; 0.05\nsignificant &lt;- tr_res[small_fdr, ]\n\nIf you use nrow(significant), you will find that there are 1478 significant genes in the data set.\n The same can be achieved using the filter() function from tidyverse:filter()\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05)\n\nSimlarly, we can use the str_detect() function (which you learned about yesterday) to select only genes that contain the word “interferon” in the description:\n\ninterferon &lt;- filter(tr_res, \n                     str_detect(Description, \"interferon\"))\n\nThere are 80 such genes.\nBut what if we want to set two conditions? In statistics it is common to set a threshold not only for the p-value or FDR, but also for an effect size. In our case, the measure of the effect size is the log2 fold change. Thus, we might want to select only genes that are at leas 2-fold upregulated or down-regulated. Twofold upregulation corresponds to log2-fold change greater than 1, and 2-fold downregulation corresponds to log2-fold change less than -1.\nOne way is to use filter() with two parameters:\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05, abs(logFC) &gt; 1)\n\n However, there is a more elegant and versatile way. We can achieve the same effect by combining two logical vectors with the & operator:& (logical operator)\n\nlarge_fc &lt;- abs(tr_res$logFC) &gt; 1\nsignificant &lt;- tr_res[small_fdr & large_fc, ]\n\nOr, in tidyverse,\n\nsignificant &lt;- filter(tr_res, FDR &lt; 0.05 & abs(logFC) &gt; 1)\n\nThe & operator is the logical AND operator. It combines two logical vectors, returning (at the given position) TRUE only if both vectors are TRUE. Take a look:\n\nvec1 &lt;- c(TRUE, TRUE, FALSE, FALSE)\nvec2 &lt;- c(TRUE, FALSE, TRUE, FALSE)\nvec1 & vec2\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nAt the first position both vectors are TRUE, so the result is TRUE. At position 2, the first vector is TRUE and the second is FALSE, so the result is FALSE. And so on. Try it.\n\n\n\n\n\n\n\nExercise 4.6 (Logical vectors)  \n\nCreate a logical vector that selects genes that are both significant (FDR &lt; 0.05) and contain the string “interferon” in the Description. How many are there? How many genes are significant and do not contain “interferon” in the Description column?\nHow many of the genes containing “interferon” in the Description are are guanylate binding proteins? You can recognize the GBPs by their Gene symbol – it starts with “GBP”.\n\n\n\n\n\n\n\n4.4.4 Using the %in% operator\n Another useful utility in searching data frames is the %in% operator. It compares two vectors; for each element of the first vector, it returns TRUE if that element is in the second vector, and FALSE otherwise.%in%\n\nvec1 &lt;- c(\"a\", \"b\", \"c\")\nvec2 &lt;- c(\"a\", \"b\", \"x\", \"y\", \"z\")\nvec1 %in% vec2\n\n[1]  TRUE  TRUE FALSE\n\n\nThe elements 1, of the result are TRUE, because both a and b are in vec2. The last element, however, is FALSE, because c is not in vec2.\nWith %in% we can for example select a specified subset of genes that we are interested in.\n\ngenes &lt;- c(\"GBP1\", \"GBP2\", \"GBP3\", \"GBP4\", \"GBP5\", \"ANKRD22\")\nfilter(tr_res, Gene %in% genes)\n\n# A tibble: 7 × 6\n  Gene    Description                         logFC      FDR logFC_abs direction\n  &lt;chr&gt;   &lt;chr&gt;                               &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 GBP4    ref|Homo sapiens guanylate binding…  2.25 2.59e-21      2.25 up       \n2 GBP5    ref|Homo sapiens guanylate binding…  2.92 1.39e-20      2.92 up       \n3 GBP3    ref|Homo sapiens guanylate binding…  2.63 1.62e-15      2.63 up       \n4 ANKRD22 ref|Homo sapiens ankyrin repeat do…  4.81 5.55e-15      4.81 up       \n5 GBP2    ref|Homo sapiens guanylate binding…  1.8  1.17e-13      1.8  up       \n6 GBP1    ref|Homo sapiens guanylate binding…  2.76 7.38e-12      2.76 up       \n7 GBP3    ref|Homo sapiens guanylate binding…  1.27 3.40e- 3      1.27 up       \n\n\n\n\n\n\n\n\n\nExercise 4.7 Which of these genes are significant? Use the & operator to combine the result from %in% with the result of FDR &lt; 0.05.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#merging-data-sets",
    "href": "day4-manipulating-your-data.html#merging-data-sets",
    "title": "4  Day 4: Manipulating data frames",
    "section": "4.5 Merging data sets",
    "text": "4.5 Merging data sets",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day 4: Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "day4-manipulating-your-data.html#combining-data-sets",
    "href": "day4-manipulating-your-data.html#combining-data-sets",
    "title": "4  Manipulating data frames",
    "section": "4.5 Combining data sets",
    "text": "4.5 Combining data sets\n\n4.5.1 Binding data frames with rbind() and removing duplicates\nThe simplest way to combine two data frames is to bind them. This can be done horizontally (when you have the same number of observations – rows) or vertically, by stacking them on top of each other, when you have the same columns. The two functions are, respectively, cbind() and rbind(), and they work for both, matrices and data frames.\n Let’s start with stacking them on top of each other. For example, we might have generated two data frames with interesting results: one with interferones and the other with guanylate binding proteins, and at some point we decide to have them in one data frame. Stacking them can be done with the rbind() (“row bind”) function:rbind()\n\ninterferon &lt;- filter(tr_res, \n                     str_detect(Description, \"interferon\"))\ngbp &lt;- filter(tr_res,\n              str_detect(Gene, \"GBP\"))\nresults &lt;- rbind(interferon, gbp)\n\nNote that this works only if the data frames have the same columns. If you were to try to bind two data frames with different columns, you would get an error:\n\na &lt;- data.frame(a=1:5, b=6:10)\nb &lt;- data.frame(a=11:15, c=16:20)\nrbind(a, b)\n\nError in match.names(clabs, names(xi)): names do not match previous names\n\n\nOK, but if you have checked the interferon and gbp data frames, you would have noticed that there are some genes that are both interferones and guanylate binding proteins. You did not check, because I haven’t told you yet how to do it. Here is how:\nintersect()\n\nintersect(interferon$Gene, gbp$Gene)\n\n[1] \"GBP1P1\" \"GBP2\"   \"GBP1\"  \n\n\n The intersect() function returns the elements that are common to two vectors. As you can see, there are three: GBP1P1, GBP2 and GBP1. We can get rid of them from the results table thanks to the function duplicated(). For every element of a vector, duplicated() returns TRUE if the element occured previously in the vector. So for the vector c(1, 2, 1) the result will be FALSE, FALSE, TRUE - because the second 1 is duplicated.duplicated()\nTo remove the elements which are duplicated, however, we need to use the negation – ! to get TRUE for every element which is not duplicated. Here it is in action:\n\n# count the number of the duplicated results\nsum(duplicated(results$Gene))\n\n[1] 19\n\n# number of rows before removing dups\nnrow(results)\n\n[1] 96\n\n# remove duplicates\nresults &lt;- filter(results, !duplicated(Gene))\n\n# how many are left?\nnrow(results)\n\n[1] 77\n\n\n\n\n\n\n\n\n\nExercise 4.8 You might have been expecting that there are only three duplicates; after all, only three genes were in common between interferon and gbp- So where do these other duplicates come from? What are they?\n\n\n\n\n\n\n4.5.2 Binding data frames with cbind()\nCombining data frames (or matrices) horizontally is much easier, because you do not have to worry about column names. You just need to make sure that the number of rows is identical2.\n2 Strictly speaking, the recycling rules apply. See Day 1. This can be sometimes useful, but usually it is better to be more explicit.cbind()\n\ndf1 &lt;- data.frame(a = 1:4, b=11:14)\ndf2 &lt;- data.frame(c=21:24)\ncbind(df1, df2)\n\n  a  b  c\n1 1 11 21\n2 2 12 22\n3 3 13 23\n4 4 14 24\n\n\nWe will use cbind() tomorrow when running a principal component analysis (PCA).\n\n\n4.5.3 Merging data sets\nQuite often you will have to merge two data sets. For example, you might have one Excel file containing the covariates for the patients (like BMI, age, gender etc.) and another file containing the gene expression data. Both data frames must have some identifiers present that allow us to match the two data frames.\nThis is not a simple binding operation, because you do not have the guarantee that (a) the identifiers between these two data sets are in the same order or even that (b) both data frames have exactly the same sets of identifiers. Actually, the reverse is more common that not.\nIn order to merge two data frames we need first to identify by what to merge them. In the code below we will concoct two data frames that share some of the identifiers.\n\nids_x &lt;- paste0(\"ID\", sample(1:8, 6))\ndf_x &lt;- data.frame(ID=ids_x, val_x=rnorm(6))\ndf_x\n\n   ID       val_x\n1 ID6 -0.04278474\n2 ID4  0.85095586\n3 ID3  0.26088040\n4 ID8 -0.61587198\n5 ID2 -0.49012494\n6 ID7 -0.96600942\n\nids_y &lt;- paste0(\"ID\", sample(1:8, 6))\ndf_y &lt;- data.frame(ID=ids_y, val_y=rnorm(6, 10, 1))\ndf_y\n\n   ID     val_y\n1 ID5  9.949744\n2 ID4  8.750580\n3 ID8  8.261917\n4 ID3 10.433766\n5 ID1 12.306198\n6 ID7 10.063938\n\nintersect(ids_x, ids_y)\n\n[1] \"ID4\" \"ID3\" \"ID8\" \"ID7\"\n\n\nsample()\nThe sample() function is very useful – it generates a random sample by choosing elements from a vector. This is what is called sampling and comes in handy in many situations. Here we use it to create two sets of IDs that are similar, but not identical. As you can see from the output of intersect(), only 4 elements out of the total 6 are common between the two data frames.\nMerging can be done either with the base R function merge(), or with corresponding *_join functions from the tidyverse. Whether you prefer one or the other is a matter of preference; I like merge() and we will be using it in the examples below.\nInner join, full join\nFirst, however, let us think how we want to merge, or, more specifically, what to do with the identifiers that are present in one data frame, but not in the other? That depends only on what we want to achieve. Possibly we are not interested in any elements that are not present in both data frames. This is what is called in database lingo an inner join Alternatively, we might want to keep all possible elements, whether they are found in df_x or df_y only. This is called an full join.\nmerge()\n\n# inner join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\")\njoined\n\n   ID      val_x     val_y\n1 ID3  0.2608804 10.433766\n2 ID4  0.8509559  8.750580\n3 ID7 -0.9660094 10.063938\n4 ID8 -0.6158720  8.261917\n\n# full join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all=TRUE)\njoined\n\n   ID       val_x     val_y\n1 ID1          NA 12.306198\n2 ID2 -0.49012494        NA\n3 ID3  0.26088040 10.433766\n4 ID4  0.85095586  8.750580\n5 ID5          NA  9.949744\n6 ID6 -0.04278474        NA\n7 ID7 -0.96600942 10.063938\n8 ID8 -0.61587198  8.261917\n\n\nAs you can see, in the first case there are just 4 rows – as many as there are common elements between the ID columns of the two data frames. In the second case, we get all 8 possible IDs. Where the ID was present in one, but not the other data frame, the values were filled up with NA’s. To get the full join we used the parameter all=TRUE, standing for “use all IDs”.\nNote: it is possible to omit the by= parameter specifying by what column to join the data frames. However, do yourself a favor and always specify the column to join on explicitely.\nLeft join, right join\nThere are two more situations, allthough less common in practice. We might want to keep all elements of df_x, but discard any elements of df_y which are not present in df_x. This is called a left join. Or, vice versa, we will discard only these elements which are present in df_x, but not in df_y. This is called, you guessed it, a right join.\n\n# left join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all.x = TRUE)\njoined\n\n   ID       val_x     val_y\n1 ID2 -0.49012494        NA\n2 ID3  0.26088040 10.433766\n3 ID4  0.85095586  8.750580\n4 ID6 -0.04278474        NA\n5 ID7 -0.96600942 10.063938\n6 ID8 -0.61587198  8.261917\n\n# right join by ID\njoined &lt;- merge(df_x, df_y, by=\"ID\", all.y = TRUE)\njoined\n\n   ID      val_x     val_y\n1 ID1         NA 12.306198\n2 ID3  0.2608804 10.433766\n3 ID4  0.8509559  8.750580\n4 ID5         NA  9.949744\n5 ID7 -0.9660094 10.063938\n6 ID8 -0.6158720  8.261917\n\n\nAs you can see, in case of the left join, there are missing values in the right column only, and in case of the right join – only in the left column.\nLet us summarize it in a table:\n\n\n\n\n\n\n\n\n\n\nType of join\nall x\nall y\nmerge options\ntidyverse alternative\n\n\n\n\ninner join\nFALSE\nFALSE\nmerge(x, y)\ninner_join(x, y)\n\n\nleft join\nTRUE\nFALSE\nmerge(x, y, all.x=TRUE)\nleft_join(x, y)\n\n\nright join\nFALSE\nTRUE\nmerge(x, y, all.y=TRUE)\nright_join(x, y)\n\n\nfull join\nTRUE\nTRUE\nmerge(x, y, all=TRUE)\nfull_join(x, y)\n\n\n\nYou don’t have to remember the names (inner, left, right, full) – just remember the principle: sometimes you need the common elements, sometimes you need all elements, and sometimes you need all elements from x or y only.\nDifferent identifier column names. There is one other thing that you should know, because it happens quite often. The two data frames might have identifiers in columns that have different names, for example ID in one and PatientID in the other. You can, of course, rename the columns, but you can also specify the columns to join on explicitly:\nmerge() with different column names\n\ndf_x &lt;- data.frame(ID=ids_x, val_x=rnorm(6))\ndf_y &lt;- data.frame(PatientID=ids_y, val_y=rnorm(6, 10, 1))\n\n# inner join by ID\njoined &lt;- merge(df_x, df_y, by.x=\"ID\", by.y=\"PatientID\")\n\nThe resulting data frame has the identifiers in the column name specified in the “x” data frame, that is – in this case – in the ID column.\n\n\n4.5.4 Complex merges\nThe situation above is quite trivial. In real world, unfortunately, the situations can be quite complex.\nFirst, there can be duplicates. For example, one data frame contains the meta data on a group of lab animals, while the second contains the results of the experiments at two time points. This is not a problematic situation, but you should understand what happens here:\n\nmeta_data &lt;- data.frame(ID=paste0(\"ID\", 1:4),\n                        age=sample(1:3, 4, replace=TRUE),\n                        group=rep(c(\"control\", \"treated\"), each=2))\nmeta_data\n\n   ID age   group\n1 ID1   2 control\n2 ID2   3 control\n3 ID3   1 treated\n4 ID4   2 treated\n\ncrp &lt;- data.frame(ID=rep(paste0(\"ID\", 1:4), each=2),\n                           time=rep(c(\"day1\", \"day2\"), 4),\n                           CRP=rnorm(8))\ncrp\n\n   ID time        CRP\n1 ID1 day1 -0.9147404\n2 ID1 day2  0.1746050\n3 ID2 day1 -0.1052144\n4 ID2 day2 -1.2549840\n5 ID3 day1 -1.8076750\n6 ID3 day2 -0.9105108\n7 ID4 day1  0.5603573\n8 ID4 day2 -2.0386231\n\nmerged_data &lt;- merge(meta_data, crp, by=\"ID\")\nmerged_data\n\n   ID age   group time        CRP\n1 ID1   2 control day1 -0.9147404\n2 ID1   2 control day2  0.1746050\n3 ID2   3 control day1 -0.1052144\n4 ID2   3 control day2 -1.2549840\n5 ID3   1 treated day1 -1.8076750\n6 ID3   1 treated day2 -0.9105108\n7 ID4   2 treated day1  0.5603573\n8 ID4   2 treated day2 -2.0386231\n\n\nAs you can see, each line in the meta_data data frame is duplicated to fill up the information matching the ID in the crp data frame.\nHowever, chances are the situation is more complex. Imagine that apart from the above measurements, you have another set of measurements, say, of the albumin (ALB) levels. You would like to merge the two data frames. Naturally, you expect that you will get a data frame with 8 rows. The following will not work as expected:\n\nalbumin &lt;- data.frame(ID=rep(paste0(\"ID\", 1:4), each=2),\n                      time=rep(c(\"day1\", \"day2\"), 4),\n                      ALB=rnorm(8))\n\n# incorrect code!!!\ncrp_alb &lt;- merge(crp, albumin, by=\"ID\")\ncrp_alb\n\n    ID time.x        CRP time.y        ALB\n1  ID1   day1 -0.9147404   day1 -0.8046922\n2  ID1   day1 -0.9147404   day2  0.9248835\n3  ID1   day2  0.1746050   day1 -0.8046922\n4  ID1   day2  0.1746050   day2  0.9248835\n5  ID2   day1 -0.1052144   day1 -0.6786603\n6  ID2   day1 -0.1052144   day2  0.3257268\n7  ID2   day2 -1.2549840   day1 -0.6786603\n8  ID2   day2 -1.2549840   day2  0.3257268\n9  ID3   day1 -1.8076750   day1 -1.0614504\n10 ID3   day1 -1.8076750   day2  0.4686709\n11 ID3   day2 -0.9105108   day1 -1.0614504\n12 ID3   day2 -0.9105108   day2  0.4686709\n13 ID4   day1  0.5603573   day1  1.1110624\n14 ID4   day1  0.5603573   day2  1.2042447\n15 ID4   day2 -2.0386231   day1  1.1110624\n16 ID4   day2 -2.0386231   day2  1.2042447\n\n\nUh-oh, what happened here?\nYou see, the problem is that the merge() function merges the data frames by the column ID, but it does not take into account the time column. Since it observes that ID’s are duplicated in both data frames, it creates all possible pairs of the rows: CRP from day1 with ALB day2; but also CRP from day1 and ALB from day1, CRP from day2 and ALB from day1 and so on. For each identifier. That is why we are getting 4 (and not 2) rows for each identifier.\n This is a common problem, and the solution is to merge by more than one identifier. The important question is: what combination of identifiers uniquely identifies a row in the data frame? In our case, it is the combination of ID and time. We can specify that in the by= parameter:Merging by more than one ID\n\ncrp_alb &lt;- merge(crp, albumin, by=c(\"ID\", \"time\"))\ncrp_alb\n\n   ID time        CRP        ALB\n1 ID1 day1 -0.9147404 -0.8046922\n2 ID1 day2  0.1746050  0.9248835\n3 ID2 day1 -0.1052144 -0.6786603\n4 ID2 day2 -1.2549840  0.3257268\n5 ID3 day1 -1.8076750 -1.0614504\n6 ID3 day2 -0.9105108  0.4686709\n7 ID4 day1  0.5603573  1.1110624\n8 ID4 day2 -2.0386231  1.2042447\n\n\n\n\n4.5.5 Bringing it all together\nWe are now coming to the final exercise in this section. This exercise is based on data from the vaccination study (Weiner et al. 2019). One file contains meta-data for the samples used in transcriptomic analysis – it only includes the subjects and time points for which the gene expression data is present. The other file is quite large and contains the results of several laboratory tests taken for many patients at many time points.\n\nWeiner, January, David JM Lewis, Jeroen Maertzdorf, Hans-Joachim Mollenkopf, Caroline Bodinham, Kat Pizzoferro, Catherine Linley, et al. 2019. “Characterization of Potential Biomarkers of Reactogenicity of Licensed Antiviral Vaccines: Randomized Controlled Clinical Trials Conducted by the BIOVACSAFE Consortium.” Scientific Reports 9 (1): 20362.\nNote that the data is not real, just based on real data.\n\n\n\n\n\n\n\nExercise 4.9 (Merging large data sets) The files expression_data_vaccination_example.xlsx and labresults_full.csv contain data from the same study.\n\nRead CSV file and the first sheet from the XLSX file.\nWhich columns contains the ID the subjects? Are there any subjects in common? How do you match the subjects?\nWe are interested only in the following information: Subject ID, ARM (group), Time point, sex, age, test name and the actual measurement. Are the measurements numeric? Remember, you can use expressions like [ , c(\"ARM\", \"sex\") ] or select(df, ARM, sex) to select the desired columns from a data set.\nUse the subjects and / or other information to merge the two data frames however you see fit. Note that there are multiple time points per subject and multiple measurements per subject and time point.\n\nSolution: Section 1",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manipulating data frames</span>"
    ]
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions to Exercises",
    "section": "",
    "text": "The merging of two data frames\nSolution to Exercise 4.9.",
    "crumbs": [
      "Solutions to Exercises"
    ]
  },
  {
    "objectID": "appendix-wide-long.html",
    "href": "appendix-wide-long.html",
    "title": "Appendix 1: Wide vs long data",
    "section": "",
    "text": "Wide and Long format\nWide advantages:",
    "crumbs": [
      "Introduction",
      "Appendix 1: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#wide-and-long-format",
    "href": "appendix-wide-long.html#wide-and-long-format",
    "title": "Appendix 1: Wide vs long data",
    "section": "",
    "text": "Long advantages:Wide and long format\n\neasier to filter, process, visualize, do statistics with\nfocused on measurement (“patient ID” or equivalent is a covariate, and so is measurement type)\n\n\n\ngroups data by a covariate (“patient ID”)\ncan be easier to manage (each column one measurement type)",
    "crumbs": [
      "Introduction",
      "Appendix 1: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-wide-to-long",
    "href": "appendix-wide-long.html#converting-from-wide-to-long",
    "title": "Appendix 1: Wide vs long data",
    "section": "Converting from wide to long:",
    "text": "Converting from wide to long:\nFirst, let’s create a wide data set. We use a neat trick here that you have not seen before: you can use the base R function read.table to read data directly from a character string, rather than a file!\nread.table(text=...)\n\nwide &lt;- read.table(header=TRUE, text='\n subject sex control cond1 cond2\n       1   M     7.9  12.3  10.7\n       2   F     6.3  10.6  11.1\n       3   F     9.5  13.1  13.8\n       4   M    11.5  13.4  12.9\n')\nwide\n\n  subject sex control cond1 cond2\n1       1   M     7.9  12.3  10.7\n2       2   F     6.3  10.6  11.1\n3       3   F     9.5  13.1  13.8\n4       4   M    11.5  13.4  12.9\n\n\nThis is clearly a wide format – each row corresponds not to one observation (one measurement), but one subject. We want to convert this to long format, where each row corresponds to one observation (and therefore, for each subject, there are three rows).\npivot_longer()\n\nlibrary(tidyverse)\n\npivot_longer(wide, cols=control:cond2,\n  names_to=\"condition\", values_to=\"measurement\")\n\n# A tibble: 12 × 4\n   subject sex   condition measurement\n     &lt;int&gt; &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1       1 M     control           7.9\n 2       1 M     cond1            12.3\n 3       1 M     cond2            10.7\n 4       2 F     control           6.3\n 5       2 F     cond1            10.6\n 6       2 F     cond2            11.1\n 7       3 F     control           9.5\n 8       3 F     cond1            13.1\n 9       3 F     cond2            13.8\n10       4 M     control          11.5\n11       4 M     cond1            13.4\n12       4 M     cond2            12.9\n\n\nNote that we must put quotes around condition and measurement in the code above. These are column names, but the columns don’t exist yet.",
    "crumbs": [
      "Introduction",
      "Appendix 1: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-long-to-wide",
    "href": "appendix-wide-long.html#converting-from-long-to-wide",
    "title": "Appendix 1: Wide vs long data",
    "section": "Converting from long to wide",
    "text": "Converting from long to wide\nHere is another example data set, this time in long format:\n\nlong &lt;- read.table(header=TRUE, text='\n subject  sampleID sex condition measurement\n       1  ID000001 M   control         7.9\n       1  ID000002 M     cond1        12.3\n       1  ID000003 M     cond2        10.7\n       2  ID000004 F   control         6.3\n       2  ID000005 F     cond1        10.6\n       2  ID000006 F     cond2        11.1\n       3  ID000007 F   control         9.5\n       3  ID000008 F     cond1        13.1\n       3  ID000009 F     cond2        13.8\n')\nlong\n\n  subject sampleID sex condition measurement\n1       1 ID000001   M   control         7.9\n2       1 ID000002   M     cond1        12.3\n3       1 ID000003   M     cond2        10.7\n4       2 ID000004   F   control         6.3\n5       2 ID000005   F     cond1        10.6\n6       2 ID000006   F     cond2        11.1\n7       3 ID000007   F   control         9.5\n8       3 ID000008   F     cond1        13.1\n9       3 ID000009   F     cond2        13.8\n\n\nAs you see, there are three subject, each with three measurements (one control, one for condition 1, and one for condition 2). We want to convert it to wide format, so we expect one row per subject, in total three rows.\nHowever, watch out. The first thing we might want to try does not give the expected result:\npivot_wider()\n\n## not what we wanted!!! Why?\npivot_wider(long, names_from=condition, values_from=measurement)\n\n# A tibble: 9 × 6\n  subject sampleID sex   control cond1 cond2\n    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 ID000001 M         7.9  NA    NA  \n2       1 ID000002 M        NA    12.3  NA  \n3       1 ID000003 M        NA    NA    10.7\n4       2 ID000004 F         6.3  NA    NA  \n5       2 ID000005 F        NA    10.6  NA  \n6       2 ID000006 F        NA    NA    11.1\n7       3 ID000007 F         9.5  NA    NA  \n8       3 ID000008 F        NA    13.1  NA  \n9       3 ID000009 F        NA    NA    13.8\n\n\nThe problem is in the sampleID column. Given that names of the variables are in the condition column, and measurement is in the measurement column, R considers all the remaining columns to be the identifier columns. But the column sampleID contains only unique values, so they must be put in separate rows, as above.\nTo fix this, we need to tell R not to use sampleID as an identifier column; instead, only the subject column should be used as an identifier. We can also throw in the sex column if we want to keep it, since it has only 1 value per subject.\npivot_wider(id_cols=...)\n\n## Instead: \npivot_wider(long, id_cols=c(subject, sex),\n                  names_from=condition, values_from=measurement)\n\n# A tibble: 3 × 5\n  subject sex   control cond1 cond2\n    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 M         7.9  12.3  10.7\n2       2 F         6.3  10.6  11.1\n3       3 F         9.5  13.1  13.8\n\n\n\n\n\n\n\n\n\nExercise 1 (Converting to long format) Convert the following files to long format:\n\nlabresults_wide.csv\nThe built-in iris data set (data(iris))\ncars.xlsx (tricky!)\n\nClean up and convert to long format (what seems to be the problem? How do we deal with that?):\n\nmtcars_wide.csv",
    "crumbs": [
      "Introduction",
      "Appendix 1: Wide vs long data"
    ]
  },
  {
    "objectID": "appendix-wide-long.html#converting-from-long-to-wide-1",
    "href": "appendix-wide-long.html#converting-from-long-to-wide-1",
    "title": "6  Appendix 1: Wide vs long data",
    "section": "6.4 Converting from long to wide",
    "text": "6.4 Converting from long to wide\n\n## not what we wanted!!! Why?\npivot_wider(long, names_from=\"condition\", values_from=\"measurement\")\n\n# A tibble: 9 × 6\n  subject sampleID sex   control cond1 cond2\n    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1 ID000001 M         7.9  NA    NA  \n2       1 ID000002 M        NA    12.3  NA  \n3       1 ID000003 M        NA    NA    10.7\n4       2 ID000004 F         6.3  NA    NA  \n5       2 ID000005 F        NA    10.6  NA  \n6       2 ID000006 F        NA    NA    11.1\n7       3 ID000007 F         9.5  NA    NA  \n8       3 ID000008 F        NA    13.1  NA  \n9       3 ID000009 F        NA    NA    13.8\n\n## Instead: \npivot_wider(long, id_cols=\"subject\", names_from=\"condition\", values_from=\"measurement\")\n\n# A tibble: 3 × 4\n  subject control cond1 cond2\n    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1     7.9  12.3  10.7\n2       2     6.3  10.6  11.1\n3       3     9.5  13.1  13.8\n\n\n\n\n\n\n\n\n\nExercise 6.1 (Merging large data sets) Convert the following files to long format:\n\nlabresults_wide.csv\nThe built-in iris data set (data(iris))\ncars.xlsx (tricky!)\n\nClean up and convert to long format (what seems to be the problem? How do we deal with that?):\n\nmtcars_wide.csv",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Appendix 1: Wide vs long data</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html",
    "href": "day5-visualization-and-statistics.html",
    "title": "5  Rmarkdown, basic statistics and visualizations",
    "section": "",
    "text": "5.1 Rmarkdown",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rmarkdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#basic-statistics-with-r",
    "href": "day5-visualization-and-statistics.html#basic-statistics-with-r",
    "title": "5  Rmarkdown, basic statistics and visualizations",
    "section": "5.2 Basic statistics with R",
    "text": "5.2 Basic statistics with R",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rmarkdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#visualizations-with-r",
    "href": "day5-visualization-and-statistics.html#visualizations-with-r",
    "title": "5  Rmarkdown, basic statistics and visualizations",
    "section": "5.3 Visualizations with R",
    "text": "5.3 Visualizations with R\n\n5.3.1 Base R vs ggplot2\n\n\n\n\nWeiner 3rd, January, Benedikt Obermayer, and Dieter Beule. 2022. “Venn Diagrams May Indicate Erroneous Statistical Reasoning in Transcriptomics.” Frontiers in Genetics 13: 818683. https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.818683/full.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rmarkdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#rmarkdown",
    "href": "day5-visualization-and-statistics.html#rmarkdown",
    "title": "5  Rmarkdown, basic statistics and visualizations",
    "section": "",
    "text": "5.1.1 What is Rmarkdown?\nRmarkdown is a remarkably simple to use and yet powerfull tool. The basic idea is as follows: you write a document in a simple text format – for example, a manuscript or a report. In that document, you include R “code chunks”. These code chunks are executed by R, and the output is included in the document – including figures or tables. And then, from this single simple text document you can create a PDF, a Word document, an HTML page (or a whole website!), a book, a slide presentation and much, much more.\nIn fact, this book you are reading now has been entirely written in Rmarkdown1. You can go to the github repository for this book and see the Rmarkdown/Quarto files that generated it – for example, here is the page you are reading right now. It is also possible to create a scientific paper completely in Rmarkdown (here an example of such a paper: Weiner 3rd, Obermayer, and Beule (2022)).\n1 Actually, in its successor called “Quarto”.\n\n5.1.2 Markdown basics",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rmarkdown, basic statistics and visualizations</span>"
    ]
  },
  {
    "objectID": "day5-visualization-and-statistics.html#footnotes",
    "href": "day5-visualization-and-statistics.html#footnotes",
    "title": "5  Rmarkdown, basic statistics and visualizations",
    "section": "",
    "text": "Actually, in its successor called “Quarto”.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Rmarkdown, basic statistics and visualizations</span>"
    ]
  }
]